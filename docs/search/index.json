[{"content":"Pulsar本地单机安装 1. OS要求  目前Pulsar仅支持操作系统 MacOS 与 Linux，使用Pulsar，需保证已安装 Oracle的 Java 8 运行环境。\n 2. 安装二进制版本 ","date":"2021-03-02T11:13:47+08:00","permalink":"https://example.com/p/pulsar-1.-pulsar%E6%9C%AC%E5%9C%B0%E5%AE%89%E8%A3%85/","title":"[ Pulsar ] 1. Pulsar本地安装"},{"content":"关于Restful规范 1. Restful API Design定义 2. Action命名规范 3. 自定义方法 3.1 ","date":"2021-03-01T11:13:47+08:00","permalink":"https://example.com/p/standard-1.%E5%85%B3%E4%BA%8Erestful%E8%A7%84%E8%8C%83/","title":"[ Standard ] 1.关于Restful规范"},{"content":"面试之算法 1. 删除UserList中年龄大于20的User对象 // User实体类 public class User{ private Integer age; public Integet getAge(){ return this.age; } } 解题思路：\n此题按照最简单的lambda方式进行解答(JDK8才支持)\npublic class Main{ public static void remove(List\u0026lt;User\u0026gt; userList){ // removeIf(Predicate\u0026lt;? super E\u0026gt; filter)  userList.removeIf(x -\u0026gt; x != null \u0026amp;\u0026amp; x.getAge() != null \u0026amp;\u0026amp; x.getAge() \u0026gt; 20); } } 2. 从1000W个数中取出最小的10个数，并按照顺序打印。 解题思路： 首先应该想到堆排序(Top K堆问题)，大根堆(前k小)或小根堆(后k大)。在Java中有已经实现的PriorityQueue，解决此问题将最为简单，复杂度为O(NlogK)。\n本题是求前K小，因此选用一个容量为K的大根堆，每次poll出最大的数，则堆中保留的则是前K项小。(谨记：需要不可用小根堆，小根堆需把全部元素入堆，时间复杂度为O(NlogN),将不再是O(NlogK))，Java中PriorityQueue默认为小根堆，需作出调整重写比较器。\npublic class MaxHeap { public static void main(String[] args) { int[] arr = new int[]{10,1,0,9,22,77,12,883,99983,2772,848,3663,2626,737,2772,8388,266,83,72,7272,83883,27727,263,840,2740,884}; print(arr); } public static void print2(int[] arr) { int k = 10; int[] vec = new int[k]; // 重写PriorityQueue为大根堆  // PriorityQueue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;();默认为小根堆  PriorityQueue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;((num1, num2) -\u0026gt; num2 - num1); for (int i = 0; i \u0026lt; k; ++i) { queue.offer(arr[i]); } for (int i = k; i \u0026lt; arr.length; ++i) { if (queue.peek() \u0026gt; arr[i]) { queue.poll(); queue.offer(arr[i]); } } for (int i = 0; i \u0026lt; k; ++i) { vec[i] = queue.poll(); } System.out.println(Arrays.toString(vec)); } } 3. 3个线程A、B、C存在依赖关系，B依赖A执行结束，C依赖B执行结束，请设计实现。 解题思路： 主要考察多线程Thread对象的wait()、notify()以及线程间共享的信号量。\npublic class Main { public static void main(String[] args) { Lock lockA = new Lock(); Lock lockB = new Lock(); new A(lockA).start(); new B(lockA, lockB).start(); new C(lockB).start(); } } class Lock{ private int flag = 0; public int getFlag() { return flag; } public void setFlag(int flag) { this.flag = flag; } } class A extends Thread{ private Lock lock; public A(Lock lock) { this.lock = lock; } @Override public void run() { synchronized (lock) { try { System.out.println(\u0026#34;thread A starting \u0026#34;); if (lock.getFlag() \u0026gt; 0) { lock.notify(); } else { lock.setFlag(1); } } catch (Exception e) { e.printStackTrace(); } } } } class B extends Thread{ private Lock lockA; private Lock lockB; public B(Lock lockA,Lock lockB) { this.lockA = lockA; this.lockB = lockB; } @Override public void run() { synchronized (lockA) { try { if (lockA.getFlag() == 0) { lockA.setFlag(1); lockA.wait(); } System.out.println(\u0026#34;Thread B starting\u0026#34;); synchronized (lockB) { if (lockB.getFlag() \u0026gt; 0) { lockB.notify(); } else { lockB.setFlag(1); } } } catch (Exception e) { e.printStackTrace(); } } } } class C extends Thread { private Lock lock; public C(Lock lock) { this.lock = lock; } @Override public void run() { synchronized (lock) { try { if (lock.getFlag() == 0) { lock.setFlag(1); lock.wait(); } System.out.println(\u0026#34;Thread C starting\u0026#34;); } catch (Exception e){ e.printStackTrace(); } } } } /** * 给定一个二叉树, 检查它是否是镜像对称的 * 例如以下是镜像对称的 * 1 * / \\ * 2 2 * / \\ / \\ * 3 4 4 3 * * 下面这个则不是镜像对称的 * 1 * / \\ * 2 2 * \\ \\ * 3 3 * * TreeNode类的定义: * * @param TreeNode 一颗二叉树 * @return boolean 是否是对称的 */ // 以下给出TreeNode类, 请勿修改 static class TreeNode { int val; TreeNode left; TreeNode right; TreeNode(int x) { val = x; } } 解题思路\n解答代码\n/** * 对任意一个Map\u0026lt;String, Object\u0026gt;, 其 key 为 String, * 其 value 为 Map\u0026lt;String, Object\u0026gt; Object[] Number String 中的任意一种, * 显然叶子节点是 value 类型为 Number 或 String的节点, * 将 Map 转为多条字符串, 每条字符串表达其中一个叶子节点, * 比如: * {\u0026#34;a\u0026#34;:{\u0026#34;b\u0026#34;:[\u0026#34;v\u0026#34;,2,{\u0026#34;c\u0026#34;:0}]},\u0026#34;d\u0026#34;:[1,null,3]} * 将转化为以下这些字符串 * a.b[0] = v * a.b[1] = 2 * a.b[2].c = 0 * d[0] = 1 * d[1] = null * d[2] = 3 * * @param map 上述的 map * @return 所有的字符串 */ 解题思路\n解答代码\n/** * 注意! 本题不要遍历二维数组. 要求时间复杂度严格低于n^2, 否则视为不得分 * * 现有一个n\\*n的二维正整数数组nums，每行元素保证递增，每列元素保证递增， 求某正整数x是否存在于该二维数组中，需要尽量优化时间和空间复杂度； * @param int[][] nums * @param int x 目标数 * @return boolean */ 解题思路\n解答代码\n","date":"2021-01-08T01:13:47+08:00","permalink":"https://example.com/p/interview-2.-%E9%9D%A2%E8%AF%95%E4%B9%8B%E7%AE%97%E6%B3%95/","title":"[ Interview ] 2. 面试之算法"},{"content":"面试 X_Q 1.JVM垃圾回收机制 Java编程中，程序员不需要刻意显式进行垃圾回收去释放一个对象内存，而是JVM会进行自动垃圾回收。 JVM中存在一个 优先级较低 的线程，只有当 JVM处于空闲 或者 堆空间不足 的情况触发执行。此过程中，将扫描到 没有被引用的对象 防止垃圾回收集合中，进行垃圾回收。\n 判断对象是否可以被回收   引用计数器：\n 为每一个对象创建一个引用计数器，有引用此对象，计数器进行+1；引用释放后，计数器进行-1。当对象引用计数器 == 0时，说明此对象可以进行回收。(不能解决循环引用问题)    可达性分析算法\n 从GC Roots开始向下搜索，走过的搜索路径将形成引用链，当一个对象到 GC Roots不存在任何引用链式，说明此对象可以进行回收。      2.什么是Netty？  见 Netty\n 3.Netty粘包/拆包 3.Kubernetes工作原理 4.快排实现原理 B_D 1. Hash最终一致性算法   算法目标：\n当K个Key的请求时，后台增减节点，只会引起 K/N 的 Key发生重新映射。\n 在后台节点稳定时，同一个key的每次请求映射到的节点是一样的。 在后台节点增减时，此算法尽量将 K 个Key映射到之前相同的节点上。    Hash存在问题：\n 假定N为后台服务节点数，当前台携带关键字key发出请求时，我们通常将key进行Hash后采用 模运算(hash(key)%N) 来讲请求分发在不同的节点上。 对前台请求于后台无状态服务节点不敏感的场景而言，只要请求key具有一定的随机性，哪怕节点动态增删，该算法对于后台而言一样可以起到很好的负载均衡效果。 但在对于分布式缓存，或者分布式数据这样有状态服务的情况下，上述方式将存在问题。因为后台节点的增删会引起几乎所有的Key的重新映射：  针对分布式缓存而言，均发生cache miss； 针对分布式数据库而言，发生数据错乱的情况，影响都是灾难性的。      判断标准：\n 平衡性(Balance)：指哈希的结果能够尽可能分不到所有的缓冲中，这样可以使得所有缓冲空间都得到利用。 单调性(Monotonicity)：单调性是指如果已经有一些内容通过Hash算法分配到了缓冲中，又有新的缓冲加入到系统中，Hash的结果应该能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 分散性(Spread)：在分布式环境中，终端有可能看不到所有的缓冲。不同的终端可能通过Hash的过程将同样的缓冲内容应设在不同的缓冲中。应尽量降低分散性。 负载(Load)：实际上是另一个角度看待分散性。对于一个特定的缓冲区而言，可能被不同的用户映射到不同的内容。应尽量降低缓冲的负荷。    Hash一致性算法：\n 是一个2^32个点组成的Hash圆环 按照 顺时针方向 进行组织 将数据 Key 使用相同的 hash() 计算出Hash值，并确定在此 Hash环 上的位置，从此位置按照 顺时针方式 寻找，碰到的第一台节点将是定位到的节点。    Hash环发生数据倾斜\n 在服务节点太少的情况，数据容易发生倾斜 解决办法：  增加虚拟节点，形成均匀的Hash环避免数据倾斜。 通过 Hash(\u0026quot;${Node1}#1\u0026quot;) 的方式构造成虚拟节点。      2. TCP与UDP的区别  TCP  面向有连接  发送数据前两端必须进行连接。因 三次握手 建立连接，使得数据传输更加可靠     仅支持单播方式 面向字节流  不保留报文边界，通过字节流的方式进行传输   可靠传输  为保证可靠传输，每个包都拥有序号，同时保证包传输的顺序性也保证包的完整性。 接收端接收到数据后需要返回确认帧，如果在规定时延内发送端未收到确认帧，将视为丢包进行重传。   拥塞机制  在网络出现拥塞的情况下，TCP可以减小向网络注入数据的速率和数量，缓解拥塞   提供全双工通信  通讯双方的应用程序在任何时候都能发送数据     UDP  面向无连接 支持单播、多播、广播方式 面向报文  UDP对应用成交付的报文不合并不拆分，而是保留报文的边界，应用程序必须要控制合适的报文大小。   不可靠性  通信不需要建立连接。想发就发，并且不关心对方接收到的数据是否正确无误 没有拥塞控制，可能出现丢包情况。   头部数据小，数据传输高效。    3. TCP如何保证网络安全的   即TCP的特性\n 4. TCP三次握手  第一次握手  客户端 向 服务端 发出连接请求的报文，请求发送成功后，客户端进入 SYN-SENT 状态   第二次握手  服务端 收到 客户端 连接请求的报文后，如同意连接，发出一个应答，发送完成后进入 SYN-RECEIVED 状态   第三次握手  当 客户端 收到连接同意的应答后，需向服务端发送一个确认报文。发送完成后，客户端进入 ESTABLISHED 状态， 服务端 收到这个确认应答后也将进入 ESTABLISHED    5. TCP四次挥手 （1）：客户端发送终止命令FIN\n（2）：服务端收到后回复ACK，处于close_wait状态\n（3）：服务器将关闭前需要发送信息发送给客户端后处于last_ack状态\n（4）：客户端收到FIN后发送ack后处于tim-wait而后进入close状态\n5. Kubernetes中Pod间如何通讯的  通过 CNI(Container Network Interface) 进行网络通讯。\n 6. Mysql索引  Mysql索引\n 7. Mysql B+ Tree索引实现原理 8. Zookeeper在Kafka中的作用   Broker注册\n分布式下的Broker之间相互独立，需要一个注册中心将其Broker集群管理起来，Zookeeper将会对Broker集群列表进行记录。\n  Topic注册\n同一个Topic的消息会被分成多个分区分散在不同的Broker，Zookeeper将会维护分区信息以及与broker之间对应关系。\n  负责Producer、Consumer负载均衡\nZookeeper通过负载均衡，协助Producer、Consumer将消息合理的发送或消费到指定的Broker。\n  分区与Consumer的关系\n每一个消费者一旦确定一个消息分区的消费能力，需要将其对应信息写入至Zookeeper对应消息分区的临时节点上。\n  记录Consumer消费进度Offset\n在Consumer对指定消息分区进行消费的过程中，会定时将分区消息的Offset记录到Zookeeper上。以便此消费者重启或变更其他消费者消费时再次继续进行消费。\n  9. 有没有做过DockerImage精简 10. 服务是如何在服务器运行的，如何实现发布DockerImage发布的 ","date":"2021-01-08T01:13:47+08:00","permalink":"https://example.com/p/interview-3.-%E9%9D%A2%E8%AF%95%E4%B9%8Bjava/","title":"[ Interview ] 3. 面试之Java"},{"content":"面试准备 1. 什么是解耦 2. 什么是异步 3. 什么是消峰填谷 4. RocketMQ执行流程 5. 怎么理解Producer 6. 怎么理解Consumer 7. 消费者消费模式有哪些 8. 消费者获取消息有几种模式 9. 定时消息是什么样的？如何实现的？ 10. RocketMQ如何保证高可用的？ 11. 如何保证消息不被重复消费？或者说如何保证消息消费是的幂等性？ 12. 如何保证消息的可靠性传输？若消息出现丢失如何处理？ 13. 如何保证消息的顺序性？ 14. 如何解决消息队列的延时以及过期失效问题？ 15. 消息队列满了以后如何处理？有几百万消息持续挤压几小时，如何解决？ 16. 如何解决高性能读写数据问题？ ","date":"2021-01-08T01:13:47+08:00","permalink":"https://example.com/p/rocketmq-1.%E9%9D%A2%E8%AF%95/","title":"[ RocketMQ ] 1.面试"},{"content":"面试之清单  加粗加斜 : 为本人面试所遇到重复出现的点儿 加粗 : 为本人面试所遇到的点儿 正常 : 为本人面试刷的面经  Algorithm Sort  手撸插入排序 手撸二分查找 堆排序(Top K) 快素排序思想 反转单链表 合并二叉树(递归与非递归两种办法)  Java Basic  Object是所有对象的父类，那Object类中有哪些方法 hashCode()与equals()有什么关系 String、StringBuilder、StringBuffer区别 反射机制 方法(静态、一般方法)；代码块(this，ClassName.class)  Collections  List之ArrayList List之LinkedList List之Vector Map之HashMap Map之LinkedHashMap Map之ConcurrentHashMap Map之TreeMap HashTable Set之HashSet Set之TreeSet Queue之ArrayBlockingQueue Queue之ConcurrentLinkedQueue Queue之LinkedBlockingQueue  Thread  线程的几种状态以及如何转化的 三种线程初始化方法的区别(Thread,Callable,Runnable) 线程池几个重要参数 线程池四种拒绝策略 线程池四个类型 线程池(ThreadPoolExecutor)原理 关于锁 synchronized锁以及锁升级 ReentrantLock与synchronized区别 有界、无界任务队列，手写BlockingQueue ThreadLocal：底层数据结构、ThreadLocalMap、原理、应用场景 Atomic类：原理、应用场景 Volatile：原理、有序性、可见性  JVM   JVM内存结构\n  final修饰的常量处于JVM中哪部分内存\n  JVM垃圾回收机制\n  引用类型\n  JVM垃圾回收器，各有什么特点\n  垃圾回收算法\n  对象什么时间可以被回收\n  新生代垃圾回收器与老年代垃圾回收器有哪些？区别是什么？\n  CMS垃圾回收器\n  JVM中Survivor空间是做什么的\n  类加载器\n  双亲委派模型\n  Tomcat打破双亲委派\n  类加载机制\n  类加载\n  Java Framework Spring   SpringIOC原理\n  容器加载Bean流程\n  SpringAOP原理\n  FactoryBean 与 BeanFactory区别\n  Spring三级缓存\n  SpringBoot循环依赖\n  SpringBoot订阅发布模式。ApplicationEvent处于什么角色，如何实现的？\n  JDK动态代理和CGLIB动态代理的区别\n  SpringAOP常用的切入点\n  SpringIOC常用注解\n  @Autowired与@Resource区别\n  @Qualifier与@Autowired\n  SpringBoot上下文切换\n  SpringBoot编译打包后包的结构?通过main()函数作为主入口的程序打包后如何启动？\n  Spring启动加载配置文件的流程\n  自动装配\n  MyBatis   MyBatis二级缓存\n  Mybatis原理\n  Netty  聊聊Netty Netty中的拆包、粘包是为什么？如何解决？ Netty的线程模型 Netty的重要组件 Netty中EventLoopGroup与EventLoop什么区别  Dubbo  Dubbo中的SPI，为什么不适用JDK的SPI？ Dubbo中的SPI Dubbo服务注册与发现流程 Dubbo通信原理 Dubbo分层 Dubbo服务暴露流程 Dubbo服务引用流程 Dubbo的管理控制台能做什么 Dubbo集群容错方案 Dubbo支持什么协议 Dubbo如何做负载均衡 Dubbo如何实现异步调用 Dubbo在Provide上可以配置Consumer哪些属性 Zookeeper和Dubbo的关系 Dubbo分层  Zookeeper  一致性算法原理 选举流程 ZAB协议  Kafka  Zookeeper在Kafka中的作用  RocketMQ ElasticSearch Middleware Redis   什么是击穿？什么是雪崩？\n  Redis支持的几种数据结构\n  Redis跳表\n  Redis中的BitMap\n  MySQL   讲讲Mysql索引\n  Mysql中B+ Tree索引原理\n  Mysql索引失效的几种情况\n  Mysql强制索引\n  分库分表如何实现的\n  分库分表中间件\n  Distribution Base  一致性Hash算法  Kubernetes  Kubernetes中Pod间是如何通信的 Kubernetes中IngressController  操作系统 TCP/IP  TCP三次握手/四次挥手 TCP与UDP的区别  多线程与并发  说说进程、线程、协程 操作系统的互斥锁原理，阻塞过程中(抢锁失败)线程会做什么  设计模式  观察者模式 单例模式  ","date":"2021-01-07T01:13:47+08:00","permalink":"https://example.com/p/interview-1.-%E9%9D%A2%E8%AF%95%E4%B9%8B%E6%B8%85%E5%8D%95/","title":"[ Interview ] 1. 面试之清单"},{"content":"Netty基础 从一答一问开始Netty 1. 为什么用Netty？ a. Netty是一个基于JDK NIO的Client/Server架构的框架。可以快速进行网络开发; b. 相比JDK NIO，极大简化TCP、UDP套接字服务器网络变成，并且性能、安全性出色; c. 支持多种协议：FTP、SMTP、HTTP以及各种二进制和基于文本的传统协议; d. 统一的API、支持多种传输类型(阻塞、非阻塞I/O); e. 简单且强大的线程模型; f. 自带编解码器解决粘包、拆包问题; g. 自带各种协议栈; h. 安全性不错，支持完整的SSL/TLS及StartTLS协议; i. 相比JDK NIO，API具有更高吞吐量、更低延迟、更低资源消耗和更少的内存复制; j. 成熟项目很多：Dubbo、RocketMQ。 2. Netty应用场景 a. RPC框架的网络通信工具 b. 自实现HTTP服务器 c. 实现即时通信系统 d. 实现消息推送系统 3. Netty核心组件   Channel\n对网络操作的抽象类。除I/O基本操作之外，支持bind(),connect(),read(),write()操作。\n  EventLoop\nEventLoop是Netty的核心抽象，用于处理连接在生命周期中所发生的事件。 主要作用：负责监听网络事件并调用事件处理器进行相关的I/O操作。\nEventLoop 负责处理注册到其上的 Channel 处理I/O操作，两者配合参与I/O操作。\n  ChannelFuture\nNetty是异步非阻塞，所有的I/O操作都是异步。所以不可以立即得到操作结果。但是\n（1）通过 ChannelFuture 接口的 addListener() 注册一个 ChannelFutureListener 对象，操作执行成功或失败后，监听会自动触发返回结果；\n（2）也可以通过 ChannelFuture 的 channel() 获取关联的 Channel 对象；\n（3) 也可以通过 ChannelFuture 的 sync() 进行让异步的操作变成同步。\n  ChannelHandler与ChannelPipeline\n  ChannelHandler 是消息的具体处理器。他负责处理读写操作、客户端连接等事情。\n  ChannelPipeline 为 ChannelHandler 链提供了一个容器并定义了用于沿着链传播入站和出站事件流的 API 。当 Channel 被创建时，它会被自动地分配到它专属的 ChannelPipeline 。\n  我们可以在 ChannelPipeline 上通过 addLast() 方法添加一个或者多个ChannelHandler ，因为一个数据或者事件可能会被多个 Handler 处理。当一个 ChannelHandler 处理完之后就将数据交给下一个 ChannelHandler 。\n    4. Netty线程模型   单线程模型\n一个线程需要执行处理所有的 accept、read、decode、process、encode、send 事件。对于高负载、高并发，并且对性能要求比较高的场景不适用。\n//1.eventGroup既用于处理客户端连接，又负责具体的处理。  EventLoopGroup eventGroup = new NioEventLoopGroup(1); //2.创建服务端启动引导/辅助类：ServerBootstrap  ServerBootstrap b = new ServerBootstrap(); boobtstrap.group(eventGroup, eventGroup) //......   多线程模型\n一个 Acceptor 线程只负责监听客户端的连接，一个 NIO 线程池负责具体处理 满足绝大部分应用场景，并发连接量不大的时候没啥问题，但是遇到并发连接大的时候就可能会出现问题，成为性能瓶颈。\n// 1.bossGroup 用于接收连接，workerGroup 用于具体的处理 EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //2.创建服务端启动引导/辅助类：ServerBootstrap  ServerBootstrap b = new ServerBootstrap(); //3.给引导类配置两大线程组,确定了线程模型  b.group(bossGroup, workerGroup) //......   主从多线程模型\n从一个 主线程 NIO 线程池中选择一个线程作为 Acceptor 线程，绑定监听端口，接收客户端连接的连接，其他线程负责后续的接入认证等工作。连接建立完成后，Sub NIO 线程池负责具体处理 I/O 读写。如果多线程模型无法满足你的需求的时候，可以考虑使用主从多线程模型 。\n// 1.bossGroup 用于接收连接，workerGroup 用于具体的处理 EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //2.创建服务端启动引导/辅助类：ServerBootstrap  ServerBootstrap b = new ServerBootstrap(); //3.给引导类配置两大线程组,确定了线程模型  b.group(bossGroup, workerGroup) //.....   5. EventLoopGroup是什么？与EventLoop关系。   EventLoop 的主要作用实际就是负责监听网络事件并调用事件处理器进行相关 I/O 操作的处理。\n  EventLoopGroup 包含多个 EventLoop（每一个 EventLoop 通常内部包含一个线程）。 并且 EventLoop 处理的 I/O 事件都将在它专有的 Thread 上被处理，即 Thread 和 EventLoop 属于 1 : 1 的关系，从而保证线程安全。\n  BossEventLoopGroup 用于接收连接，WorkerEventLoopGroup 用于具体的处理（消息的读写以及其他逻辑处理）\n  当客户端通过 connect() 连接服务端时，BossEventLoopGroup 处理客户端连接请求。当客户端处理完成后，会将这个连接提交给 WorkerEventLoopGroup 来处理，然后 WorkerEventLoopGroup 负责处理其 IO 相关操作。\n  6. Netty中TCP粘包、拆包 TCP 粘包/拆包 就是基于 TCP 发送数据的时候，出现了多个字符串“粘”在了一起或者一个字符串被“拆”开的问题。\n解决办法：\n 使用 Netty 自带的解码器  LineBasedFrameDecoder : 发送端发送数据包的时候，每个数据包之间以换行符作为分隔，LineBasedFrameDecoder 的工作原理是它依次遍历 ByteBuf 中的可读字节，判断是否有换行符，然后进行相应的截取。 DelimiterBasedFrameDecoder : 可以自定义分隔符解码器，LineBasedFrameDecoder 实际上是一种特殊的 DelimiterBasedFrameDecoder 解码器。 FixedLengthFrameDecoder: 固定长度解码器，它能够按照指定的长度对消息进行相应的拆包。 LengthFieldBasedFrameDecoder + LengthFieldPrepender 配合使用: 处理粘拆包的主要思想是在生成的数据包中添加一个长度字段，用于记录当前数据包的长度。(分为有头部的拆包与粘包、长度字段在前且有头部的拆包与粘包、多扩展头部的拆包与粘包。)  LengthFieldBasedFrameDecoder会按照参数指定的包长度偏移量数据对接收到的数据进行解码，从而得到目标消息体数据 LengthFieldPrepender则会在响应的数据前面添加指定的字节数据，这个字节数据中保存了当前消息体的整体字节数据长度     自定义序列化编解码器  通过继承LengthFieldBasedFrameDecoder和LengthFieldPrepender来实现粘包和拆包的处理    7. Netty长连接、心跳机制 在 TCP 保持长连接的过程中，可能会出现断网等网络异常出现，异常发生的时候， client 与 server 之间如果没有交互的话，它们是无法发现对方已经掉线的。为了解决这个问题, 我们就需要引入 心跳机制 。\n  心跳机制的工作原理是: 在 client 与 server 之间在一定时间内没有数据交互时, 即处于 idle 状态时, 客户端或服务器就会发送一个特殊的数据包给对方, 当接收方收到这个数据报文后, 也立即发送一个特殊的数据报文, 回应发送方, 此即一个 PING-PONG 交互。所以, 当某一端收到心跳消息后, 就知道了对方仍然在线, 这就确保 TCP 连接的有效性.\n  Netty中的心跳机制实现： TCP 实际上自带的就有长连接选项，本身是也有心跳包机制，也就是 TCP 的选项：SO_KEEPALIVE。但是，TCP 协议层面的长连接灵活性不够。所以，一般情况下我们都是在应用层协议上实现自定义心跳机制的，也就是在 Netty 层面通过编码实现。通过 Netty 实现心跳机制的话，核心类是 IdleStateHandler\n  8. Netty零拷贝 在 OS 层面上的 Zero-copy 通常指避免在 用户态(User-space) 与 内核态(Kernel-space) 之间来回拷贝数据。而在 Netty 层面，零拷贝主要体现在对于数据操作的优化。\nNetty 中的零拷贝体现在以下几个方面：\n 使用 Netty 提供的 CompositeByteBuf 类， 可以将多个ByteBuffer 合并为一个逻辑上的 ByteBuffer， 避免了各个 ByteBuf 之间的拷贝。 ByteBuffer 支持 slice 操作， 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf， 避免了内存的拷贝。 通过 FileRegion 包装的FileChannel.tranferTo 实现文件传输， 可以直接将文件缓冲区的数据发送到目标 Channel， 避免了传统通过循环 write 方式导致的内存拷贝问题。  ","date":"2021-01-05T10:36:27+08:00","permalink":"https://example.com/p/netty-1.netty%E5%9F%BA%E7%A1%80/","title":"[ Netty ] 1.Netty基础"},{"content":"面试准备 一、学习路线 1.Java基础  面向对象特性：封装、继承、多态(动态绑定、向上转型) 泛型、类型擦除 反射、其原理及其优缺点 static,final关键字 String,StringBuffer,StringBuilder底层区别 BIO、NIO、AIO Object类的方法 自动拆箱与自动装箱  2.集合框架  List  ArrayList LinkedList Vector CopyOnWriteArrayList   Set  HashSet TreeSet LinkedHashSet   Queue  PriorityQueue   Map  HashMap TreeMap LinkedMap   fast-fail，fast-safe机制 源码分析(底层数据结构，插入、扩容过程)、线程安全分析  3.Java虚拟机  类加载机制、双亲委派模式、3种加载器(BootstrapClassLoader，ExtensionClassLoader，ApplicationClassLoader) 运行时内存分区(PC，Java虚拟机栈，本地方法栈，堆，方法区[永久代、元空间]) JMM: Java内存模型分析 引用计数、可达性分析 垃圾回收算法：标记-清除、标记-整理、复制 垃圾回收器：比较区别(Serial，ParNew，ParallelScavenge，CMS，G1) 强、软、弱、虚引用 内存溢出、内存泄漏排查 JVM调优、常用命令  4.Java并发  三种线程初始化方法的区别(Thread,Callable,Runnable) 线程池(ThreadPoolExecutor，7大参数、原理、四种拒绝策略、四个类型[Fixed、Single、Cached、Scheduled]) Synchronized 使用：方法(静态、一般方法)；代码块(this，ClassName.class) jdk1.6优化：锁粗化、锁消除、自适应自旋锁、偏向锁、轻量级锁 锁升级的过程与细节：无锁-\u0026gt;偏向锁-\u0026gt;轻量级锁-\u0026gt;重量级锁(不可逆) ReentrantLock:与Synchronized区别、公平锁、非公平锁、可中断锁、原理、用法 有界、无界任务队列，手写BlockingQueue 乐观锁：CAS(优缺点，ABA问题，DCAS) 悲观锁 ThreadLocal：底层数据结构、ThreadLocalMap、原理、应用场景 Atomic类：原理、应用场景 Volatile：原理、有序性、可见性  5.MySQL  架构：Server层、引擎层(缓存、连接器、分析器、优化器、处理器) 引擎：InnoDB、MyISAM、Memory的区别 聚簇索引、非聚簇索引区别(从二叉平衡搜索树复习[AVL，红黑树] -\u0026gt; B树 -\u0026gt; B+树) MySQL、SQL语句优化 覆盖索引、最左前缀匹配 当前读、快照读 MVCC原理(事务ID、隐藏字段、Undo、ReadView) Gap Lock、Next-Key Lock、Record Lock 三大范式 常用SQL 连接：自连接、内连接(等值、非等值、自然连接)、外连接(左、右、全) Group BY 与 Having Explain  5.Spring  AOP原理(JDK动态代理、CGLIB动态代理)、IOC原理 SpringBean生命周期 SpringMVC原理 SpringBoot常用注解  6.Unix Network  OSI模型、TCP/IP模型 TCP、UDP区别 TCP可靠性传输原理  重传 流量控制 拥塞控制 序列号、确认应答号 校验和   三次握手、四次挥手原理 timewait、closewait HTTP  报文格式 协议 状态码 无状态解决(Cookie、Session)   HTTPS  CA证书 对称、非对称加密   DNS解析过程原理 IP、ICMP、ARP、ROUTE协议 攻击手段：XSS、CSRF、SQL注入、DOS、DDOS  7.OS  进程、线程、协程区别 进程通信方式(管道、消息队列、共享内存、信号、信号量、socket) 进程调度算法：  先进先出 短作业优先 时间片轮换 多级反馈队列 优先级调度   内存管理  页面置换算法：手写LRU 分段 虚拟内存    二、面试题锦 1.算法\n 手写最大堆 一道 LeetCode 难问题：接雨水（动态规划解决） 手写 LRU（要求用泛型写）、手写 DCL 一道动态规划题目：不同路径 手写个归并排序 了解 A*算法吗？ 手写地杰斯特拉算法？ 说说深度优先搜索算法、回溯算法 一道算法题：一个走迷宫问题，DFS+回溯解决 一道算法题：最长公共子串 两个栈实现队列 最近公共祖先节点 一道算法题：完全平方数（动态规划） 手写一个堆排序。 手写快排 算法题：按 K 位反转链表 一百亿个数，n 个机器，怎么排序？（桶排序）  2.设计模式\n 设计模式了解吗？几大类型？谈谈工厂模式？  3.Java 集合\n 谈一下 Java 集合框架？HashMap 线程安全的吗？会出现什么问题？ 说说 fast-fail 和 fast-safe？ 讲讲 HashMap 的原理，put 过程？resize 过程？线程安全吗？死循环问题  4.Java 并发\n 知道哪些 Java 的锁？CAS 的缺点？Synchronized 优化内容？锁升级过程？ 线程池了解吗？原理？可以写个 BlockingQueue 吗？ 了解死锁吗？怎么解决？ 哪些对象可以作为 GC ROOTS 谈谈公平锁和非公平锁？ Synchronized 和 ReentrantLock 区别 ThreadLocal 了解吗？原理？  5.JVM\n 谈谈Java虚拟机你的认识？垃圾回收算法？垃圾回收器？ Java是怎么进行垃圾回收的？ 谈谈Java虚拟机？类加载机制？ 知道双亲委派模式吗？有什么好处？ Java运行时内存分区？ 讲一下CMS垃圾回收过程 OOM 怎么排查？  6.MySQL\n InnoDB 和 MyISAM 区别？谈谈 MySQL 的各种引擎？ 知道聚簇索引和非聚簇索引吗？B 树和 B+树区别？ 说说 MySQL 的架构 说说 MYSQL 优化策略？ 覆盖索引和非覆盖索引区别？ MYSQL 优化方法有哪些？ MySQL 的索引为什么快？有哪些索引？原理数据结构？ 谈谈各种索引？为什么用 B+树不用 B 树？ B+ Tree索引和Hash索引区别？  7. Redis\n Redis  8.Spring\n 谈谈 Spring AOP 和 IOC SpringBoot 常用哪些注解？  9.Mybatis\n Mybatis  10.ElasticSearch\n ES  11.Kafka\n Kafka  12.Dubbo\n13.Netty\n14.RocketMQ\n15.Zookeeper\n16.OS\n DNS 解析过程 进程间通信方式？哪种最高效？ ARP 过程？ 进程调度算法？  17.Network\n TCP 如何保证可靠性传输？（校验和，序列号和确认应答号，重传，流量控制，拥塞控制） TCP拥塞控制算法？ TCP 和 UDP 区别？ HTTP 的状态码记得哪些？ ICMP 是哪层的？有什么用？  18.分布式系统\n 谈谈你对分布式系统的认识？  ","date":"2021-01-05T01:13:47+08:00","permalink":"https://example.com/p/interview-2.%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/","title":"[ Interview ] 2.面试准备"},{"content":"面试准备  Java线程池  JDK中JUC包  JVM调优：如果发现某个服务慢，如何排查，如何处理；发现某个服务器CPU100%了，应该如何处理 遵循六步走原则： 1）找到罪魁祸首的进程 2）分析进程对应的线程 3）生成JVM当前时刻线程快照 4）分析定位代码问题 5）  垃圾回收器G1与GC算法CMS  Tomcat 如何实现类隔离  Spring IOC原理  Spring AOP原理  Spring事务  Dubbo服务发现、注册流程  Dubbo通信原理  Dubbo SPI与Java SPI  Zookeeper选举协议  大数据量的排序如何处理  如何判断链表是回文链表、快排序、归并排序  MySQL调优：B+ Tree索引、Hash索引  Mybatis的一、二级缓存  Mybatis工作原理  Redis Bitmap  ","date":"2020-12-31T11:13:47+08:00","permalink":"https://example.com/p/interview-1.%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/","title":"[ Interview ] 1.面试准备"},{"content":"Dubbo中的SPI 1. Dubbo SPI与Java SPI  SPI(Service Provider Interface)，主要用于框架，框架定义接口。   不同使用者将存在不同需求，也必然出现不同实现方式。\n  而SP\u0008I就是通过定义一个特定的位置，Java SPI约定在Classpath下的META-INF/services/路径下创建一个以服务接口命名的文件，然后文件中记录的是此jar包提供的具体实现类的全限定名，并由服务加载器读取配置文件，加载实现类，这样可以在运行时动态为接口替换实现类。\n  Dubbo SPI\n 并非是Java原生的SPI，而是重新实现的SPI。   Java SPI通过ServiceLoader进行加载； Dubbo SPI通过ExtensionLoader进行拓展加载。  支持的注解：  @SPI(标记为拓展接口) @Adaptive(自适应拓展实现类标志) @Activate(自动激活条件标记)   配置文件放在classpath下的META-INF/dubbo/以及 META-INF/dubbo/internal下 Dubbo SPI增加了对拓展点IOC和AOP的支持，一个拓展点可以直接通过Setter注入其他拓展点。 Java SPI会一次性实例化拓展点所有实现，如果有拓展实现初始化过程很耗时，并且用不上，将会造成资源浪费。    Dubbo中SPI的具体实现\n 协议扩展(Protocol)：RPC协议扩展，用于封装远程调用细节 调用拦截扩展(Filter)：服务提供方和服务消费方调用过程拦截，Dubbo本身大多数功能都是基于此扩展点实现，每次远程方法执行，该拦截都会被执行 引用监听扩展(InvokerListener)：当有服务被引用时触发此事件 暴露监听扩展(ExporterListener)：当有服务被暴露时触发此事件 集群扩展(Cluster)：当存在多个服务提供方时，将多个服务提供方组成一个集群，并伪装成一个服务提供方 路由扩展(RouterFactory)：从多个服务提供方中选择一个进行调用 负载均衡(LoadBalance)：从多个服务提供方中选择一个进行调用 合并扩展(Merger)：合并返回结果，用于分组聚合 注册中心扩展(Registry)：负责服务的注册与发现 监控扩展(Monitor) ：负责服务调用次数以及调用时间的监控 扩展点加载扩展(ExtensionFactory)：扩展本身的加载容器，可从不同容器加载扩展点 动态代理扩展(ProxyFactory)：将Invoker接口转换成业务接口 编译器扩展(Compiler)： Java代码编译器，用于动态生成字节码，加速调用 配置中心扩展(DynamicConfiguration)：作为Key-Value存储 消息派发扩展(Dispatcher)：通道信息派发器，用于指定线程池模型 线程池扩展(ThreadPool)：服务提供方线程池的实现策略 序列化扩展(Serialization)：将对象转化成字节流，用于网络传输，以及将字节流转为对象，用于接收到字节流数据时还原成对象 网络传输扩展(Transporter)：远程通信的服务器以及客户端传输实现 信息交换扩展(Exchange)：`` 组网扩展(Networker)：对等网络节点组网器 Telnet命令扩展(TelnetHandler)：所有服务器均支持telnet访问，用于人工干预 状态检查扩展(StatusChecker)：检查服务依赖各种资源的状态，此状态检查可同时用于telnet的status命令和hosting的status页面 容器扩展(Container)：服务容器扩展，用于自定义加载内容 缓存扩展(CacheFactory)：用于请求参数作为Key，缓存返回结果 验证扩展(Validation)：参数验证扩展点 日志适配扩展(LoggerAdapter)：日志输出适配扩展点      2. 详细了解Dubbo SPI a. SPI注解  使用@SPI仅能配置一种实现类，但是可以根据@SPI(\u0026quot;dubbo\u0026quot;)指定不同的实现类，再调用ExtensionLoader获取实现类的时候，会加载配置文件中key所对应的实现类。 多个模块编译后会将META-INF/dubbo/或META-INFO/dubbo/internal/下的统一拓展点的配置文件中的配置合并在一起。  b. Activate注解  被@Activate注解注释的扩展点默认被激活启用，还可以通过注解的value属性指定此扩展点在什么情况下被自动激活 被@Activate注解注释的扩展点，在获取实现类是不能再使用ExtensionLoader.getExtension(String name)，而应该使用ExtensionLoader.getActivateExtension(URL url, String[] values) 目的也是为了支持一个扩展点存在多个实现类需要同时启用的场景。 主要是用扩展点(接口)的实现类上，所以此扩展点被@Activate注解的实现类都会在指定条件下自动激活  c. Adaptive注解  @Adaptive注解是Dubbo自适应拓展机制，最重要的三个字应该就是自适应 目的： 拓展点在方法被调用的时候，根据运行时参数进行加载 实现： Dubbo使用javassist为拓展接口生成具有代理功能的代码，然后通过JDK编译这段代码得到Class类，最后在通过反射创建代理类。 应用：  在RPC远程调用过程中，会在url上携带参数，比如调用的目标是XxxService的yy()，而服务提供者XxxService具有多个实现类，那么可以在url上指定使用哪个实现类(配置文件中该实现类的key)，然后在通过SPI获取到此实现类的实例。 javassist生成的代码，就是拿到方法的url参数，从url中动态获取配置的参数，然后通过SPI加载具体的实现类，最后调用实现类的方法 所以，先判断方法url参数是否为Null，如果为Null则抛出异常，否则从url中获取参数，如果没有获取到也抛出异常，如果获取到就通过SPI获取实例。    ","date":"2020-12-25T15:36:27+08:00","permalink":"https://example.com/p/dubbo-2.-dubbo%E4%B8%AD%E7%9A%84spi/","title":"[ Dubbo ] 2. Dubbo中的SPI"},{"content":"Dubbo SPI中的Protocol扩展点 ","date":"2020-12-25T15:36:27+08:00","permalink":"https://example.com/p/dubbo-3-1.-dubbo-spi%E4%B8%AD%E7%9A%84protocol%E6%89%A9%E5%B1%95%E7%82%B9/","title":"[ Dubbo ] 3-1. Dubbo SPI中的Protocol扩展点"},{"content":"Dubbo SPI中的Filter扩展点 ","date":"2020-12-25T15:36:27+08:00","permalink":"https://example.com/p/dubbo-3-2.-dubbo-spi%E4%B8%AD%E7%9A%84filter%E6%89%A9%E5%B1%95%E7%82%B9/","title":"[ Dubbo ] 3-2. Dubbo SPI中的Filter扩展点"},{"content":"Dubbo通信原理 1. Dubbo多线程通信原理  获取DubboInvoker对象； 将请求体信息封装在一个Request对象中，Request中会包括一个自增的id； 然后将Request存到一个ConcurrentHashMap中（key=id，value= DefaultFuture）,将request数据写入Channel Consumer Thread执行Defaultfuture#get()方法等待返回结果 服务提供方创建多线程处理用户请求，并将放回结果封装在Response中（包括Request#id）将response写入Channel 消费方从Channel中收到数据以后，解析出id，从Map中解析出DefaultFuture唤醒Consumer Thread，返回结果 DefaultFuture也会启动一个定时程序，检查在timeout内，结果是否返回，如果未返回，将DefaultFuture从map中移除，并抛出超时异常  ","date":"2020-12-25T15:36:27+08:00","permalink":"https://example.com/p/dubbo-3.-dubbo%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86/","title":"[ Dubbo ] 3. Dubbo通信原理"},{"content":"Dubbo基础概念 1.Dubbo核心组件  Provider： 暴露服务的服务提供方 Consumer： 调用远程服务的消费方 Register： 服务注册与发现注册中心 Monitor： 监控中心和访问调用统计 Container：服务运行时容器   Dubbo分层主要为业务层、RPC层和Remote层，如果把每层进行详细划分的话，整体划分为：\n  业务层：  service: 包含各业务代码的接口与实现；   RPC层：  config: 配置层，主要围绕ServiceConfig(暴露的服务配置)和ReferenceConfig(引用的服务配置)两个类展开，初始化配置信息； proxy: 服务代理层，不论生产者还是消费者，Dubbo都会生成一个代理类，在调用远程接口时，就可以像本地接口一样，代理层自动做远程调用并返回结果； registry: 注册层，负责Dubbo框架的服务注册与发现； cluster: 集群容错层，主要负责远程调用失败时的集群容错策略(如快速失败、快速重试等)； monitor: 监控层，负责监控统计调用次数和调用时间等； protocol: 远程调用层，封装RPC调用具体过程，是Invoker暴露和引用的主要功能入口，负责管理Invoker的整个生命周期；   Remote层：  exchange: 信息交换层，封装请求相应模式，如同步请求转换为异步请求； transport: 网络传输层，把网络传输抽象为统一接口； serialize: 序列化层，将需要网络传输的数据极性序列化，转为二进制流。    2.Dubbo服务注册与发现流程  Container负责启动，加载，运行服务提供者 Provider启动时，向注册中心注册自己并提供服务 Consumer启动时，向注册中心订阅自已需调用服务 Register返回服务提供者地址列表给服务消费者，如运行期间，服务提供者发生变动，将通过长连接推送至服务消费者 Consumer通过负载均衡算法(软方式)，选取注册中心所返回的服务提供者列表中的一个节点进行调用，如果调用失败将尝试其他节点进行调用 Consumer、Provider将调用次数、时间记录于内存中，并定时每分钟发送至Monitor监控中心  3-1. Dubbo服务暴露过程  Dubbo 会在 Spring 实例化完 bean 之后， 在刷新容器最后一步发布 ContextRefreshEvent 事件的时候，通知实现了 ApplicationListener 的 ServiceBean 类进行回调 onApplicationEvent 事件方法。 Dubbo 会在这个方法中调用 ServiceBean 父类 ServiceConfig 的 export 方法，而该方法真正实现了服务的发布。  3-2. Dubbo服务引用过程  服务暴露之后，客户端就要引用服务，然后才是调用的过程。 首先客户端根据配置文件信息从注册中心订阅服务 之后DubboProtocol根据订阅的得到provider地址和接口信息连接到服务端server，开启客户端client，然后创建invoker invoker创建完成之后，通过invoker为服务接口生成代理对象，这个代理对象用于远程调用provider，服务的引用就完成了  4. Dubbo的管理控制台能做什么  路由规则 动态配置 服务降级 访问控制 权重调整 负载均衡等管理功能  5. Dubbo集群容错方案  Failover Cluster(默认方案)  失败后自动切换，出现失败将会重试其他服务节点(retries重试次数) 通常用于读操作，但重试会带来很重的延迟   Failfast Cluster  快速失败 只发起一次调用，失败后立即抛出异常 用于写操作   Failsafe Cluster  失败安全机制 出现异常时，将进行忽略 通常用于写入审计日志   Failback Cluster  失败自动恢复 后台记录下失败请求，通过定时回调进行重试 常用于消息通知   Forking Cluster  并行调用 只要存在一个成功就返回 通常用于实时性较高的读操作，但浪费资源 可通过设定forks的值为2，限制最大并行数   Broadcast Cluster  广播调用 任意一台报错则报错，通常用于通知所有提供者更新缓存或者日志等资源。    6. Dubbo支持什么协议  dubbo：单一长连接、NIO异步通讯，适用于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况 hessian：短连接，http，适合页面传输、文件传输。与原生hessian服务互操作 http：适用于同时给应用程序和浏览器使用时 webservice：适用于系统集成，跨语言使用 rmi：适用于常规的远程服务方法调用，与原生RMI服务互操作  7. Dubbo如何做负载均衡 \u0026lt;dubbo:service interface=\u0026#34;...\u0026#34;loadbalance=\u0026#34;random\u0026#34;/\u0026gt; \u0026lt;dubbo:reference interface=\u0026#34;...\u0026#34;loadbalance=\u0026#34; random \u0026#34;/\u0026gt;  RandomLoadBalance: 随机负载均衡(默认方式) RoundRobinLoadBalance: 轮询负载均衡 LeastActiveLoadBalance: 最少活跃调用数(相同活跃数的随机)  活跃数指调用前后计数差 使响应慢的Provider收到更少的请求，因为越慢的provider的调用前后计数差越大   ConsistentHashLoadBalance: 一致性Hash负载均衡，相同参数的请求总是落在同一台机器上  8. Dubbo如何实现异步调用  API注入时添加异步调用标识  @Reference(interfaceClass=xxx.class,async-true)   启动类开启异步调用  @EnableAsync   异步调用接口添加异步调用代码  RpcContext.getContext.future()    9. 在Provider上可以配置Consumer端的属性有哪些  timeout：调用超时 retries：重试次数(默认是2次) LoadBalance：负载均衡算法 actives：消费者端最大并发调用限制  \u0026lt;dubbo:service interface=\u0026#34;com.alibaba.hello.api.HelloService\u0026#34; version=\u0026#34;1.0.0\u0026#34; ref=\u0026#34;helloService\u0026#34; timeout=\u0026#34;300\u0026#34; retry=\u0026#34;2\u0026#34; loadbalance=\u0026#34;random\u0026#34; actives=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;dubbo:service interface=\u0026#34;com.alibaba.hello.api.WorldService\u0026#34; version=\u0026#34;1.0.0\u0026#34; ref=\u0026#34;helloService\u0026#34; timeout=\u0026#34;300\u0026#34; retry=\u0026#34;2\u0026#34; loadbalance=\u0026#34;random\u0026#34; actives=\u0026#34;0\u0026#34; \u0026gt; \u0026lt;dubbo:method name=\u0026#34;findAllPerson\u0026#34; timeout=\u0026#34;10000\u0026#34; retries=\u0026#34;9\u0026#34; loadbalance=\u0026#34;leastactive\u0026#34; actives=\u0026#34;5\u0026#34; /\u0026gt; \u0026lt;dubbo:service/\u0026gt; 10. Zookeeper和Dubbo的关系  Dubbo将注册中心进行抽象，使得它可以外接不同的存储媒介给注册中心提供服务 使用Zookeeper作为存储媒介   负载均衡：\n 单注册中心的承载能力是有限的，在流量达到一定程度的时候就需要分流，负载均衡就是为了分流而存在的，一个 ZooKeeper 群配合相应的 Web 应用就可以很容易达到负载均衡    资源同步：\n 节点之间的数据和资源需要同步，ZooKeeper集群就天然具备有这样的功能    命名服务：\n 通过树状结构用于维护全局的服务地址列表 服务提供者在启动的时候，向 ZooKeeper上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这样就可以完成服务发布    Master选举：\n  分布式锁：\n 独占锁  即一次只能有一个线程使用资源   共享锁  读锁共享，读写互斥，即可以有多线线程同时读同一个资源，如果要使用写锁也只能有一个线程使用        11. Dubbo分层   分为三层：\n business层：(业务逻辑层)自己提供接口进行实现、以及一些配置 RPC层：(RPC调用核心层)封装了调用过程、负载均衡、集群容错、代理功能 remoting层：对网络传输协议和数据转换的封装    分为十层：\n service：业务逻辑层 config：配置层，初始化配置信息 proxy：代理层，为Provider和Consumer提供代理 register：服务注册层，封装服务注册与发现 cluster：路由层，封装provider路由与负载均衡 monitor：监控统计层，提供rpc调用时间、次数监控与统计 protocol：远程调用层，封装rpc调用 exchange：信息交换层，封装请求响应模式，同步转异步 transport：网络传输层，对Netty的封装 serialize：序列化层，对数据进行序列化、反序列化    x. Dubbo项目结构 $ tree -L 1 . ├── dubbo-all ├── dubbo-bom ├── dubbo-build-tools ├── dubbo-cluster # 集群容错模块，涵盖负载均衡策略、集群容错策略及路由 ├── dubbo-common # 通用逻辑模块，提供工具类和通用类型 ├── dubbo-compatible # 兼容性模块 ├── dubbo-config # 配置模块，主要实现API配置、属性配置、XML配置等 ├── dubbo-configcenter # 动态配置中心模块 ├── dubbo-container # 容器运行时，采用Main方法加载Spring容器 ├── dubbo-demo # 三种远程调用方式实例 ├── dubbo-dependencies #  ├── dubbo-dependencies-bom ├── dubbo-distribution ├── dubbo-filter # 过滤器 ├── dubbo-metadata # 元数据信息 ├── dubbo-monitor # 监控模块，主要监控接口调用次数及时间等信息 ├── dubbo-plugin # 插件拓展模块 ├── dubbo-registry # 服务发现与注册中心模块 ├── dubbo-remoting # 远程通信模块，为消费者、生产者间提供远程调用能力 ├── dubbo-rpc # 抽象各种通信协议以及动态代理(易混淆remoting) ├── dubbo-serialization ","date":"2020-12-22T15:36:27+08:00","permalink":"https://example.com/p/dubbo-1.dubbo%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/","title":"[ Dubbo ] 1.Dubbo基础概念"},{"content":"深入Service  Service是Kubernetes最为核心的概念。Service可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发至后端的各个容器应用上。\n 1.Service参数定义 # 必填，版本号apiVersion:v1# 必填kind:Service# 必填，元数据metadata:# 必填，Service名称(符合RFC 1035规范)name:string# 必填，命名空间(默认default)namespace:string# 自定义标签属性列表labels:- name:string# 自定义注解属性列表annotations:- name:string# 必填，配置内容详细描述 spec:# 必填，LabelSelector配置，将选择具有特定Label标签的Pod对象作为管理对象selector:[]# 必填，可选值[ClusterIP | NodePort | LoadBalancer]## ClusterIP:虚拟服务IP地址，该地址用于Kubernetes集群内部Pod对象访问，#在Node节点上Kube-proxy通过设置的Iptables规则进行转发## NodePort:使用宿主机端口，使能够访问各Node的外部客户端通过Node的#IP地址和端口号即可访问到应用。## LoadBalancer:使用外接负载均衡器完成到服务的负载分发，需要#spec.status.loadBalaner字段指定外部负载均衡器的IP地址，并同时定义#nodePort和clusterIP，用于公有云环境。type:string# 虚拟服务IP地址：当type为ClusterIP时，如果不指定将自动进行分配；也可手动指定。#当type为LoadBalancer时，必须指定clusterIP:string# 是否支持Session，可选值为ClientIP，默认值为空。#ClientIP表示将同一个客户端(有客户端IP地址决定)的访问请求都转发到同一个后端#Pod对象sessionAffinity:string# Service需要暴露的端口列表ports:- name:string# 端口协议：TCP/UDP(默认TCP)protocol:string# 服务监听端口号port:int# 需要转发到后端Pod对象的端口号targetPort:int# type为NodePort时，指定映射到物理机的端口号nodePort:int# type为LoadBalancer时，设置外部负载均衡器的地址，用于公有云环境。status:# 外部负载均衡器loadBalancer:ingress:ip:stringhostname:string","date":"2020-09-18T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-3-1.%E6%B7%B1%E5%85%A5service/","title":"[ Kubernetes ] 3-1.深入Service"},{"content":"了解Pod对象 1.Pod参数定义 # 必填,版本号apiVersion:stringkind:Pod# 必填,元数据metadata:# 必填,Pod对象的名称(命名规范需要符合RFC 1035规范)name:string# 必填,Pod对象所属的命名空间,默认值为defaultnamespace:string# 自定义标签列表(取值类型:List)labels:- name:string# 自定义标签注解(取值类型:List)annotations:- name:string# 必填,Pod对象中容器的详细定义 spec:# 必填,Pod对象容器列表(取值类型:List)containers:# 必填,容器的名称(需要符合RFC 1035规范)- name:string# 必填,容器镜像名称image:string# 获取镜像的策略，默认值为:Always# Always: 每次都尝试重新下载镜像# Never: 仅使用本地镜像# IfNotPresent: 如果本地不存在，就下载镜像imagePullPolicy:[Always | Never | IfNotPresent]# 容器启动命令列表，若不指定则使用镜像打包时使用的启动命令command:[string]# 容器的启动命令参数列表args:[string]# 容器的工作目录workingDir:string# 挂载到容器内部的存储卷配置(取值类型:List)volumeMounts:# 引用Pod定义的共享存储卷的名称，需使用镜像volumes[]部分定义的共享卷名称- name:string# 存储卷在容器内Mount的绝对路径(应少于512个字符)mountPath:string# 是否只读模式,默认false(读写模式)readOnly:boolean# 容器需要暴露的端口号(取值类型:List) ports:# 端口的名称- name:string# 容器需要监听的端口号containerPort:int# 容器所在主机需要监听的端口号，默认与containerPort相同# (设置hostPort时，同一台宿主机将无法启动该容器的第二副本，由于端口占用问题)hostPort:int# 端口协议[TCP/UDP],默认为TCPprotocol:string# 容器运行前需要设置的环境变量列表env:# 环境变量的名称- name:string# 环境变量的值value:string# 资源限制和资源请求的设置resource:# 资源限制设置limits:# CPU限制(单位为：core)将用于docker run --cpu-shares参数cpu:string# 内存限制(单位为：MiB/GiB)将用于docker run --memory参数memory:string# 资源限制设置(请求)requests:# CPU请求(单位为：core)将用于容器启动的初始化可用数量cpu:string# 内存请求(单位为：MiB/GiB)将用于容器启动的初始化可用数量memory:string# 对Pod对象内各个容器进行安全检查的设置，当探测无响应几次后，将自动重启该容器# 包含[exec | httpGet | TcpSocket]三种方式，任选其一即可livenessProbe:exec:# 需要执行的脚本command:[string]httpGet:# 请求路径path:string# 请求端口port:numberhost:stringscheme:stringhttpHeader:- name:stringvalue:stringtcpSocket:port:number# 完成容器启动后首次进行探测的时间(单位为：s)initialDelaySeconds:0# 对容器健康检查探测等待超时时间(单位为：s)，默认值为1timeoutSeconds:0# 对容器健康检查的探测时间周期(单位为：s)，默认值为10periodSeconds:0successThreshold:0failureThreshold:0securityContext:privileged:boolean## Pod对象的重启策略，可选值[Always | Never | OnFailure]## Always: Pod对象一旦终止，则不关心容器是如何停止的，kubelet都将重器容器## Never: Pod对象终止后，kubelet将退出码返回给Master，不再重启该容器## OnFailure: 只有当Pod对象以非零退出码终止时，kubelet才会重启该容器# (容器正常结束的退出码为零)#restartPolicy:[Always | Never | OnFailure]# 表示将Pod对象调度到包含这些label的Node上(以key:value形式指定)nodeSelector:object# Pull镜像时使用的secret名称(以name:secretValue形式指定)imagePullSecrets:- name:string# 是否使用主机模式(默认值为:false)## 如果设置为true，表示容器使用宿主机网络，不再使用Docker网桥# 该Pod对象将无法在同一台宿主机上启动第二个副本hostNetwork:boolean# 在该Pod对象上定义的共享储存卷列表volumes:# 共享储存卷名称，一个Pod对象中每个储存卷定义一个名称(命名应按照RFC 1035规范)- name:string# Pod对象同生命周期的一个临时目录，值为{}空对象emptyDir:{}# 挂载Pod对象所在宿主机的目录hostPath:# 将用于容器中mount的目录path:string# 挂载集群中预定义的secret对象到容器内部secret:secretName:stringitems:- key:stringpath:string# 挂载集群预定义的configMap对象到容器内部configMap:name:stringitems:- key:stringpath:string2.Pod的基本用法 Pod对象可以由1个或者多个容器组合而成。当两个或者多个容器应用为紧耦合关系，应该组合成一个整体对外提供服务，即将这两个容器打包为一个Pod对象。\n2.1 静态Pod对象  静态Pod对象是由Kubelet进行管理的仅存在于特定Node上的Pod对象。他们不可以通过API Server进行管理，无法与ReplicationController、Deployment或者DaemonSet进行关联，并且kubelet也无法对他们进行健康检查。静态Pod对象总是由kubelet进行创建，并且总是运行在kubelet所在的Node上运行。\n 创建静态Pod对象的两种方式：\n  配置文件\na.设置kubelet的启动参数**[\u0026ndash;config]**: 指定kubelet需要监控的配置文件所在的目录，kubelet将会定期扫描该目录，并根据该目录下的*.yaml或者*.json进行创建. b.重启kubelet服务\n  HTTP方式\n设置kubelet启动参数**[\u0026ndash;manifest-url]**: kubelet将会定期请求此URL下载Pod对象的定义文件，并以*.yaml或*.json文件格式解析，创建Pod对象。\n  2.2 Pod对象容器共享卷  在同一个Pod对象中的多个容器能够共享Pod对象级别的存储卷。Volume可以被定义为各种类型，容器各自进行挂载操作，将一个Volume挂载为容器内容存储卷。\n 配置文件示例：\napiVersion:v1kind:Podmetadata:name:volume-podspec:containers:- name:tomcatimage:tomcatports:- containerPort:8080volumeMounts:- name:app-logsmountPath:/usr/local/tomcat/log- name:logreaderimage:busyboxcommand:[\u0026#34;sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;tail -f /logs/catalina*.log\u0026#34;]volumeMounts:- name:app-logsmountPath:/logsvolumes:app-logs - name:app-logsemptyDir:{}2.3 Pod对象的配置管理  为了提高应用部署的复用能力以及灵活性，可以将应用所需要的配置文件与程序进行分离。将应用打包为容器镜像后，可以通过环境变量配置、挂载外部文件的方式在创建容器时进行配置注入，但唯一的缺点维护性与复杂性将会在大规模容器集群中所体现。但在Kubernetes中可以通过ConfigMap进行管理。\n 1）ConfigMap的概念\n 生成为容器内的环境变量 设置容器的启动命令参数 通过Volume的形式挂载到容器内部  ConfigMap以一个或者多个[Key:Value]的形式保存在Kubernetes系统中。可以通过*.yaml配置文件或者kubelet create [-f configmap.yaml]命令进行创建配置管理内容。\n2）创建ConfigMap资源对象\na) *.yaml实例\napiVersion:v1kind:ConfigMapmetadata:name:cm-appvarsdata:apploglevel:infoappdatadir:/var/data b) kubelet命令\n# 1.创建configmap.yaml配置文件 kubelet create -f cm-appvars.yaml \u0026gt; configmap \u0026#34;cm-appvars\u0026#34; created # 2.查看创建完成的配置文件 kubelet get configmap # NAME DATA AGE # cm-appvars 2 3s # 3.查看指定配置的详细内容 kubelet get configmap cm-appvars -o yaml ","date":"2020-09-17T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-2-1.%E4%BA%86%E8%A7%A3pod%E5%AF%B9%E8%B1%A1/","title":"[ Kubernetes ] 2-1.了解Pod对象"},{"content":"SpringBoot\u0026ndash;Pods项目初体验  在SpringBoot项目中通过fabric8打包插件构建docker镜像\n 通过Kubernetes的接口请求Pod对象，相信信息如下：\n{ \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kubernetes-hello-world-779c4c748b-2rv27\u0026#34;, \u0026#34;generateName\u0026#34;: \u0026#34;kubernetes-hello-world-779c4c748b-\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/api/v1/namespaces/default/pods/kubernetes-hello-world-779c4c748b-2rv27\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;dea46f9f-cd4b-11e9-b38e-025000000001\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;34209\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-02T06:35:35Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;kubernetes-hello-world\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;org.springframework.cloud\u0026#34;, \u0026#34;pod-template-hash\u0026#34;: \u0026#34;779c4c748b\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;fabric8\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.1.0.M2\u0026#34; }, \u0026#34;annotations\u0026#34;: { \u0026#34;fabric8.io/docs-url\u0026#34;: \u0026#34;scp://static.springframework.org/var/www/domains/springframework.org/static/htdocs/spring-cloud/docs/kubernetes-hello-world/1.1.0.M2/spring-cloud-kubernetes/spring-cloud-kubernetes-examples/kubernetes-hello-world\u0026#34;, \u0026#34;fabric8.io/iconUrl\u0026#34;: \u0026#34;img/icons/spring-boot.svg\u0026#34;, \u0026#34;fabric8.io/metrics-path\u0026#34;: \u0026#34;dashboard/file/kubernetes-pods.json/?var-project=kubernetes-hello-world\\u0026var-version=1.1.0.M2\u0026#34;, \u0026#34;fabric8.io/scm-con-url\u0026#34;: \u0026#34;scm:git:git://github.com/spring-cloud-incubator/spring-cloud-kubernetes.git/spring-cloud-kubernetes-examples/kubernetes-hello-world\u0026#34;, \u0026#34;fabric8.io/scm-devcon-url\u0026#34;: \u0026#34;scm:git:ssh://git@github.com/spring-cloud-incubator/spring-cloud-kubernetes.git/spring-cloud-kubernetes-examples/kubernetes-hello-world\u0026#34;, \u0026#34;fabric8.io/scm-tag\u0026#34;: \u0026#34;HEAD\u0026#34;, \u0026#34;fabric8.io/scm-url\u0026#34;: \u0026#34;https://github.com/spring-cloud-incubator/spring-cloud-kubernetes/spring-cloud-kubernetes-examples/kubernetes-hello-world\u0026#34; }, \u0026#34;ownerReferences\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;apps/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ReplicaSet\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;kubernetes-hello-world-779c4c748b\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;dea3c6a5-cd4b-11e9-b38e-025000000001\u0026#34;, \u0026#34;controller\u0026#34;: true, \u0026#34;blockOwnerDeletion\u0026#34;: true } ] }, \u0026#34;spec\u0026#34;: { \u0026#34;volumes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default-token-b4crd\u0026#34;, \u0026#34;secret\u0026#34;: { \u0026#34;secretName\u0026#34;: \u0026#34;default-token-b4crd\u0026#34;, \u0026#34;defaultMode\u0026#34;: 420 } } ], \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;spring-boot\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;cloud/kubernetes-hello-world:1.1.0.M2\u0026#34;, \u0026#34;ports\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;containerPort\u0026#34;: 8080, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;containerPort\u0026#34;: 9779, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;jolokia\u0026#34;, \u0026#34;containerPort\u0026#34;: 8778, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; } ], \u0026#34;env\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;KUBERNETES_NAMESPACE\u0026#34;, \u0026#34;valueFrom\u0026#34;: { \u0026#34;fieldRef\u0026#34;: { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldPath\u0026#34;: \u0026#34;metadata.namespace\u0026#34; } } } ], \u0026#34;resources\u0026#34;: { }, \u0026#34;volumeMounts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default-token-b4crd\u0026#34;, \u0026#34;readOnly\u0026#34;: true, \u0026#34;mountPath\u0026#34;: \u0026#34;/var/run/secrets/kubernetes.io/serviceaccount\u0026#34; } ], \u0026#34;livenessProbe\u0026#34;: { \u0026#34;httpGet\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;port\u0026#34;: 8080, \u0026#34;scheme\u0026#34;: \u0026#34;HTTP\u0026#34; }, \u0026#34;initialDelaySeconds\u0026#34;: 180, \u0026#34;timeoutSeconds\u0026#34;: 1, \u0026#34;periodSeconds\u0026#34;: 10, \u0026#34;successThreshold\u0026#34;: 1, \u0026#34;failureThreshold\u0026#34;: 3 }, \u0026#34;readinessProbe\u0026#34;: { \u0026#34;httpGet\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;port\u0026#34;: 8080, \u0026#34;scheme\u0026#34;: \u0026#34;HTTP\u0026#34; }, \u0026#34;initialDelaySeconds\u0026#34;: 10, \u0026#34;timeoutSeconds\u0026#34;: 1, \u0026#34;periodSeconds\u0026#34;: 10, \u0026#34;successThreshold\u0026#34;: 1, \u0026#34;failureThreshold\u0026#34;: 3 }, \u0026#34;terminationMessagePath\u0026#34;: \u0026#34;/dev/termination-log\u0026#34;, \u0026#34;terminationMessagePolicy\u0026#34;: \u0026#34;File\u0026#34;, \u0026#34;imagePullPolicy\u0026#34;: \u0026#34;IfNotPresent\u0026#34;, \u0026#34;securityContext\u0026#34;: { \u0026#34;privileged\u0026#34;: false, \u0026#34;procMount\u0026#34;: \u0026#34;Default\u0026#34; } } ], \u0026#34;restartPolicy\u0026#34;: \u0026#34;Always\u0026#34;, \u0026#34;terminationGracePeriodSeconds\u0026#34;: 30, \u0026#34;dnsPolicy\u0026#34;: \u0026#34;ClusterFirst\u0026#34;, \u0026#34;serviceAccountName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;serviceAccount\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;nodeName\u0026#34;: \u0026#34;docker-desktop\u0026#34;, \u0026#34;securityContext\u0026#34;: { }, \u0026#34;schedulerName\u0026#34;: \u0026#34;default-scheduler\u0026#34;, \u0026#34;tolerations\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;node.kubernetes.io/not-ready\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;Exists\u0026#34;, \u0026#34;effect\u0026#34;: \u0026#34;NoExecute\u0026#34;, \u0026#34;tolerationSeconds\u0026#34;: 300 }, { \u0026#34;key\u0026#34;: \u0026#34;node.kubernetes.io/unreachable\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;Exists\u0026#34;, \u0026#34;effect\u0026#34;: \u0026#34;NoExecute\u0026#34;, \u0026#34;tolerationSeconds\u0026#34;: 300 } ], \u0026#34;priority\u0026#34;: 0, \u0026#34;enableServiceLinks\u0026#34;: true }, \u0026#34;status\u0026#34;: { \u0026#34;phase\u0026#34;: \u0026#34;Running\u0026#34;, \u0026#34;conditions\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Initialized\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;lastProbeTime\u0026#34;: null, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2019-09-02T06:35:35Z\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;Ready\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;lastProbeTime\u0026#34;: null, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-01-17T01:47:07Z\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;ContainersReady\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;lastProbeTime\u0026#34;: null, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-01-17T01:47:07Z\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;PodScheduled\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;lastProbeTime\u0026#34;: null, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2019-09-02T06:35:35Z\u0026#34; } ], \u0026#34;hostIP\u0026#34;: \u0026#34;192.168.65.3\u0026#34;, \u0026#34;podIP\u0026#34;: \u0026#34;10.1.0.49\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2019-09-02T06:35:35Z\u0026#34;, \u0026#34;containerStatuses\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;spring-boot\u0026#34;, \u0026#34;state\u0026#34;: { \u0026#34;running\u0026#34;: { \u0026#34;startedAt\u0026#34;: \u0026#34;2020-01-17T01:46:50Z\u0026#34; } }, \u0026#34;lastState\u0026#34;: { \u0026#34;terminated\u0026#34;: { \u0026#34;exitCode\u0026#34;: 255, \u0026#34;reason\u0026#34;: \u0026#34;Error\u0026#34;, \u0026#34;startedAt\u0026#34;: \u0026#34;2020-01-16T09:29:52Z\u0026#34;, \u0026#34;finishedAt\u0026#34;: \u0026#34;2020-01-17T01:46:30Z\u0026#34;, \u0026#34;containerID\u0026#34;: \u0026#34;docker://ec8892c9949c0f226636d68c0f0f9b675b60693011d510f3a218e579fe43f992\u0026#34; } }, \u0026#34;ready\u0026#34;: true, \u0026#34;restartCount\u0026#34;: 8, \u0026#34;image\u0026#34;: \u0026#34;cloud/kubernetes-hello-world:1.1.0.M2\u0026#34;, \u0026#34;imageID\u0026#34;: \u0026#34;docker://sha256:559a7fbc5858f9b3ca2bb9ada10235dbc169846e988a46a80c8ab123560ea168\u0026#34;, \u0026#34;containerID\u0026#34;: \u0026#34;docker://af0de58af170361c986110e79eeca49b85c0eef23ae688f52b23b5e95111fbe0\u0026#34; } ], \u0026#34;qosClass\u0026#34;: \u0026#34;BestEffort\u0026#34; } } ","date":"2020-09-17T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-2-2.springboot-pods%E9%A1%B9%E7%9B%AE%E5%88%9D%E4%BD%93%E9%AA%8C/","title":"[ Kubernetes ] 2-2.SpringBoot--Pods项目初体验"},{"content":"安装初始化K8s集群 1.安装K8s 1.1CentOS安装   预先准备工作\n# 修改设置主机名称 hostnamectl set-hostname master # 绑定主机各节点hosts 192.168.0.1 master 192.168.0.2 node1 192.168.0.3 node2 # 验证每节点的Mac地址与UUID是否唯一 # mac地址注意查看网卡 cat /sys/class/net/eth1/address cat /sys/class/dmi/id/product_uuid # 关闭缓存交换swap swapoff -a # 临时关闭 sed -i.bak \u0026#39;/swap/s/^/#/\u0026#39; /etc/fstab #永久关闭   安装Kubernetes\n# 设置K8s安装源，由于防火墙问题使用阿里云源 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo   [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 更新源缓存 yum clean all yum -y makecache\n# 查看k8s版本 yum list kubelet --showduplicates | sort -r # 默认安装最新版本 yum install -y kubelet kubeadm kubectl # 选择指定版本进行安装 yum install -y kubelet-\u0026lt;version\u0026gt; kubeadm-\u0026lt;version\u0026gt; kubectl-\u0026lt;version\u0026gt; ```  1.2MacOS安装 Docker-Desktop版内置(需要访问科学上网) 2.准备Kubernetes依赖镜像 k8s.gcr.io/kube-apiserver:v1.17.1 k8s.gcr.io/kube-controller-manager:v1.17.1 k8s.gcr.io/kube-proxy:v1.17.1 k8s.gcr.io/kube-scheduler:v1.17.1 k8s.gcr.io/coredns:v1.17.1 k8s.gcr.io/etcd:v1.17.1 由于国外站点问题，需要科学上网，或者通过其他镜像仓库拉去，然后通过docker tag打标签的形式保存在docker仓库\n# 通过科学上网拉去官网仓库镜像 docker pull k8s.gcr.io/kube-apiserver:v1.17.1 ... k8s.gcr.io/etcd:v1.17.1 ## 然后通过本地惊醒仓库打包导出tar的形式 docker save k8s.gcr.io/kube-apiserver:v1.17.1 \u0026gt; k8s.tar.gz ## 导入目标服务器的本地镜像仓库 docker load \u0026lt; k8s.tar.gz 3.使用kubeadm初始化集群主节点   初始化主节点\nkubeadm init –apiserver-advertise-address=192.168.66.176 --kubernetes-version=v1.17.1 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap # 增加Kubernetes本地全局变量配置 ### 非root用户 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ### root用户 echo \u0026#34;export KUBECONFIG=/etc/kubernetes/admin.conf\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile source .bash_profile   初始化主节点网络\n# 使用flannel配置网络 kubectl apply -f kube-flannel.yml   关于节点污点问题\n taint:污点的意思.如果某节点设置为污点，那么pod将不允许在此节点上运行。\n # 查看污点信息 kubectl describe node master|grep -i taints # 删除默认污点 kubectl taint nodes master node-role.kubernetes.io/master- # 设置污点 kubectl taint node master key1=value1:NoSchedule # 删除污点 kubectl taint nodes master key1- ### 关于污点语法 kubectl taint node [node] key=value[effect] 其中[effect] 可取值: [ NoSchedule | PreferNoSchedule | NoExecute ] NoSchedule: 一定不能被调度 PreferNoSchedule: 尽量不要调度 NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod   Node节点加入集群\n  查看令牌\nkubeadm token list 如果令牌过期可以重新生成令牌\n  初始化令牌\nkubeadm token create   生成新的加密串\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39;   node加入master\nkubeadm join 192.168.66.175:6443 --token uirohl.1auw4f6ebu1c1etc \\   \u0026ndash;discovery-token-ca-cert-hash sha256:f0d231c5a175c4f84d94cf0d7df2efc96e4ac396482ed9e04880a9d2c9b6a84e ```\n  集群移除Node节点\n### 已验证 # 设置节点不可调度 kubectl cordon ${Node} # 恢复节点调度 kubectl uncordon ${Node} # 驱逐节点上运行的业务容器 kubectl drain --ignore-daemonsets ${Node} # 移除节点 kubectl delete node ${Node} ### master剔除node（待验证） etcdctl --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/server.pem --key=/etc/etcd/pki/server-key.pem --endpoints=https://210.74.13.8:2379 del /registry --prefix   ","date":"2020-09-14T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-1-4.kubeadm%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/","title":"[ Kubernetes ] 1-4.kubeadm命令使用"},{"content":"Kubectl命令行工具 1.kubectl用法  $~: kubectl [command] [TYPE] [NAME] [flags]\n   [command] 子命令。用于操作Kubernetes集群资源对象。\n可取值：[create | delete | describe | get | apply]\n  [TYPE] 资源对象的类型。区分大小写\n备注：可以通过单数形式、复数形式、简写形式表示。\n# 例：不同写法的Type,但是效果一致 kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1   [NAME] 资源对象名称。区分大小写 备注： 如果不指定名称，将返回属于TYPE的所有对象列表。\n# 例：返回所有对象列表 kubectl get pods   [flags] kubectl子命令的可选参数\n  2.kubectl操作实例   创建资源对象\n# 由配置文件(*.yaml)创建一次性对象 # 创建一个对象 kubectl create -f service.yaml # 创建对个对象 kubectl create -f service.yaml -f pod.yaml   查看资源对象\n# 查看所有Pod列表 kubectl get pods # 查看指定对象 kubectl get service,pod   资源对象详情\n# 显示Node的详细信息 kubectl describe nodes node1 # 显示Pod的详细信息 kubectl describe pods/service # 显示由node1管理的pod对象 kubectl describe pods node1-service   删除资源对象\n# 基于配置文件(*.yaml)定义中名称的Pod对象 kubectl delete -f service.yaml # 删除包含指定label的所有Pod和Service对象 kubectl delete pods,services -l name=label-obj # 删除所有Pod对象 kubectl delete pods --all   运行资源对象\n# 指定Pod对象的date命令，默认情况下在Pod对象的第一个容器中执行 kubectl exec \u0026lt;pod-name\u0026gt; date # 指定Pod对象在某个特定容器中执行 kubectl exec \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; date # 通过bash获取Pod对象中特定容器的TTY(可以理解为登录容器) kubectl exec -ti \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt;   查看容器日志\n# 查看容器输出到stdout日志 kubectl logs \u0026lt;pod-name\u0026gt; # 跟踪查看容器日志(与tail -f命令具有相同效果) kubectl logs -f \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt;   ","date":"2020-09-12T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-1-2.kubectl%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/","title":"[ Kubernetes ] 1-2.Kubectl命令行工具"},{"content":"kubeadm命令使用 一、kubeadm概述 $~:kubeadm --help # kubeadm [command] |———— alpha [command] |———— completion |———— config |———— images |———— list 列出所有依赖镜像 |———— pull |———— help 查看命令详细描述 |———— init 初始化Kubernetes集群Master |———— join 在Kubernetes集群中增加Node |———— reset 重置Kubernetes集群 |———— token |———— upgrade |———— version ","date":"2020-09-12T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-1-3.kubeadm%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/","title":"[ Kubernetes ] 1-3.kubeadm命令使用"},{"content":"K8s之Helm包管理工具  Helm 是 Deis 开发的一个用于 Kubernetes 应用的包管理工具，主要用来管理 Charts。有点类似于 Ubuntu 中的 APT 或 CentOS 中的 YUM。\n 一、安装 Helm Release Link\n1.OS-CentOS 当前使用版本为Helm v3.1.2 linux\n# 下载二进制可执行文件压缩包 wget -O /data/helm.tar.gz https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz # 解压 tar -xzvf /data/helm.tar.gz # 移动helm二进制文件，方便全局访问 mv linux-amd64/helm /usr/local/bin/helm 2.OS-MacOS ①.自动安装 PS：操作系统已安装brew工具\nbrew install helm ②.手动安装 当前使用版本为Helm v3.1.2 darwin\n# 下载二进制可执行文件压缩包 wget -O ~/helm.tar.gz https://get.helm.sh/helm-v3.1.2-darwin-amd64.tar.gz # 解压缩 tar -xzvf ~/helm.tar.gz # 移动helm二进制文件，方便全局访问 mv darwin-amd64/helm /usr/local/bin/helm 二、入门 1.调整helm源 # 查看源 helm repo list # 设置国内镜像源(选用阿里云源) helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 2.搜索应用 搜索Nginx-Ingress\nhelm search repo nginx-ingress ### 搜索结果如下 ### # NAME CHART VERSION APP VERSION DESCRIPTION  #stable/nginx-ingress 0.9.5 0.10.2 An nginx Ingress controller that uses ConfigMap... #stable/nginx-lego 0.3.1 Chart for nginx-ingress-controller and kube-lego  3.安装应用 # 开启rbac权限，并通过externalIP方式进行工作 helm install --name nginx-ingress --set \u0026#34;rbac.create=true,controller.service.externalIPs[0]=192.168.100.211,controller.service.externalIPs[1]=192.168.100.212,controller.service.externalIPs[2]=192.168.100.213\u0026#34; stable/nginx-ingress ","date":"2020-09-12T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-1-5.k8s%E4%B9%8Bhelm%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/","title":"[ Kubernetes ] 1-5.K8s之Helm包管理工具"},{"content":"Kubernetes Kubernetes(K8s)，译文成为【舵手】。从官网的Logo可以看出是轮船上的舵。结合container(集装箱，容器)的概念，Kubernetes看起来则是管理这些容器的。是一个自动化的容器编排平台，负责应用的部署、应用的弹性以及应用的管理，前提则是这些应用都是基于容器的。\n核心功能   服务的发现与负载均衡\n附属组件KubeDNS为系统内置了服务发现功能，可以将每一个Service增加DNS名称，使得集群内节点直接通过此名称访问到；同时Service通过iptables、ipvs支持了负载均衡。\n  自动装箱\n构建于容器之上，基于资源依赖及其他约束在不影响其可用性的情况下自动完成容器的部署工作。\n  自我修复\n容器故障后自动重启、节点故障后自动重新进行容器调度、节点健康状态检查异常后会关闭容器进行重新创建。\n  水平扩展\n通过命令、UI手动水平扩展、基于CPU等资源负载率进行自动水平扩展。\n  自动发布与回滚\n使用灰度方式更新应用或其配置，过程中的应用健康状态将得到监控，以保证不在同一时刻kill掉所有实例；同时，过程中健康状态出现异常情况，将会立即自动执行回滚。\n  存储编排\nPod对象自动挂载不同类型的存储系统。\n  批量处理\n支持批处理作业、CI持续集成。\n  秘钥与配置管理\nK8s的configMap将配置与Docker镜像解耦，更新配置时，无需重新构建Docker镜像。同时，敏感数据将通过Secret对象进行解耦，以保障一定程度上的最大安全。\n  ","date":"2020-09-11T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-0.kubernetes/","title":"[ Kubernetes ] 0.Kubernetes"},{"content":"初识K8s 术语及原理   Master(主节点:control plane) 集群中的神经中枢网关。负责整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控、纠错等管理功能。\n  ApiServer\n集群的网关。\n 负责输出RESTful风格K8s接口，则是通往集群所有REST操作命令的入口，并负责接收、校验、相应所有的REST请求，最终结果状态存储在etcd中。\n   Controller Manager\n负责生命周期功能及API业务逻辑。\n **a.生命周期功能：**Namespace创建和生命周期、Event垃圾回收、Pod对象终止相关的垃圾回收、级联垃圾回收、Node的垃圾回收 **b.API业务逻辑：**由ReplicaSet执行的Pod对象扩展\n   Scheduler\n在API Server确认Pod对象之后，由调度器(Scheduler)根据集群中各节点的可用资源状态、目标运行容器的资源需求做出调度策略。\n  Etcd\n基于Raft协议开发的分布式键值存储，用于服务发现、共享配置、保证一致性(数据库的主从节点选择，分布式锁等)。\n a.etcd是独立的组件，并不属于K8s集群。 b.生产环境etcd应该按照集群方式部署运行，以提升高可用。\n     Node(从节点:worker node)\n工作节点。负责接收来自Master节点的工作指令并根据指令相应的创建或者销毁Pod对象，以及调整网络规则以合理的路由转发流量。\n  Pod\nKubernetes并不会直接运行容器，而是使用一个抽象的资源对象封装一个或者多个容器，此对象就是Pod对象。 是K8s最小的调度单元。 **一个Pod对象可以拥有多个Container容器应用。**通常情况下，这些在同一个Pod对象中的Container容器是高耦合。因为其共用同一个Pod对象下的网络名称空间、存储资源、UTS命名空间(同一个主机名称)、PID命名空间(不同应用程序可以看到其他应用程序的PID)\n  Pod Controller(Pod 控制器)\n虽说Pod对象是最小的调度单元，但实际应用中，并不会直接部署、管理Pod对象，而是借助Pod Controller对其进行管理。\n  Replication Controller(复制控制器)\nK8s的核心概念，用于管理Pod的声明周期。在主节点中，控制管理器进程同RC的定义完成Pod的创建、监控、启停等操作。\n  Replica Set\n保证在某个时间点儿上，一定数量的Pod对象在运行。是Replication Controller的升级版本。\n 主要区别在于Selector选择器 Replica Set:支持集合级别的选择器。 Replication Controller:支持在等号描述的选择器。\n   Deployment\n为保证指定的Pod对象的副本数量精确符合定义，否则将会按照「多退少补」原则进行自动管理。\n    Label(标签)\n将资源进行分类的标识符，一组附加在对象上的键值对类型数据。主要为了解决Pod对象与Service之间的关联关系。\n 一个对象可以拥有多个标签，一个标签也可以附加在多个对象。\n   Service\n建立在Pod对象之上的资源抽象，通过标签选择器选定一组Pod对象，并设定统一且固定的访问入口(通常情况下表现形式是IP地址)。\n 拥有唯一指定的名字 拥有一个虚拟IP地址和端口号 能够提供某种远程能力 被映射到提供服务能力的一组容器之上 Service的服务进程目前通过Socket方式对外提供服务  如果Kubernetes集群存在DNS附件，将会在Service对象创建时为其自动指定一个DNS名称用于客户端服务发现。\n  Volume(容器共享存储卷)\n独立于容器文件系统之外的存储空间，常用在扩展容器的存储空间并为其提供持久化存储能力。临时卷与本地卷一般位于Node本地，一旦Pod对象被调度至其他Node节点，此类型的存储卷将无法正常访问，所以此类存储卷用于数据缓存。持久数据将存放在Persistent Volume(持久卷)。\n Persistent Volume(持久卷) Persistent Volume Claims    Annotation\n另外一种附加在对象之上的键值类型数据，但拥有更大的数据量。常用于将各种**非标识型元数据(metadata)**附加到对象，但不能标识和选择对象。K8s将不会直接使用，仅当方便工具或用户查找等用途。\n a.build信息、release信息、Docker镜像信息等 b.日志库、监控库、分析库资源等资源库地址信息 c.程序调试工具信息 d.团队信息\n   Namespace\n使用Namespace来组织kubernetes的各种对象，可以实现用户的分组(多租户)，对不同的租户还可以进行单独的资源设置和管理，是的整个集群的资源配置非常灵活。\n a.在同一命名空间中，同一类型资源对象的名称必须具有唯一性 b.名称空间空间通常用于实现租户或项目的资源隔离，以达到逻辑分组目的。\n   Ingress\n解决Pod对象与外部网络隔离无法访问的组件。\n 由于K8s将Pod对象与外网进行隔离，同时Pod与Service等对象间的通信全是由K8s内部网络进行。\n     ","date":"2020-09-11T15:36:27+08:00","permalink":"https://example.com/p/kubernetes-1-1.%E5%88%9D%E8%AF%86k8s/","title":"[ Kubernetes ] 1-1.初识K8s"},{"content":"ETCD  其中ETCD是一个用于存储关键数据的键值存储，ZK是一个用于管理配置等信息的中心化服务 ETCD包括 Raft 协议、存储两大模块. etcd 的使用其实非常简单，它对外提供了 gRPC 接口，我们可以通过 Protobuf 和 gRPC 直接对 etcd 中存储的数据进行管理，也可以使用官方提供的 etcdctl 操作存储的数据。\n raft协议  每一个 Raft 集群中都包含多个服务器，在任意时刻，每一台服务器只可能处于 Leader、Follower 以及 Candidate 三种状态；在处于正常的状态时，集群中只会存在一个 Leader，其余的服务器都是 Follower。\n 节点选举  使用 Raft 协议的 etcd 集群在启动节点时，会遵循 Raft 协议的规则，所有节点一开始都被初始化为 Follower 状态，新加入的节点会在 NewNode 中做一些配置的初始化，包括用于接收各种信息的 Channel\n 竞选流程 如果集群中的某一个 Follower 节点长时间内没有收到来自 Leader 的心跳请求，当前节点就会通过 MsgHup 消息进入预选举或者选举的流程。 如果收到 MsgHup 消息的节点不是 Leader 状态，就会根据当前集群的配置选择进入 PreElection 或者 Election 阶段，PreElection 阶段并不会真正增加当前节点的 Term，它的主要作用是得到当前集群能否成功选举出一个 Leader 的答案，如果当前集群中只有两个节点而且没有预选举阶段，那么这两个节点的 Term 会无休止的增加，预选举阶段就是为了解决这一问题而出现的。 当前节点会立刻调用 becomeCandidate 将当前节点的 Raft 状态变成候选人；在这之后，它会将票投给自己，如果当前集群只有一个节点，该节点就会直接成为集群中的 Leader 节点。\n如果集群中存在了多个节点，就会向集群中的其他节点发出 MsgVote 消息，请求其他节点投票，在 Step 函数中包含不同状态的节点接收到消息时的响应 如果当前节点投的票就是消息的来源或者当前节点没有投票也没有 Leader，那么就会向来源的节点投票，否则就会通知该节点当前节点拒绝投票。 每当收到一个 MsgVoteResp 类型的消息时，就会设置当前节点持有的 votes 数组，更新其中存储的节点投票状态并返回投『同意』票的人数，如果获得的票数大于法定人数 quorum，当前节点就会成为集群的 Leader 并向其他的节点发送当前节点当选的消息，通知其余节点更新 Raft 结构体中的 Term 等信息\n存储\netcd 目前支持 V2 和 V3 两个大版本，这两个版本在实现上有比较大的不同，一方面是对外提供接口的方式，另一方面就是底层的存储引擎，V2 版本的实例是一个纯内存的实现，所有的数据都没有存储在磁盘上，而 V3 版本的实例就支持了数据的持久化。 在 V3 版本的设计中，etcd 通过 backend 后端这一设计，很好地封装了存储引擎的实现细节，为上层提供一个更一致的接口.\n","date":"2020-08-30T00:00:00Z","permalink":"https://example.com/p/etcd-1.%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A81/","title":"[ Etcd ] 1.基础入门(1)"},{"content":"Etcd源码阅读与分析①-raft demo Etcd 与 Zookeeper 对比  一致性协议:配置共享\u0026amp;服务发现组件的核心基础。  Zookeeper采用ZAB协议(一种类Paxos协议)实现一致性 Etcd采用Raft协议，相比Paxos协议更容易理解，工程化。   API接口: 包含有两个版本V2、V3  V2: 提供HTTP+Json方式调用 V3: 提供grpc方式调用   性能  官方测试数据显示：10000+/s写入(优于Zookeeper性能)   安全  Etcd支持TSL(权限控制优于Zookeeper)    Etcd是一个基于Raft协议的简单内存KV项目\n源码分析 本文档将以etcd作者在项目中所提供的demo程序进行源码试读。demo名称为raftexample。 路径在\n1.项目结构 (base) {11:45}~/etcd:master ✗ ➭ tree -d -L 1 . . ├── Documentation # 项目文档 ├── auth # 认证授权 ├── client # 客户端相关(v2) ├── clientv3 # 客户端相关(v3) ├── contrib # (待验证) ├── default.etcd # 已编译完成的etcd ├── embed # 封装的etcd函数 ├── etcdctl # etcd操作命令，命令行客户端 ├── etcdmain # main函数入口这里 ├── etcdserver # 服务端相关 ├── functional # 目测验证功能测试套件 ├── hack # 开发者相关 ├── integration # (待验证) ├── lease # 实现etcd租约 ├── logos # 日志相关 ├── mvcc # MVCC存储相关 ├── pkg # 通用依赖库 ├── proxy # 代理相关Http、Https、Socks ├── raft # raft一致性协议实现 ├── scripts # 各类脚本 ├── security # 安全相关 ├── tests # (待验证) ├── tools # 工具 ├── vendor # go vendor依赖环境 ├── version # 版本信息 └── wal # Write-Ahead-Log实现 ","date":"2020-08-30T00:00:00Z","permalink":"https://example.com/p/etcd-2.%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A82/","title":"[ Etcd ] 2.基础入门(2)"},{"content":"文件系统之UFS UFS 联合文件系统[Union File System]，把其他文件系统联合到一个联合挂载点的文件系统服务(适用于Linux、FreeBSD、NetBSD OS)。\n 原理： 使用branch把不同文件系统的文件、目录「透明的」进行覆盖，形成一个单一一直的文件系统。branch具有要么read-only,要么read-write的特点。\n  思想： 写时复制(copy-on-write),如果一个资源重复，但并未被修改，将不被立即创建出新的资源，直接为新旧实例提供共享。创建新资源将发生在第一次被修改写入时。该资源共享方式，可以明显降低未修改资源复制时的消耗，但同样的，也会在写入修改是增加部分开销。\n  AFUS(Advanced Multi-Layered Unification FileSystem)  ","date":"2020-08-23T15:36:27+08:00","permalink":"https://example.com/p/docker-8.%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B9%8Bufs/","title":"[ Docker ] 8.文件系统之UFS"},{"content":"关于Linux的Cgroups 概念  Linux Cgroups(Control Groups)在Linux Namespace为进程隔离出一定空间的基础上为此进行的资源限制、控制以及统计的能力。资源包含有：CPU、内存、存储、网络等。通过Cgroups可以限制某个进程的资源占用、并且可以实时监控进程以及统计信息。\n cgroups各模块   cgroup: 针对进程进行分组的一种策略机制。\n 每一个cgroup中包含有一组进程。并且可以使用subsystem模块进行参数控制作用于此cgroup上的进程。\n   subsystem: 此模块对资源进行控制。\n Ubuntu OS可以通过apt install cgroup-bin安装命令行工具，使用lssubsys查看Kernel所支持的subsystem list。\n  blkio: 设置对块设备输入输出进行控制 cpu: 设置cgroup中进程的CPU调度策略 cpuacct: 统计cgroup中进程CPU占用情况 cpuset: 在多核机器上设置cgroup中进程可以使用的CPU和内存(内存仅适用于NUMA架构) devices: 控制cgroup中进程对设备的访问 freezer: 用于挂起(suspend)和恢复(resume)cgroup中的进程 memory: 限制cgroup中进程的内存占用 net_cls: 将cgroup中进程的网络包进行分类，以至于通过分类区区分不同cgroup中进程的网络包，并进行监控、限流等。 net_prio: 设置cgroup中进程产生的网络流量的优先级 ns: 使cgroup中进程在新的Namespace中fork出新进程(NEWNS)，同时创建出新的cgroup，并且此cgroup包含有新Namespace中的进程。    hierarchy: cgroup进程的继承关系。\n 例如： 系统通过cgroup1针对一组定时任务进程进行CPU使用限制，同时其中一个进程还需要限制磁盘IO，这时候将可以通过cgroup2继承cgroup1限制CPU的同时增加磁盘IO限制。即cgroup2同时具有CPU、IO限制，并且不影响cgroup1组中其他进程的IO限制。\n   cgroup各模块间关系  系统创建hierarchy后，系统下所有进程都将被加入cgroup中，cgroup为根节点，被hierarchy创建的cgroup将被作为此cgroup根节点下的子节点; subsystem与hierarchy是1:1关系(即，一个subsystem只能作用于一个hierarchy之上); hierarchy与subsystem是1:n关系(即，一个hierarchy可以作用于多个subsystem之上); 一个进程可以分布在不同的cgroup中，但需满足cgroup分布在不同的hierarchy中; 一个进程fork出一个子进程的同时，子进程与父进程在同一cgroup中，但可以根据需求调整到其他cgroup中。  ","date":"2020-08-22T15:36:27+08:00","permalink":"https://example.com/p/docker-7.%E5%85%B3%E4%BA%8Elinux%E7%9A%84cgroups/","title":"[ Docker ] 7.关于Linux的Cgroups"},{"content":"Docker-本地构建none包处理 踩坑①.打包构建Dockerfile镜像 每次本地打包构建Dockerfile镜像，如果更新镜像版本号会出现none的镜像在仓库中\n# 停掉none相关的镜像进程占用 docker rm $(docker ps -a | grep \u0026#34;Exited\u0026#34; | awk \u0026#39;{print $1 }\u0026#39;) # 递归依次从仓库移除这些镜像 docker rmi $(docker images | grep \u0026#34;^\u0026lt;none\u0026gt;\u0026#34; | awk \u0026#34;{print $3}\u0026#34;) # 或者，使用一下命令进行移除 docker image prune # (此命令用于删除未使用的映像) # docker image prune [options] # -- options可选值： # -a 显示所有映像(默认隐藏中间映像) # -f 不提示确认，强制直接执行删除 ","date":"2020-08-21T15:36:27+08:00","permalink":"https://example.com/p/docker-5.docker-%E6%9C%AC%E5%9C%B0%E6%9E%84%E5%BB%BAnone%E5%8C%85%E5%A4%84%E7%90%86/","title":"[ Docker ] 5.Docker-本地构建none包处理"},{"content":"关于命名空间(Linux Namespace) 概念  1.Linux Namespace 是Kernel的一个功能，可以针对一系列的系统资源进行隔离。例如：PID(process id)、UID(User id)、Network so on.\n  2.就像chroot允许把当前目录变成根目录一样进行隔离。\n  3.Namespace进行隔离用户，当前用户将在特定的Namespace中具有root权限。但在真是物理机层面，此用户仍然是以UID运行的那个用户。\n Linux包含的Namespace类型    Type Params Kernel Effect Information     Mount CLONE_NEWNS 2.4.19 隔离Namespace下的文件系统   UTS CLONE_NEWUTS 2.6.19 用作隔离nodename和domainname   IPC CLONE_NEWIPC 2.6.19 隔离System V IPC 和POSIX Message queues   PID CLONE_NEWPID 2.6.24 针对进程ID进行隔离   Network CLONE_NEWNET 2.6.29 用于隔离网络设备、IP地址端口等网络栈   User CLONE_NEWUSER 3.8 用于隔离用户及用户组    #Demo Coding ","date":"2020-08-21T15:36:27+08:00","permalink":"https://example.com/p/docker-6.%E5%85%B3%E4%BA%8E%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4linux-namespace/","title":"[ Docker ] 6.关于命名空间(Linux Namespace)"},{"content":"SpringBoot项目Docker化 一 ","date":"2020-08-20T15:36:27+08:00","permalink":"https://example.com/p/docker-4.springboot%E9%A1%B9%E7%9B%AEdocker%E5%8C%96/","title":"[ Docker ] 4.SpringBoot项目Docker化"},{"content":"Docker安装  存储库安装   安装yum-config-manager所需依赖包\n$~:sudo yum install -y yum-utils \\  device-mapper-persistent-data \\  lvm2   通过yum-config-manager添加存储库\n$~:sudo yum-config-manager \\  --add-repo \\  https://download.docker.com/linux/centos/docker-ce.repo   列出存储库中排序后可用的全部版本\nyum list docker-ce --showduplicates | sort -r   进行安装\n# 指定版本号安装 sudo yum install docker-ce-\u0026lt;VERSION_STRING\u0026gt; docker-ce-cli-\u0026lt;VERSION_STRING\u0026gt; containerd.io # 安装最新版本（不指定版本号默认为最新） sudo yum install docker-ce docker-ce-cli containerd.io   安装docker-compose\n# 下载二进制可执行文件，并保存在指定路径 sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose # 修改文件权限 sudo chmod +x /usr/local/bin/docker-compose # 创建软链到全局可执行路径 sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose    软件包安装  ","date":"2020-08-18T15:36:27+08:00","permalink":"https://example.com/p/docker-2.docker%E5%AE%89%E8%A3%85/","title":"[ Docker ] 2.Docker安装"},{"content":"Docker之jdk1.8最简镜像构建 1.准备JRE 在Java下载网站下载JRE。 Tips:此JRE为Oracle作品，而非Openjdk\n2.精简JRE中无关文件 # 进入已经下载jre压缩包的路径,执行解压 tar xzvf ~/Downloads/jre-8u241-linux-x64.tar.gz\u0026amp;\u0026amp;cd jre1.8.0_241 # 删除说明、其他文档 rm -rf COPYRIGHT LICENSE README \\ THIRDPARTYLICENSEREADME-JAVAFX.txt \\ THIRDPARTYLICENSEREADME.txt \\ Welcome.html # 删除非必要依赖文件 rm -rf lib/plugin.jar \\  lib/ext/jfxrt.jar \\  bin/javaws \\  lib/javaws.jar \\  lib/desktop \\  plugin \\  lib/deploy* \\  lib/*javafx* \\  lib/*jfx* \\  lib/amd64/libdecora_sse.so \\  lib/amd64/libprism_*.so \\  lib/amd64/libfxplugins.so \\  lib/amd64/libglass.so \\  lib/amd64/libgstreamer-lite.so \\  lib/amd64/libjavafx*.so \\  lib/amd64/libjfx*.so # 移除完成后文件大小共111M，然后进行压缩;压缩后大小为44M tar czvf jre8.tar.gz * 3.编写DockerFile FROMdocker.io/jeanblanchard/alpine-glibcMAINTAINERcheneyin xy410257@163.comADD jre8.tar.gz /usr/local/java/jdk8/ENV JAVA_HOME /usr/local/java/jdk8ENV PATH ${PATH}:${JAVA_HOME}/binWORKDIR/optTips:由于Java依赖于glibc，基础镜像选择alpine-glibc并非alpine\n4.构建打包 docker build -t touch_star/java8:1.0 . 5.测试运行 docker run -it touch_star/java8:1.0 #### Print #### /opt # java -version java version \u0026#34;1.8.0_241\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) /opt #  ","date":"2020-08-18T15:36:27+08:00","permalink":"https://example.com/p/docker-3.docker%E4%B9%8Bjdk1.8%E6%9C%80%E7%AE%80%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA/","title":"[ Docker ] 3.Docker之jdk1.8最简镜像构建"},{"content":"Docker命令  docker [option] command\n   option\n \u0026ndash;config string: 客户端配置文件的位置 \u0026ndash;context string[-c]: 用于连接到守护程序的上下文的名称 \u0026ndash;debug[-D]: 调试模式 \u0026ndash;host list[-H]: 要连接的守护程序套接字 \u0026ndash;log-level string[-l]: 日志等级[ debug | info | warn | error | fatal ]默认为info \u0026ndash;tls: 使用加密模式 \u0026ndash;tlscacert string: 签名证书文件路径 \u0026ndash;tlscert string: 密钥文件路径 \u0026ndash;tlskey string: key文件路径 \u0026ndash;tlsverify: 使用加密并验证远程连接 \u0026ndash;version[-v]: 版本信息    Management Commands(管理命令)\n builder: 管理构建 config: 管理Docker配置 container: 管理容器 context: 管理镜像构建上下文 image: 管理镜像 network: 管理网络 node: 管理Swarm节点 plugin: 管理插件 secret: 管理Docker secrets service: 管理服务 stack: 管理Docker stacks swarm: 管理Swarm集群 system: 查看系统信息 trust: 管理对Docker映像的信任 volume: 管理卷    Commands(命令)\n attach: 将本地标准输入，输出和错误流附加到正在运行的容器 build: 从Dockerfile构建镜像 commit: 根据容器的更改创建新镜像 cp: 在容器和本地文件系统之间复制文件/文件夹 create: 创建一个新的容器 deploy: 部署新堆栈或更新现有堆栈 diff: 检查容器文件系统上文件或目录的更改 events: 从服务器获取实时事件 exec: 在正在运行的容器中运行命令 export: 将容器的文件系统导出为tar存档 history: 显示镜像的历史记录 images: 显示镜像列表 import: 从tarball导入内容以创建文件系统映像 info: 显示系统范围的信息 inspect: 返回有关Docker对象的低级信息 kill: 杀死一个或多个正在运行的容器 load: 从tar存档或STDIN加载镜像 login: 登录Docker镜像仓库 logout: 退出Docker镜像仓库 logs: 提取容器的日志 pause: 暂停一个或多个容器中的所有进程 port: 列出端口映射或容器的特定映射 ps: 显示所有容器 pull: 从镜像仓库中提取镜像或存储库 push: 提交镜像或存储库到镜像仓库 rename: 为容器重新命名 restart: 重启一个或多个容器 rm: 移除一个或者多个容器 rmi: 移除一个或者多个镜像 run: 在新容器中执行命令 save: 保存一个或多个镜像到tar存档（默认情况下流式传输到STDOUT） search: 从Docker Hub中搜索镜像 start: 运行一个或者多个停止状态的容器 stats: 显示实时的容器资源使用情况统计流 stop: 停止一个或者多个运行状态的容器 tag: 创建一个引用了SOURCE_IMAGE的标签TARGET_IMAGE top: 显示某个容器的运行进程 unpause: 取消暂停一个或多个容器中的所有进程 update: 更新一个或多个容器的配置 version: 显示docker的版本信息 wait: 阻塞一个或多个容器直到停止，然后打印其退出代码    ","date":"2020-08-16T15:36:27+08:00","permalink":"https://example.com/p/docker-1.docker%E5%91%BD%E4%BB%A4/","title":"[ Docker ] 1.Docker命令"},{"content":"Go并发之协程 首先需要了解几个概念：  channel(通道) select  1.协程、线程与进程 进程，属于操作系统，是系统资源分配的最小单位。充分利用CPU资源实现并发。\n线程，所属于进程，是进程的内部实现，大大降低了上下文切换的消耗，突破一个进程只可以处理一件事的缺陷，从而提高了系统的并发性。\n协程，粒度更细，属于线程中的调度。填补了线程在IO上性能的缺陷，避免陷入内核级上下文切换所导致的性能损耗。\n2.协程实现原理 线程实现原理\n线程是操作系统的内核对象，多线程情况下，线程达到一定数量，将会导致上下文频繁切换，CPU的额外消耗会提升。 高并发的网络编程如果一个线程对应一个socket连接将不是最好的处理方式，所以操作系统提供基于事件模式的异步编程模型。 协程实现原理\n ","date":"2020-04-22T21:36:27+08:00","permalink":"https://example.com/p/1.go%E5%B9%B6%E5%8F%91%E4%B9%8B%E5%8D%8F%E7%A8%8B/","title":"1.Go并发之协程"},{"content":"Mysql基础 Mysql引擎   MyISAM\n  InnoDB\n  Memory\n  Archive\n     功能点 MyISAM InnoDB Memory Archive      存储限制 256TB RAM 64TB -    事务 N N Y N    全文检索 Y N N N    B+ Tree索引        哈希索引        数据缓存        外键         ","date":"2020-01-06T01:13:47+08:00","permalink":"https://example.com/p/mysql-2-1.mysql%E5%9F%BA%E7%A1%80/","title":"[ MySQL ] 2-1.MySQL基础"},{"content":"MySQL索引  Index(索引)，在存储引擎中用于快速找到记录的一种数据结构。索引用来快速寻找特定值的记录。\n 如果没有索引，执行查询时，MySQL必须从第一个记录开始扫描整个表的所有记录，知道找到符合要求的记录。如果存在索引，MySQL无需扫描全表即可迅速查找到目标记录所在的位置。\n1. 索引类型   Hash索引：\n 底层实现是基于哈希表，是一种以Key-Value形式存储数据的结构。 多个数据在存储关系上是没有任何顺序关系的。对于区间查询是无法通过索引查询的， 只能通过全表扫描的方式进行。Hash索引适用于等值查询场景。    B+ Tree索引：(MySQL引擎Innodb实现方式)\n B+ Tree索引是一种多路平衡查询树。 其节点是天然有序的(左节点 \u0026lt; 父节点 \u0026lt; 右节点)。 对于范围查询时候不需要做全表扫描。    相比Hash索引，B+ Tree的优点：\n Hash索引适合等值查询，但是无法进行范围查询 Hash索引没办法利用索引完成排序 Hash索引不支持多列联合索引的最左匹配规则 如果有大量重复键值的情况下，Hash索引的效率会很低，因为存在Hash碰撞问题    2. MySQL索引失效的几种情况  如果条件中有or，即使其中有条件带索引也不会使用 对于多列索引，不是使用的第一部分(第一个)，则不会使用索引 like查询是以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql估计使用全表扫描要比使用索引快,则不使用索引 not in ,not exist. 范围查询  索引的底层实现是B+树，为何不采用红黑树，B树? （1）：B+Tree非叶子节点只存储键值信息，降低B+Tree的高度，所有叶子节点之间都有一个链指针，数据记录都存放在叶子节点中\n（2）： 红黑树这种结构，h明显要深的多，效率明显比B-Tree差很多\n（3）：B+树也存在劣势，由于键会重复出现，因此会占用更多的空间。但是与带来的性能优势相比，空间劣势往往可以接受，因此B+树的在数据库中的使用比B树更加广泛\n七种事务传播行为 （1）Propagation.REQUIRED\u0026lt;默认\u0026gt; 如果当前存在事务，则加入该事务，如果当前不存在事务，则创建一个新的事务。\n（2）Propagation.SUPPORTS 如果当前存在事务，则加入该事务；如果当前不存在事务，则以非事务的方式继续运行。\n（3）Propagation.MANDATORY 如果当前存在事务，则加入该事务；如果当前不存在事务，则抛出异常。\n（4）Propagation.REQUIRES_NEW 重新创建一个新的事务，如果当前存在事务，延缓当前的事务。\n（5）Propagation.NOT_SUPPORTED 以非事务的方式运行，如果当前存在事务，暂停当前的事务。\n（6）Propagation.NEVER 以非事务的方式运行，如果当前存在事务，则抛出异常。\n（7）Propagation.NESTED 如果没有，就新建一个事务；如果有，就在当前事务中嵌套其他事务。\n3. 强制索引 # 强制索引 select * from table force index(PRI) limit 2; # 禁止索引 select * from table ignore index(PRI) limit 2; ","date":"2020-01-06T01:13:47+08:00","permalink":"https://example.com/p/mysql-2.mysql%E7%B4%A2%E5%BC%95/","title":"[ MySQL ] 2.MySQL索引"},{"content":"Redis 1. 什么是跳表  跳跃表是一种有序的数据结构，它通过在每个节点中维持多个指向其他的几点指针，从而达到快速访问队尾目的。跳跃表的效率可以和平衡树想媲美了，最关键是它的实现相对于平衡树来说，代码的实现上简单很多 跳跃表 level 层级完全是随机的。一般来说，层级越多，访问节点的速度越快。 一是实现有序集合键，二是集群节点中用作内部数据结构。 相比于红黑树、平衡二叉树，跳表不仅查找、插入、删除时间复杂度都是O(logN)，并且实现简单很多。  2. Redis中BitMap   Bitmap并不是一种独立的数据结构，而是基于String数据结构进行的位图操作。\n 最大空间即为String数据结构所支持的512MB， 所以bitmap所支持的最大offset为2^32-1.    一般使用场景用于大数据签到、日活统计、在线统计等等。\n 其主要优点可以节省大量空间。 例如： 进行日活统计：  使用日期作为key，用户ID作为偏移量，1为当日活跃，0为不活跃      ### 基本操作演示(以下为Redis-cli命令) # 设置一个字符串 1Aa  # 存储在redis中的二进制为 0b001100010100000101100001 set testkey 1Aa # 进行bitmap操作 setbit testkey 10 1 setbit testkey 18 0 # testkey对象index为10的bit值为1 getbit testkey 10 # testkey对象index为18的bit值为1 getbit testkey 18 # 进行位统计 ,结果为bit上为1的和， # testkey输出结果为 8 bitcount testkey # testkey输出结果为 3 bitcount testkey 0 0 3. redis事务   Multi开启事务\n  Exec执行事务块内命令\n  Discard 取消事务\n  Watch 监视一个或多个key，如果事务执行前key被改动，事务将打断\n  4. Redis的同步机制  全量拷贝，  1.slave第一次启动时，连接Master，发送PSYNC命令， 2.master会执行bgsave命令来生成rdb文件，期间的所有写命令将被写入缓冲区。  master bgsave执行完毕，向slave发送rdb文件 slave收到rdb文件，丢弃所有旧数据，开始载入rdb文件 rdb文件同步结束之后，slave执行从master缓冲区发送过来的所以写命令。   此后 master 每执行一个写命令，就向slave发送相同的写命令。   增量拷贝  如果出现网络闪断或者命令丢失等异常情况，从节点之前保存了自身已复制的偏移量和主节点的运行ID 主节点根据偏移量把复制积压缓冲区里的数据发送给从节点，保证主从复制进入正常状态。    5. 缓存淘汰策略 （1）：先进先出算法（FIFO）\n（2）：最近使用最少Least Frequently Used（LFU）\n（3）：最长时间未被使用的Least Recently Used（LRU）\n当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致LRU命中率急剧下降，缓存污染情况比较严重\n6.redis过期key删除策略 （1）：惰性删除，cpu友好，但是浪费cpu资源\n（2）：定时删除（不常用）\n（3）：定期删除，cpu友好，节省空间\n7.缓存击穿原因以及处理办法 频繁请求查询系统中不存在的数据导致；\n 处理方法：  cache null策略，查询反馈结果为null仍然缓存这个null结果，设置不超过5分钟过期时间 布隆过滤器，所有可能存在的数据映射到足够大的bitmap中 google布隆过滤器：基于内存，重启失效不支持大数据量，无法在分布式场景 redis布隆过滤器：可扩展性，不存在重启失效问题，需要网络io，性能低于google    8.缓存雪崩以及处理办法 同一时刻大量缓存失效；\n处理方法：\n（1）：缓存数据增加过期标记\n（2）：设置不同的缓存失效时间\n（3）：双层缓存策略C1为短期，C2为长期\n（4）：定时更新策略\n9.Redis如何做持久化 bgsave做镜像全量持久化，aof做增量持久化。因为bgsave会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据 ，所以需要aof来配合使用。在redis实例重启时，会使用bgsave持久化文件重新构建内存，再使用aof重放近期的操作指令来 实 现完整恢复重启之前的状态。\nbgsave的原理是什么？ fork和cow。fork是指redis通过创建子进程来进行bgsave操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写进的页面数据会逐渐和子进程分离开来。\nRDB与AOF区别 （1）：R文件格式紧凑，方便数据恢复，保存rdb文件时父进程会fork出子进程由其完成具体持久化工作，最大化redis性能，恢复大数据集速度更快，只有手动提交save命令或关闭命令时才触发备份操作；\n（2）：A记录对服务器的每次写操作（默认1s写入一次），保存数据更完整，在redis重启是会重放这些命令来恢复数据，操作效率高，故障丢失数据更少，但是文件体积更大；\nredis如何实现延时队列？ 使用sortedset，想要执行时间的时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。\n为啥redis zset使用跳跃链表而不用红黑树实现? （1）：skiplist的复杂度和红黑树一样，而且实现起来更简单。\n（2）：在并发环境下红黑树在插入和删除时需要rebalance，性能不如跳表。\n","date":"2020-01-06T01:13:47+08:00","permalink":"https://example.com/p/redis-1.-redis/","title":"[ Redis ] 1. Redis"},{"content":"ElasticSearch集群搭建 注： #A:修改/etc/security/limits.conf #\u0026lt;domain\u0026gt; \u0026lt;type\u0026gt; \u0026lt;item\u0026gt; \u0026lt;value\u0026gt; * soft nofile 65536 * hard nofile 131072 * soft nproc 2048 * hard nproc 4096 #B:修改/etc/sysctl.conf vm.max_map_count=262144 # 保存执行： sysctl -p # 或者 sysctl -w vm.max_map_count=262144 ","date":"2019-12-22T15:36:27+08:00","permalink":"https://example.com/p/1.elasticsearch%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","title":"1.ElasticSearch集群搭建"},{"content":"ElasticSearch集群原理 一、关于ES集群需要思考几个问题  需要多大规模的集群？  # 首先从两个方面考虑 ①.数据量有多大？数据增长情况如何？ ②.服务器硬件设施配置：CPU、Memory、Disk # 推算依据 ES Jvm heap 最大设置为32G。 30G heap大约可以存储数据量10T；服务器memory若为128G，可运行多个实例节点。 # 应用场景 A:用于构建业务搜索模块，且多是垂直领域搜索。（数据量级几千万至十亿级,一般需要2-4台机器） B:用于大规模数据的实时联机处理分析(OLAP),例如ELK，数据规模可达上千亿乃至更多，需要几十甚至上百实例节点。  集群中节点角色如何分配？  # 一个节点可以充当一个或多个角色，默认三个角色都有 # 节点角色 ①.Master node.master: true # 实例节点为主节点 ②.DataNode node.data: true # 默认是数据节点。 ③.CoordinateNode # 以上两项置为false，则此节点为协调节点； # 协调节点：一个节点只作为接收请求、转发请求到其他节点、汇总各个节点返回数据等功能的节点。 # 具体分配 A:小规模集群不需要具体区分； B:中、大规模集群(十个节点以上)，并发查询量大，查询的合并量大，可以增加独立的协调节点。角色分开的好处是分工分开，不互影响。如不会因协调角色负载过高而影响数据节点的能力。  如何避免脑裂问题发生？   索引应该设置多少个分片？ 分片应该设置多少个副本？  ","date":"2019-12-22T15:36:27+08:00","permalink":"https://example.com/p/2.elasticsearch%E9%9B%86%E7%BE%A4%E5%8E%9F%E7%90%86/","title":"2.ElasticSearch集群原理"},{"content":"MySQL之热备份工具(xtrabackup) 1.原理 2.安装  进入xtrabackup官网选择Percona XtraBackup   3.实战 ","date":"2019-11-22T19:36:27+08:00","permalink":"https://example.com/p/1.mysql%E4%B9%8B%E7%83%AD%E5%A4%87%E4%BB%BD%E5%B7%A5%E5%85%B7xtrabackup/","title":"1.MySQL之热备份工具(xtrabackup)"},{"content":"编译安装过程 1.预编译 cmake . -DCMAKE_INSTALL_PREFIX=/data/ops/mysql/ \\ -DMYSQL_DATADIR=/data/ops/mysql/data \\ -DWITH_BOOST=../boost_1_59_0 \\ -DSYSCONFDIR=/etc \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_PARTITION_STORAGE_ENGINE=1 \\ -DWITH_FEDERATED_STORAGE_ENGINE=1 \\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\ -DWITH_MYISAM_STORAGE_ENGINE=1 \\ -DENABLED_LOCAL_INFILE=1 \\ -DENABLE_DTRACE=0 \\ -DDEFAULT_CHARSET=utf8mb4 \\ -DDEFAULT_COLLATION=utf8mb4_general_ci \\ -DWITH_EMBEDDED_SERVER=1 2.编译安装 make -j `grep processor /proc/cpuinfo | wc -l` #编译很消耗系统资源，小内存可能编译通不过make install make install 3.启动配置 ls -lrt /usr/local/mysql # 创建启动脚本，并增加可执行权限 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld chmod +x /etc/init.d/mysqld # 开机自启动 systemctl enable mysqld 4.修改Mysql配置文件 5.添加环境变量 6.初始化数据库 7.启动数据库 ","date":"2019-11-22T19:36:27+08:00","permalink":"https://example.com/p/1.mysql%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B/","title":"1.MySQL编译安装过程"},{"content":"Zookeeper基础 1. ZAB协议  ZAB协议是为分布式协调服务Zookeeper专有的一种协议，此协议是为了应对崩溃恢复的原子广播 崩溃恢复  整个zk集群刚启动或者Leader节点宕机、重启或者不可以正常提供服务时超出一半的情况下，所有节点将会进入崩溃恢复模式 首先通过选举产生Leader 然后集群中的Follwer节点与新产生的Leader节点进行数据同步 一旦集群中一半数量的节点与Leader节点完成了数据同步，集群就会退出崩溃恢复模式，进入到消息广播模式   消息广播  Leader节点开始接受客户端的事务请求，生成事务的提案进行事务请求处理。    ","date":"2019-11-12T01:36:27+08:00","permalink":"https://example.com/p/zookeeper-2.-zookeeper%E5%9F%BA%E7%A1%80/","title":" [ Zookeeper ] 2. Zookeeper基础"},{"content":"zookeeper分布式部署 一.配置服务器IP地址映射 [root@localhost zk]~#: vim /etc/hosts\n192.168.1.111 zoo1 192.168.1.112 zoo2 192.168.1.113 zoo3 192.168.1.114 zoo4 192.168.1.115 zoo5 二.修改配置ZK文件 1.下载Zookeeper\n# 进入ZK路径 wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.5.6/apache-zookeeper-3.5.6-bin.tar.gz 2.修改配置文件 进入conf目录，在配置文件前，先cp zoo_sample.cfg zoo.cfg,然后vim zoo.cfg。配置如下：\ntickTime=2000 initLimit=10 syncLimit=5 dataDir=/data/ops/zk/zookeeper-3.5.6-master/conf clientPort=2181 server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 3.启动ZK ①.在每个节点的服务器依次启动服务： [root@localhost zk]~#: ./bin/zkServer.sh start 在启动过程中日志会出现异常，由于其他节点还未启动，所以属于正常情况（正常情况下，仅有最后一个节点启动不会出现异常）。待所有节点全部启动，集群会逐渐稳定下来。 ②.查询每一个节点角色 [root@localhost zk]~#: ./bin/zkServer.sh status\n# LeaderNode ZooKeeper JMX enabled by default Using config: /data/ops/zk/zookeeper-3.5.6-follower/bin/../conf/zoo.cfg Client port found: 2181. Client address: localhost. Mode: leader # FollowerNode ZooKeeper JMX enabled by default Using config: /data/ops/zk/zookeeper-3.5.6-master/bin/../conf/zoo.cfg Client port found: 2181. Client address: localhost. Mode: follower ","date":"2019-11-12T01:36:27+08:00","permalink":"https://example.com/p/1.zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/","title":"1.zookeeper分布式部署"},{"content":"1.Nginx入门 一、Nginx为什么受青睐 ​\t在介绍Nginx具体的安装、配置以及原理之前先聊聊概念常识问题。那就是目前为什么Nginx深受青睐？那我们先从Nginx是什么开始聊起。\n1.Nginx是什么   简单介绍\n ***Nginx***来自于俄罗斯，是在**RamblerMedia**工作的**Igor Sysoev**使用***C***语言编写而成的跨平台轻量级高性能的*Web*服务器。***Nginx***可以运行在**Linux**、**FreeBSD**、**Solaris**(*Sun*公司的类*Unix OS*)、**MacOS**、以及**Windows**等操作系统。操作系统的不同，也给***Nginx***带来了一些好处，***Nginx***会使用当前操作系统中特有的一些高效**API**来提高自身的性能。    Nginx和它的对手们\nNginx的对手们有Apache、Lighttpd、Tomcat、Jetty、IIS，它们同为Web服务器：具备Web服务器的基本功能；基于Rest架构风格，以**统一资源描述符（URI）或统一资源定位符（URL）作为沟通依据，通过HTTP为浏览器等Client程序提供各种网络服务。\n但是，这些Web服务器呢，都多多少少因为各自的定位与发展方向都不尽相同，使得每一款Web服务器都各有特色：\n​\t1.Tomcat、Jetty：都是面向Java语言设计的。但是它们在性能方面与Nginx没有什么可比性，因为这两款服务器都是重量级选手。可能有伙伴会很疑惑，我已经用Tomcat跑起服务，同样配置后可以直接访问为什么还要在加层外套Nginx，对于这个问题，在后边对这一点进行详细的分析。【】\n​\t2.IIS：这位选手呢，来自于微软家族。然后特点大家可能就很清楚了，它只能在Windows OS运行（不过网上也有工具可以把它运行在LinuxOS中，但是并不是很完美哦）。可能拉低它颜值的就是稳定性与性能了，Windows OS作为服务器的话，稳定性和部分性能都不能和类Unix OS进行媲美，所以呢，在高性能Web服务器的场合中，IIS可能就要被“淘汰”了。\n​\t3.Apache：这是一位压轴级选手，是发展周期最长的，毫无疑问是世界第一大Web服务器，在2012年遥遥领先其他选手。它毕竟有很多优秀的地方：稳定、开源、跨平台等。但是美中不足的是，它被设计成为了重量级、不支持高并发的Web服务器。如果有数以万计的HTTP请求同时访问，服务器就会面临大量内存消耗的问题，操作系统也会跟着收到牵连，毕竟Apache的进程做进程间切换时会给服务器的CPU带来重大压力，同时会伴随着响应效率降低，这致命的一击，导致这位来自“贵族世家”的选手在高性能Web服务器的舞台上没有了地位。\n​\t4.Lighttpd：与Nginx同样是轻量级、高性能的Web服务器。但是它并没有得到国内开发者的钟爱，而是被欧美的开发者们所追捧。\n  恩宠\u0026ndash;Nginx\nNginx的代码也是开源的而且是最自由的2-clause BSD-like license许可证。Nginx使用的架构是基于事件驱动的，能够并发处理百万级别的TCP连接。由于Nginx的高度模块化和具有最自由的许可证，让Nginx的第三方模块扩展功能更加充实。优秀的设计还带来了极佳的稳定性体验。所以，Nginx大量应用于大流量的网站来高效处理大规模高并发连接。种种迹象表明，Nginx在性能方面很出色。\n  2.Nginx的特点   更快\n快主要体现在两方面：①在正常的情况下，单次请求会得到更快的响应；②在数以万计的并发请求中，Nginx可以比其他Web服务器更快的响应请求。\n  高扩展性\nNginx的高度模块化决定了其具有高扩展性。它完全是由多个不同功能、不同层次、不同类型以及耦合度极低的模块组合而成。它的模块都是嵌入到二进制文件中执行，使得第三方开发的模块也一样完美支持性能。所以高并发的网站完全可以根据自身项目业务特性定制属于自己的模块。\n  高可靠性\n这个特点应该是选择Web服务器最基本的条件。Nginx的稳定性，大家有目共睹。国内多家高流量并发的网站在核心的服务器上大规模使用Nginx。官方提供的常用模块是非常稳定的，每一个Worker进程都相对独立，把耦合性降至最低。master进程在其中一个Worker进程出错时可以快速“拉起”新的Worker子进程提供相应的服务。\n  低内存消耗\n据数据测试，一般情况下，1W个不活跃的HTTP Keep-Alive连接在Nginx中消耗只有2.5MB的内存。（这也是Nginx能够支持高并发连接的基础）\n  单机支持10W+的并发连接\n由于现在是海量数据时代，高并发无疑成为大家青睐的对象。理论上，Nginx支持的并发连接数量取决于内存，10W+的并发连接并没有到极限。但是，能否及时处理更多的并发连接应该取决于项目业务的需求。\n  热部署\nmaster管理进程和Worker进程是相互隔离的，这使得Nginx能够彰显热部署的能力。通俗点来说，就是完全可以在7*24h不停止服务正常工作的情况下，可以升级Nginx的可执行文件、更新配置选项、更新日志文件等功能操作。\n  最自由的BSD许可协议\n俗话说**“众人拾柴火焰高”**。也正是BSD许可协议带来的极大优势，为**Nginx**提供更强劲的发展动力。\n  综上所述，选择Nginx的核心理由还是由于它能在支持高并发请求的同时保持高效的服务。  二、Nginx的安装 1.源码安装 在正式安装Nginx前需要保证服务器主机已经安装有编译环境GCC开发库之类的环境。\n GCC编译环境工具安装  ①Ubuntu OS编译环境使用如下命令：\napt-get install build-essential apt-get install libtool ②CentOS编译环境使用如下命令：\nyum install -y gcc automake autoconf libtool make yum install -y gcc -c++ 安装完成编译环境，就可以着手准备Nginx所需要的类库PCRE库、zlib库、OpenSSL开发库。\n PCRE库安装  首先介绍一下PCRE库的作用，为Nginx的HTTP模块提供解析正则表达式的基础。这里直接通过下载源码的方式进行编译安装。根据需要的版本在PCRE源码中选择URL然后用下边命令进行下载\nwget https://ftp.pcre.org/pub/pcre/pcre-8.42.tar.gz ## 现在完成后，对源码包进行解压 tar -xzvf pcre-8.42.tar.gz ## 解压完成进入pcre-8.42目录 cd pcre-8.42 ## 执行配置 ./configure ## 进行编译并安装 make make install  Zlib库安装  Zlib库主要是针对HTTP包的内容做gzip格式的压缩。例如，Nginx的配置nginx.conf文件中配置gzip on。Zlib-1.2.11下载\n## 使用Wget命令下载源码 wget http://zlib.net/zlib-1.2.11.tar.gz ## 进行解压 tar -xzvf zlib-1.2.11.tar.gz ## 进入zlib目录进行配置编译安装操作 cd zlib-1.2.11 ./configure make make install  Openssl安装  如果对版本没有特殊要求，OpenSSL我们采用命令安装\n## CentOS 安装命令 yum install openssl openssl-devel ## Ubuntu/Debian 安装命令 sudo apt-get install openssl sudo apt-get install libssl-devel  Nginx安装  首先去Nginx官网下载合适版本的源码。同时我们可以直接在服务器使用wget命令进行下载。\nwget http://nginx.org/download/nginx-1.14.0.tar.gz ## 解压Nginx源码 tar -xzvf nginx-1.14.0.tar.gz cd nginx-1.14.0 ## 进行配置项的配置:(以下只是选择了编译Nginx源码时的部分配置选项) # --sbin-path: 指定了可执行文件的放置路径,默认路径在\u0026lt;prefix\u0026gt;/sbin/nginx # --conf-path: 指定了配置选项文件的放置路径,默认路径在\u0026lt;prefix\u0026gt;/conf/nginx.conf # --pid-path: 指定了pid文件的放置路径,默认路径在\u0026lt;prefix\u0026gt;/logs\u0026gt;nginx.pid # --with-http_ssl_module:安装http_ssl_module，使得Nginx支持SSL协议，保证HTTPS服务。 # --with-pcre: 指定PCRE库源码的路径，编译Nginx时会进入此路径对源码进行编译。 # --with-zlib: 指定Zlib库源码的路径，编译Nginx时会进入此路径对源码进行编译。 ./configure \\ --sbin-path=/usr/local/nginx/nginx \\  --conf-path=/usr/local/nginx/nginx.conf \\ --pid-path=/usr/local/nginx/nginx.pid \\ --with-http_ssl_module \\ --with-pcre=/opt/app/openet/oetal1/cheney/pcre-8.42 \\ --with-zlib=/opt/app/openet/oetal1/cheney/zlib-1.2.11 ## 配置完成后进行编译安装 make make install 安装完成之后可以进行简单的测试，进入配置的**${\u0026ndash;sbin-path}**路径下启动Nginx。\n## 直接使用默认配置执行nginx可执行文件启动服务 /usr/local/nginx/sbin/nginx 在保证服务器主机端口可以访问的情况下，使用浏览器访问主机IP地址，例如http://127.0.0.1，可以看到Nginx服务的默认主页。这就说明Nginx服务正常启动了。然后可以根据项目具体的业务需求，对Nginx进行特殊配置进行详细定制。\n Nginx的具体配值问题，下篇文章进行详细介绍。 ","date":"2019-11-12T01:36:27+08:00","permalink":"https://example.com/p/nginx-1.nginx%E5%85%A5%E9%97%A8/","title":"[Nginx] 1.Nginx入门"},{"content":"三、Pod对象详解 ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":"4-1.K8s构建高可用Mysql集群 一、MySQL高可用架构图 1.主从复制+读写分离  此方案更适用于数据库读数据的场景(针对数据强一致性非严格的情况)，毕竟Replication存在一定的时延。 通过快速扩容Slave节点提高MySQL集群读取数据能力，不用过度依赖于Master节点。\n 实现过程 ①.为其提供PV存储盘 ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":" ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":"1.Kuberbetes-dashboard(Web UI)  Kuberbetes-dashboard是基于web的Kubernetes用户界面。\n ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":"云原生 ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":"目录   云原生探索\n  第一部分-Goland学习\n Go并发  协程      第二部分-Docker\n 1.Docker命令 2.Docker安装 3.Docker之jdk最简镜像构建 4.Docker-SpringBoot构建 5.Docker-本地构建none包处理 6.Docker-Linux Namespace 7.Docker-Linux cgroup 8.Docker-Linux UFS    第三部分-Kubernetes\n 1.初识K8s  1-1.K8s基础 1-2.Kubectl工具 1-3.Kubeadm工具 1-4.Kubenetes集群初始化 1-5.Kubenetes之Helm包管理工具   2.深入Pod  2-1.Pod对象 2-2.Pod示例 2-3.Pod详解   3.深入Service  3-1.Service对象   4.K8s服务构建  4-1.Mysql-K8s主从构建   K8s-PlugIn  1.Dashboard插件       第四部分-ServiceMesh\n  ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":"Go编程之指针 ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":"一、OpenCV-Java的入门  OpenCV的Java官方文档地址\n 1.OpenCV的Java环境构建 首先，声明一下在本文中选用的环境配置如下：\n  MacOS操作系统\n  IntelliJ IDEA开发编译器\n   MacOS的安装有两种方式：一种是靠强大的BrewHome安装器自动安装；另外一种就是相对麻烦的手动安装了。\n在这里我选择的是BrewHome进行安装。（前提是MacOS已经安装了BrewHome）\n","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":"一、相关概念的简介  在了解深度学习前应该还有两个专业名词大家也想必是耳熟能详，那么就是人工智能、机器学习。\n  人工智能(Artificial Intelligence)：也就是我们经常听到的AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。但是在早期的时候，人工智能充满了局限性，只是在特定的环境范围下。 机器学习(Machine Learning)：简称ML。机器学习其实是人工智能的一个分支。如果一个程序可以在**任务*(T)上，随着经验(E)***的增加，*效果(P)***也可以随之增加，则就可以说这个程序可以从经验中得到学习。 深度学习(Deep Learning)：简称DL。深度学习也是机器学习的一个分支。主要是在机器学习的基础之上有所改进：它除了可以学习特征和任务之间的关联，还可以自动的将简单的特征组合成更加复杂的特征，并使用这些组合特征解决问题。   二、了解机器学习(ML)  1. 用例子认识机器学习的概念 其实，邮件系统中判断收到的邮件是否为垃圾邮件就可以看做是一个机器学习的过程。首先对垃圾邮件分类概念进行一个拆分、类比：\n  一个程序 \u0026lt;======\u0026gt; 需要用到的机器学习算法，比如逻辑回归算法 任务(T) \u0026lt;======\u0026gt; 区分此邮件是否是垃圾邮件这个任务 经验(E) \u0026lt;======\u0026gt; 已经区分过是否为垃圾邮件的历史事件 效果(P) \u0026lt;======\u0026gt; 机器学习算法在区分此邮件是否是垃圾邮件这个任务的精确率   在整个过程中，首先会从每一封邮件中抽取出对分类结果可能有影响的因素（比如：发件人的地址、邮件的标题、收件人的数量、邮件正题内容，so on）。这样的每一个**因素**其实可以成为是一个**特征** ***(feature)***。然而机器学习算法中的**逻辑回归算法**可以从训练数据中计算出每个特征和预测结果的相关度。例如，在垃圾邮件分类过程中，可能会发现如果一个邮件的收件人越多，那么这封邮件是垃圾邮件的可能性越大。 在对一封完全未知的邮件进行区分时，**逻辑回归算法**会根据这封邮件中抽取到的每一个**特征**以及**这些特征**和垃圾邮件的**相关度**进行判断是否为垃圾邮件。 所以，从例子中不难看出：一般情况下，在训练数据达到一定数量之前，越多的训练数据可以使得逻辑回归算法对未知邮件做出的判断越精确。\n 也就是说**逻辑回归算法可以根据训练数据【经验(E)】提高垃圾邮件分类问题【任务(T)】上的准确率【效果(P)】**\n 2.机器学习的分类  有监督学习(Supervised Learning)   有监督学习可分为回归和分类问题。例如上述示例垃圾邮件分类就属于有监督学习。 1.在回归问题中，我们会预测一个连续值；也就是我们试图将输入变量和输出用一个连续函数对应起来。 2.在分类问题中，我们会预测一个离散值，我们试图将输入变量与离散的类别对应起来。\n  无监督学习(Unsupervised Leanring)   这种学习方式，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。\n  增强式学习(Reinforcement Learning)   输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。\n ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":"一、关于神经网络  训练神经网络主要包含以下四部分：  层，多个层组合成网络(模型) 输入数据和相应的目标 损失函数，即用于学习的反馈信号 优化器，决定学习过程如何进行    ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""},{"content":" ","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/","title":""}]