<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zookeeper on Quinn</title>
    <link>https://example.com/tags/zookeeper/</link>
    <description>Recent content in Zookeeper on Quinn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 12 Nov 2019 01:36:27 +0800</lastBuildDate><atom:link href="https://example.com/tags/zookeeper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title> [ Zookeeper ] 2. Zookeeper基础</title>
      <link>https://example.com/p/zookeeper-2.-zookeeper%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/zookeeper-2.-zookeeper%E5%9F%BA%E7%A1%80/</guid>
      <description>Zookeeper 1. ZAB协议  ZAB协议是为分布式协调服务Zookeeper专有的一种协议，此协议是为了应对崩溃恢复的原子广播 崩溃恢复  整个zk集群刚启动或者Leader节点宕机、重启或者不可以正常提供服务时超出一半的情况下，所有节点将会进入崩溃恢复模式 首先通过选举产生Leader 然后集群中的Follwer节点与新产生的Leader节点进行数据同步 一旦集群中一半数量的节点与Leader节点完成了数据同步，集群就会退出崩溃恢复模式，进入到消息广播模式   消息广播  Leader节点开始接受客户端的事务请求，生成事务的提案进行事务请求处理。    </description>
    </item>
    
    <item>
      <title> [ Zookeeper ] 3. Zookeeper作为注册中心的缺馅</title>
      <link>https://example.com/p/zookeeper-3.-zookeeper%E4%BD%9C%E4%B8%BA%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E7%BC%BA%E9%A6%85/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/zookeeper-3.-zookeeper%E4%BD%9C%E4%B8%BA%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E7%BC%BA%E9%A6%85/</guid>
      <description>Zookeeper作为注册中心的缺馅 一、 关于服务发现中心 1.CAP原理   C(Consistency)：一致性
在更新操作成功后，所有节点在同一时间的数据保证一致。
  A(Availability)：可用性
服务提供正常服务，并且保证都是正常响应时间。
  P(Partition Tolerance)：分区容错性
在分布式系统中，由于某一个节点出现或者网络分区出现异常情况，仍可以提供满足一致性或者用性 的服务。
  2.Zookeeper保证CP  当向注册中心查询服务列表时，可以容忍注册中心返回的是在此之前已经注册的信息，但不能接受服务直接不可用的现象 服务注册功能对可用性的标准要高于一致性 在ZK中，存在这样一种情况，如果master节点因为网络故障与其他节点失去联系，剩余节点将会重新进行Leader选举。 在ZK leader选举过程中，是一个耗时操作（大概30-120s，耗时未经实际生产验证），并且选举过程中，ZK集群是处于不可用状态，这时候将会导致服务发现、服务注册处于不可用 云部署环境下，因网络问题导致ZK集群丢失Master节点是大概率事件，虽说服务可以通过重新选举恢复到可用状态，但是漫长的选举过程耗时是不可容忍的。  二、Alibaba为什么不使用ZK作为注册中心 1. 借助一篇文章示例了解CAP选择  网上存在一篇文章阿里巴巴为什么不用 ZooKeeper 做服务发现?。文中提到一个示例，三个机房进行异地容灾部署，在机房3中存在一个zk节点5，但是由于网络问题，导致机房3在网络上成为孤岛。这时，ZK集群还是可用的，但是会导致与 zk节点5 无法通信，此节点就会无法写入。
 此时的状况就是对 CAP 三种特性的一个选择。但是在现实中场景下，是不允许因为防止分区容错 条件下保证 数据的强一致性而舍弃掉可用性。
 在设计注册中心时应该考虑一点，不能因为注册中心的任何原因导致服务间调用出现异常。 其实，在注册中心(服务发现)这个应用场景下，进行CAP的选择，更加注重可用性。毕竟数据不一致在一定时间范围内是可以容忍的。 如果在保证分区容错(P)的情况下，不能保证可用性(A)，就违反了注册中心(服务发现)的设计原则，以及实际生产环境的要求。  2. 随时间推移ZK的能力有所下降  在注册中心(服务发现)的场景下，随着大量的服务注册到ZK集群中，频繁的写请求、服务健康检查的心跳机制都会给ZK集群带来压力。 如果通过不同业务使用不同的ZK集群进行服务发现，就会破坏注册中心一开始的设计初衷：不得以自身任何原因破坏掉服务间的互通性。  3. ZK自身保证事务的特性  ZAB协议处理每一个写请求的同时，会在ZK集群的每一个节点上进行事务日志写入 定期通过内存数据快照同步到磁盘，保证数据一致性、数据持久性、宕机数据可恢复性。注册中心保存的数据信息包含有：注册成功的服务的可用服务地址信息。但是在 注册中心(服务发现) 的应用场景下，仅是为了保证每次可以正常调用到下游服务，并不关心下游服务的历史信息，所以说一定程度上，持久化这些数据是没有必要的。 但是从另外一个角度，注册中心还是要持久元数据信息的，譬如：服务版本、所在数据中心、服务路由权重、鉴权策略信息等等。 不单单需要提供持久性，同时还需要满足持久化数据的搜索能力。  4. 例如Dubbo服务间调用采用ZK作为注册中心  在实际环境中，不能因为ZK集群挂掉，导致服务间调用出现异常。 Dubbo服务间调用：Dubbo服务在注册到注册中心后，会将必要的数据信息在本地环境下进行持久化，也就是说，在ZK集群处于瘫痪，并且没有新服务注册进来提供被调用能力时，原有服务间的调用不能收到影响。 注册中心最关键的是在 新服务注册、节点上下线等情况下提供支持。  5.</description>
    </item>
    
    <item>
      <title>1.zookeeper分布式部署</title>
      <link>https://example.com/p/1.zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/1.zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</guid>
      <description>zookeeper分布式部署 一.配置服务器IP地址映射 [root@localhost zk]~#: vim /etc/hosts
192.168.1.111 zoo1 192.168.1.112 zoo2 192.168.1.113 zoo3 192.168.1.114 zoo4 192.168.1.115 zoo5 二.修改配置ZK文件 1.下载Zookeeper
# 进入ZK路径 wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.5.6/apache-zookeeper-3.5.6-bin.tar.gz 2.修改配置文件 进入conf目录，在配置文件前，先cp zoo_sample.cfg zoo.cfg,然后vim zoo.cfg。配置如下：
tickTime=2000 initLimit=10 syncLimit=5 dataDir=/data/ops/zk/zookeeper-3.5.6-master/conf clientPort=2181 server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 3.启动ZK ①.在每个节点的服务器依次启动服务： [root@localhost zk]~#: ./bin/zkServer.sh start 在启动过程中日志会出现异常，由于其他节点还未启动，所以属于正常情况（正常情况下，仅有最后一个节点启动不会出现异常）。待所有节点全部启动，集群会逐渐稳定下来。 ②.查询每一个节点角色 [root@localhost zk]~#: ./bin/zkServer.sh status
# LeaderNode ZooKeeper JMX enabled by default Using config: /data/ops/zk/zookeeper-3.5.6-follower/bin/../conf/zoo.cfg Client port found: 2181. Client address: localhost. Mode: leader # FollowerNode ZooKeeper JMX enabled by default Using config: /data/ops/zk/zookeeper-3.</description>
    </item>
    
  </channel>
</rss>
