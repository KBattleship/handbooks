<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linux on Quinn</title>
    <link>https://example.com/tags/linux/</link>
    <description>Recent content in Linux on Quinn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 12 Nov 2019 01:36:27 +0800</lastBuildDate><atom:link href="https://example.com/tags/linux/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title> [ Zookeeper ] 2. Zookeeper基础</title>
      <link>https://example.com/p/zookeeper-2.-zookeeper%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/zookeeper-2.-zookeeper%E5%9F%BA%E7%A1%80/</guid>
      <description>Zookeeper 1. ZAB协议  ZAB协议是为分布式协调服务Zookeeper专有的一种协议，此协议是为了应对崩溃恢复的原子广播 崩溃恢复  整个zk集群刚启动或者Leader节点宕机、重启或者不可以正常提供服务时超出一半的情况下，所有节点将会进入崩溃恢复模式 首先通过选举产生Leader 然后集群中的Follwer节点与新产生的Leader节点进行数据同步 一旦集群中一半数量的节点与Leader节点完成了数据同步，集群就会退出崩溃恢复模式，进入到消息广播模式   消息广播  Leader节点开始接受客户端的事务请求，生成事务的提案进行事务请求处理。    </description>
    </item>
    
    <item>
      <title> [ Zookeeper ] 3. Zookeeper作为注册中心的缺馅</title>
      <link>https://example.com/p/zookeeper-3.-zookeeper%E4%BD%9C%E4%B8%BA%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E7%BC%BA%E9%A6%85/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/zookeeper-3.-zookeeper%E4%BD%9C%E4%B8%BA%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E7%BC%BA%E9%A6%85/</guid>
      <description>Zookeeper作为注册中心的缺馅 一、 关于服务发现中心 1.CAP原理   C(Consistency)：一致性
在更新操作成功后，所有节点在同一时间的数据保证一致。
  A(Availability)：可用性
服务提供正常服务，并且保证都是正常响应时间。
  P(Partition Tolerance)：分区容错性
在分布式系统中，由于某一个节点出现或者网络分区出现异常情况，仍可以提供满足一致性或者用性 的服务。
  2.Zookeeper保证CP  当向注册中心查询服务列表时，可以容忍注册中心返回的是在此之前已经注册的信息，但不能接受服务直接不可用的现象 服务注册功能对可用性的标准要高于一致性 在ZK中，存在这样一种情况，如果master节点因为网络故障与其他节点失去联系，剩余节点将会重新进行Leader选举。 在ZK leader选举过程中，是一个耗时操作（大概30-120s，耗时未经实际生产验证），并且选举过程中，ZK集群是处于不可用状态，这时候将会导致服务发现、服务注册处于不可用 云部署环境下，因网络问题导致ZK集群丢失Master节点是大概率事件，虽说服务可以通过重新选举恢复到可用状态，但是漫长的选举过程耗时是不可容忍的。  二、Alibaba为什么不使用ZK作为注册中心 1. 借助一篇文章示例了解CAP选择  网上存在一篇文章阿里巴巴为什么不用 ZooKeeper 做服务发现?。文中提到一个示例，三个机房进行异地容灾部署，在机房3中存在一个zk节点5，但是由于网络问题，导致机房3在网络上成为孤岛。这时，ZK集群还是可用的，但是会导致与 zk节点5 无法通信，此节点就会无法写入。
 此时的状况就是对 CAP 三种特性的一个选择。但是在现实中场景下，是不允许因为防止分区容错 条件下保证 数据的强一致性而舍弃掉可用性。
 在设计注册中心时应该考虑一点，不能因为注册中心的任何原因导致服务间调用出现异常。 其实，在注册中心(服务发现)这个应用场景下，进行CAP的选择，更加注重可用性。毕竟数据不一致在一定时间范围内是可以容忍的。 如果在保证分区容错(P)的情况下，不能保证可用性(A)，就违反了注册中心(服务发现)的设计原则，以及实际生产环境的要求。  2. 随时间推移ZK的能力有所下降  在注册中心(服务发现)的场景下，随着大量的服务注册到ZK集群中，频繁的写请求、服务健康检查的心跳机制都会给ZK集群带来压力。 如果通过不同业务使用不同的ZK集群进行服务发现，就会破坏注册中心一开始的设计初衷：不得以自身任何原因破坏掉服务间的互通性。  3. ZK自身保证事务的特性  ZAB协议处理每一个写请求的同时，会在ZK集群的每一个节点上进行事务日志写入 定期通过内存数据快照同步到磁盘，保证数据一致性、数据持久性、宕机数据可恢复性。注册中心保存的数据信息包含有：注册成功的服务的可用服务地址信息。但是在 注册中心(服务发现) 的应用场景下，仅是为了保证每次可以正常调用到下游服务，并不关心下游服务的历史信息，所以说一定程度上，持久化这些数据是没有必要的。 但是从另外一个角度，注册中心还是要持久元数据信息的，譬如：服务版本、所在数据中心、服务路由权重、鉴权策略信息等等。 不单单需要提供持久性，同时还需要满足持久化数据的搜索能力。  4. 例如Dubbo服务间调用采用ZK作为注册中心  在实际环境中，不能因为ZK集群挂掉，导致服务间调用出现异常。 Dubbo服务间调用：Dubbo服务在注册到注册中心后，会将必要的数据信息在本地环境下进行持久化，也就是说，在ZK集群处于瘫痪，并且没有新服务注册进来提供被调用能力时，原有服务间的调用不能收到影响。 注册中心最关键的是在 新服务注册、节点上下线等情况下提供支持。  5.</description>
    </item>
    
    <item>
      <title>1.Linux-Vim</title>
      <link>https://example.com/p/1.linux-vim/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/1.linux-vim/</guid>
      <description>vim 1.基本操作命令 普通模式(Normal)  a: append current location i: insert current location o: open a line below A: append after line I: insert before line O: open a line above y: copy p: parse ^: 光标移至行首 $: 光标移至行末 G: 光标移至末行 1G: 光标移至首行 H: 光标移至当前窗口首行 M: 光标移至当前窗口中间位置  命令行模式(Command)  % s/string1/string2/g  可视化模式(Visual)  v:  插入编辑模式(Insert) 2.快速纠错 编辑模式下：  ctrl+w: 删除上一个单词 ctrl+u: 删除当前行  </description>
    </item>
    
    <item>
      <title>1.zookeeper分布式部署</title>
      <link>https://example.com/p/1.zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/1.zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</guid>
      <description>zookeeper分布式部署 一.配置服务器IP地址映射 [root@localhost zk]~#: vim /etc/hosts
192.168.1.111 zoo1 192.168.1.112 zoo2 192.168.1.113 zoo3 192.168.1.114 zoo4 192.168.1.115 zoo5 二.修改配置ZK文件 1.下载Zookeeper
# 进入ZK路径 wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.5.6/apache-zookeeper-3.5.6-bin.tar.gz 2.修改配置文件 进入conf目录，在配置文件前，先cp zoo_sample.cfg zoo.cfg,然后vim zoo.cfg。配置如下：
tickTime=2000 initLimit=10 syncLimit=5 dataDir=/data/ops/zk/zookeeper-3.5.6-master/conf clientPort=2181 server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 3.启动ZK ①.在每个节点的服务器依次启动服务： [root@localhost zk]~#: ./bin/zkServer.sh start 在启动过程中日志会出现异常，由于其他节点还未启动，所以属于正常情况（正常情况下，仅有最后一个节点启动不会出现异常）。待所有节点全部启动，集群会逐渐稳定下来。 ②.查询每一个节点角色 [root@localhost zk]~#: ./bin/zkServer.sh status
# LeaderNode ZooKeeper JMX enabled by default Using config: /data/ops/zk/zookeeper-3.5.6-follower/bin/../conf/zoo.cfg Client port found: 2181. Client address: localhost. Mode: leader # FollowerNode ZooKeeper JMX enabled by default Using config: /data/ops/zk/zookeeper-3.</description>
    </item>
    
    <item>
      <title>[Nginx] 1.Nginx入门</title>
      <link>https://example.com/p/nginx-1.nginx%E5%85%A5%E9%97%A8/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/nginx-1.nginx%E5%85%A5%E9%97%A8/</guid>
      <description>1.Nginx入门 一、Nginx为什么受青睐 ​	在介绍Nginx具体的安装、配置以及原理之前先聊聊概念常识问题。那就是目前为什么Nginx深受青睐？那我们先从Nginx是什么开始聊起。
1.Nginx是什么   简单介绍
 ***Nginx***来自于俄罗斯，是在**RamblerMedia**工作的**Igor Sysoev**使用***C***语言编写而成的跨平台轻量级高性能的*Web*服务器。***Nginx***可以运行在**Linux**、**FreeBSD**、**Solaris**(*Sun*公司的类*Unix OS*)、**MacOS**、以及**Windows**等操作系统。操作系统的不同，也给***Nginx***带来了一些好处，***Nginx***会使用当前操作系统中特有的一些高效**API**来提高自身的性能。    Nginx和它的对手们
Nginx的对手们有Apache、Lighttpd、Tomcat、Jetty、IIS，它们同为Web服务器：具备Web服务器的基本功能；基于Rest架构风格，以**统一资源描述符（URI）或统一资源定位符（URL）作为沟通依据，通过HTTP为浏览器等Client程序提供各种网络服务。
但是，这些Web服务器呢，都多多少少因为各自的定位与发展方向都不尽相同，使得每一款Web服务器都各有特色：
​	1.Tomcat、Jetty：都是面向Java语言设计的。但是它们在性能方面与Nginx没有什么可比性，因为这两款服务器都是重量级选手。可能有伙伴会很疑惑，我已经用Tomcat跑起服务，同样配置后可以直接访问为什么还要在加层外套Nginx，对于这个问题，在后边对这一点进行详细的分析。【】
​	2.IIS：这位选手呢，来自于微软家族。然后特点大家可能就很清楚了，它只能在Windows OS运行（不过网上也有工具可以把它运行在LinuxOS中，但是并不是很完美哦）。可能拉低它颜值的就是稳定性与性能了，Windows OS作为服务器的话，稳定性和部分性能都不能和类Unix OS进行媲美，所以呢，在高性能Web服务器的场合中，IIS可能就要被“淘汰”了。
​	3.Apache：这是一位压轴级选手，是发展周期最长的，毫无疑问是世界第一大Web服务器，在2012年遥遥领先其他选手。它毕竟有很多优秀的地方：稳定、开源、跨平台等。但是美中不足的是，它被设计成为了重量级、不支持高并发的Web服务器。如果有数以万计的HTTP请求同时访问，服务器就会面临大量内存消耗的问题，操作系统也会跟着收到牵连，毕竟Apache的进程做进程间切换时会给服务器的CPU带来重大压力，同时会伴随着响应效率降低，这致命的一击，导致这位来自“贵族世家”的选手在高性能Web服务器的舞台上没有了地位。
​	4.Lighttpd：与Nginx同样是轻量级、高性能的Web服务器。但是它并没有得到国内开发者的钟爱，而是被欧美的开发者们所追捧。
  恩宠&amp;ndash;Nginx
Nginx的代码也是开源的而且是最自由的2-clause BSD-like license许可证。Nginx使用的架构是基于事件驱动的，能够并发处理百万级别的TCP连接。由于Nginx的高度模块化和具有最自由的许可证，让Nginx的第三方模块扩展功能更加充实。优秀的设计还带来了极佳的稳定性体验。所以，Nginx大量应用于大流量的网站来高效处理大规模高并发连接。种种迹象表明，Nginx在性能方面很出色。
  2.Nginx的特点   更快
快主要体现在两方面：①在正常的情况下，单次请求会得到更快的响应；②在数以万计的并发请求中，Nginx可以比其他Web服务器更快的响应请求。
  高扩展性
Nginx的高度模块化决定了其具有高扩展性。它完全是由多个不同功能、不同层次、不同类型以及耦合度极低的模块组合而成。它的模块都是嵌入到二进制文件中执行，使得第三方开发的模块也一样完美支持性能。所以高并发的网站完全可以根据自身项目业务特性定制属于自己的模块。
  高可靠性
这个特点应该是选择Web服务器最基本的条件。Nginx的稳定性，大家有目共睹。国内多家高流量并发的网站在核心的服务器上大规模使用Nginx。官方提供的常用模块是非常稳定的，每一个Worker进程都相对独立，把耦合性降至最低。master进程在其中一个Worker进程出错时可以快速“拉起”新的Worker子进程提供相应的服务。
  低内存消耗
据数据测试，一般情况下，1W个不活跃的HTTP Keep-Alive连接在Nginx中消耗只有2.5MB的内存。（这也是Nginx能够支持高并发连接的基础）
  单机支持10W+的并发连接
由于现在是海量数据时代，高并发无疑成为大家青睐的对象。理论上，Nginx支持的并发连接数量取决于内存，10W+的并发连接并没有到极限。但是，能否及时处理更多的并发连接应该取决于项目业务的需求。
  热部署
master管理进程和Worker进程是相互隔离的，这使得Nginx能够彰显热部署的能力。通俗点来说，就是完全可以在7*24h不停止服务正常工作的情况下，可以升级Nginx的可执行文件、更新配置选项、更新日志文件等功能操作。
  最自由的BSD许可协议
俗话说**“众人拾柴火焰高”**。也正是BSD许可协议带来的极大优势，为**Nginx**提供更强劲的发展动力。
  综上所述，选择Nginx的核心理由还是由于它能在支持高并发请求的同时保持高效的服务。  二、Nginx的安装 1.</description>
    </item>
    
    <item>
      <title>CentOS修改系统镜像源</title>
      <link>https://example.com/p/centos%E4%BF%AE%E6%94%B9%E7%B3%BB%E7%BB%9F%E9%95%9C%E5%83%8F%E6%BA%90/</link>
      <pubDate>Tue, 12 Nov 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/centos%E4%BF%AE%E6%94%B9%E7%B3%BB%E7%BB%9F%E9%95%9C%E5%83%8F%E6%BA%90/</guid>
      <description>CentOS修改系统镜像源 一.备份当前源 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 二、切换阿里云镜像源  CentOS7 切换源  # 进入源所在路径 cd /etc/yum.repos.d/ # 下载阿里云镜像源并保存为源文件 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo  CentOS6 切换源  # 进入源所在路径 cd /etc/yum.repos.d/ # 下载网易163镜像源并保存为源文件 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS6-Base-163.repo </description>
    </item>
    
    <item>
      <title>1.KVM入门</title>
      <link>https://example.com/p/1.kvm%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sat, 12 Oct 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/1.kvm%E5%85%A5%E9%97%A8/</guid>
      <description>KVM入门  1.基础操作 ①.创建虚拟机 virt-install --name=centos-node1-0113 \ --ram=4096 --vcpus=2 \ --disk /data/virtualdisk\centos-node1-0113.img,sparse=false,bus=virtio,size=100 \ --cdrom=/root/Download/CentOS-7-x86_64-Minimal-1810.iso\ --network bridge=br2 \ --vnc --vncport=5921 --vnclisten=0.0.0.0 \ --noautoconsole --force --autostart ②.虚拟机相关操作 # 显示所有虚拟机列表 virsh list --all # 启动指定虚拟机 virsh start &amp;lt;name&amp;gt;/&amp;lt;id&amp;gt; # 停止指定虚拟机 virsh shutdown &amp;lt;name&amp;gt;/&amp;lt;id&amp;gt; # 销毁指定虚拟机(强制断电) virsh destory &amp;lt;name&amp;gt;/&amp;lt;id&amp;gt; # 登入指定虚拟机 virsh console &amp;lt;name&amp;gt;/&amp;lt;id&amp;gt; # 为虚拟机添加网卡绑定 virsh attach-interface \ --domain centos_node2-142-5922 \  --type bridge \ --source br0 \ --model virtio \ --config # 分离虚拟机网卡绑定 virsh detach-interface \ centos_node2-142-5922 \ bridge \ 52:54:00:aa:9c:18 # 查看虚拟机网卡绑定详细信息 virsh domiflist centos_node2-142-5922 # 查看网桥信息(brctl:网桥管理工具) brctl show # 虚拟机管理工具(virt-*) ③.</description>
    </item>
    
    <item>
      <title>1.TCP/IP入门</title>
      <link>https://example.com/p/1.tcp/ip%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sat, 12 Oct 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/1.tcp/ip%E5%85%A5%E9%97%A8/</guid>
      <description>TCP/IP入门 1.1 协议分层 ✌🏼😂😅😅</description>
    </item>
    
    <item>
      <title>2.KVM-双网卡网关配置</title>
      <link>https://example.com/p/2.kvm-%E5%8F%8C%E7%BD%91%E5%8D%A1%E7%BD%91%E5%85%B3%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Sat, 12 Oct 2019 01:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/2.kvm-%E5%8F%8C%E7%BD%91%E5%8D%A1%E7%BD%91%E5%85%B3%E9%85%8D%E7%BD%AE/</guid>
      <description>双网卡网关配置 1.在系统路由表文件/etc/iproute2/rt_tables增加配置
252 e1 251 e0 2.执行以下命令，并在/etc/rc.d/network追加此内容
# 此内容防止服务器重启路由失效 ip route flush table e0 ip route add default via 10.3.3.1 dev eth0 src 10.3.3.25 table e0 ip route add 127.0.0.0/8 dev lo table e0 ip rule add from 10.3.3.25 table e0 ip route flush table e1 ip route add default via 10.2.2.1 dev eth1 src 10.2.2.10 table e1 ip route add 127.0.0.0/8 dev lo table e1 ip rule add from 10.</description>
    </item>
    
  </channel>
</rss>
