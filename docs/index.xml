<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quinn</title>
    <link>https://touch-star.com/</link>
    <description>Recent content on Quinn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 22 Dec 2020 15:36:27 +0800</lastBuildDate>
    
        <atom:link href="https://touch-star.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Etcd基础入门(1)</title>
      <link>https://touch-star.com/post/middleware/etcd/etcd_interview/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://touch-star.com/post/middleware/etcd/etcd_interview/</guid>
      
        <description>ETCD  其中ETCD是一个用于存储关键数据的键值存储，ZK是一个用于管理配置等信息的中心化服务 ETCD包括 Raft 协议、存储两大模块. etcd 的使用其实非常简单，它对外提供了 gRPC 接口，我们可以通过 Protobuf 和 gRPC 直接对 etcd 中存储的数据进行管理，也可以使用官方提供的 etcdctl 操作存储的数据。
 raft协议  每一个 Raft 集群中都包含多个服务器，在任意时刻，每一台服务器只可能处于 Leader、Follower 以及 Candidate 三种状态；在处于正常的状态时，集群中只会存在一个 Leader，其余的服务器都是 Follower。
 节点选举  使用 Raft 协议的 etcd 集群在启动节点时，会遵循 Raft 协议的规则，所有节点一开始都被初始化为 Follower 状态，新加入的节点会在 NewNode 中做一些配置的初始化，包括用于接收各种信息的 Channel
 竞选流程 如果集群中的某一个 Follower 节点长时间内没有收到来自 Leader 的心跳请求，当前节点就会通过 MsgHup 消息进入预选举或者选举的流程。 如果收到 MsgHup 消息的节点不是 Leader 状态，就会根据当前集群的配置选择进入 PreElection 或者 Election 阶段，PreElection 阶段并不会真正增加当前节点的 Term，它的主要作用是得到当前集群能否成功选举出一个 Leader 的答案，如果当前集群中只有两个节点而且没有预选举阶段，那么这两个节点的 Term 会无休止的增加，预选举阶段就是为了解决这一问题而出现的。 当前节点会立刻调用 becomeCandidate 将当前节点的 Raft 状态变成候选人；在这之后，它会将票投给自己，如果当前集群只有一个节点，该节点就会直接成为集群中的 Leader 节点。</description>
      
    </item>
    
    <item>
      <title>Etcd基础入门(2)</title>
      <link>https://touch-star.com/post/middleware/etcd/etcd_sourcecode_1/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://touch-star.com/post/middleware/etcd/etcd_sourcecode_1/</guid>
      
        <description>Etcd源码阅读与分析①-raft demo Etcd 与 Zookeeper 对比  一致性协议:配置共享&amp;amp;服务发现组件的核心基础。  Zookeeper采用ZAB协议(一种类Paxos协议)实现一致性 Etcd采用Raft协议，相比Paxos协议更容易理解，工程化。   API接口: 包含有两个版本V2、V3  V2: 提供HTTP+Json方式调用 V3: 提供grpc方式调用   性能  官方测试数据显示：10000+/s写入(优于Zookeeper性能)   安全  Etcd支持TSL(权限控制优于Zookeeper)    Etcd是一个基于Raft协议的简单内存KV项目
源码分析 本文档将以etcd作者在项目中所提供的demo程序进行源码试读。demo名称为raftexample。 路径在
1.项目结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  (base) {11:45}~/etcd:master ✗ ➭ tree -d -L 1 .</description>
      
    </item>
    
    <item>
      <title>关于</title>
      <link>https://touch-star.com/about/</link>
      <pubDate>Tue, 28 Apr 2020 21:41:52 +0800</pubDate>
      
      <guid>https://touch-star.com/about/</guid>
      
        <description>Quinn 平时整理的一些笔记很杂乱，通过Blog进行一遍梳理。更好的记录这一路走来。
 Java 2 Golang 的小白 幻想某天可以摘星辰 因为人间烟火不易，更愿悠哉于技术这片净土 兴趣、专注、痴迷 Stay hungry, Stay foolish  </description>
      
    </item>
    
    <item>
      <title>Dubbo基础概念</title>
      <link>https://touch-star.com/post/middleware/dubbo/dubbobase/</link>
      <pubDate>Tue, 22 Dec 2020 15:36:27 +0800</pubDate>
      
      <guid>https://touch-star.com/post/middleware/dubbo/dubbobase/</guid>
      
        <description>Dubbo基础概念 1.Dubbo核心组件  Provider： 暴露服务的服务提供方 Consumer： 调用远程服务的消费方 Register： 服务注册与发现注册中心 Monitor： 监控中心和访问调用统计 Container：服务运行时容器   Dubbo分层主要为业务层、RPC层和Remote层，如果把每层进行详细划分的话，整体划分为：
  业务层：  service: 包含各业务代码的接口与实现；   RPC层：  config: 配置层，主要围绕ServiceConfig(暴露的服务配置)和ReferenceConfig(引用的服务配置)两个类展开，初始化配置信息； proxy: 服务代理层，不论生产者还是消费者，Dubbo都会生成一个代理类，在调用远程接口时，就可以像本地接口一样，代理层自动做远程调用并返回结果； registry: 注册层，负责Dubbo框架的服务注册与发现； cluster: 集群容错层，主要负责远程调用失败时的集群容错策略(如快速失败、快速重试等)； monitor: 监控层，负责监控统计调用次数和调用时间等； protocol: 远程调用层，封装RPC调用具体过程，是Invoker暴露和引用的主要功能入口，负责管理Invoker的整个生命周期；   Remote层：  exchange: 信息交换层，封装请求相应模式，如同步请求转换为异步请求； transport: 网络传输层，把网络传输抽象为统一接口； serialize: 序列化层，将需要网络传输的数据极性序列化，转为二进制流。    2.Dubbo服务器注册与发现流程  a. Container负责启动，加载，运行服务提供者 b. Provider启动时，向注册中心注册自己并提供服务 c. Consumer启动时，向注册中心订阅自已需调用服务 d. Register返回服务提供者地址列表给服务消费者，如运行期间，服务提供者发生变动，将通过长连接推送至服务消费者 e. Consumer通过负载均衡算法(软方式)，选取注册中心所返回的服务提供者列表中的一个节点进行调用，如果调用失败将尝试其他节点进行调用 f. Consumer、Provider将调用次数、时间记录于内存中，并定时每分钟发送至Monitor监控中心  3.Dubbo项目结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  $ tree -L 1 .</description>
      
    </item>
    
    <item>
      <title>1.Docker命令</title>
      <link>https://touch-star.com/post/cloudnative/docker/docker/1-command/</link>
      <pubDate>Sat, 22 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://touch-star.com/post/cloudnative/docker/docker/1-command/</guid>
      
        <description>Docker命令  docker [option] command
   option
 &amp;ndash;config string: 客户端配置文件的位置 &amp;ndash;context string[-c]: 用于连接到守护程序的上下文的名称 &amp;ndash;debug[-D]: 调试模式 &amp;ndash;host list[-H]: 要连接的守护程序套接字 &amp;ndash;log-level string[-l]: 日志等级[ debug | info | warn | error | fatal ]默认为info &amp;ndash;tls: 使用加密模式 &amp;ndash;tlscacert string: 签名证书文件路径 &amp;ndash;tlscert string: 密钥文件路径 &amp;ndash;tlskey string: key文件路径 &amp;ndash;tlsverify: 使用加密并验证远程连接 &amp;ndash;version[-v]: 版本信息    Management Commands(管理命令)
 builder: 管理构建 config: 管理Docker配置 container: 管理容器 context: 管理镜像构建上下文 image: 管理镜像 network: 管理网络 node: 管理Swarm节点 plugin: 管理插件 secret: 管理Docker secrets service: 管理服务 stack: 管理Docker stacks swarm: 管理Swarm集群 system: 查看系统信息 trust: 管理对Docker映像的信任 volume: 管理卷    Commands(命令)</description>
      
    </item>
    
    <item>
      <title>2.Docker安装</title>
      <link>https://touch-star.com/post/cloudnative/docker/docker/2-dockerinstall/</link>
      <pubDate>Sat, 22 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://touch-star.com/post/cloudnative/docker/docker/2-dockerinstall/</guid>
      
        <description>Docker安装  存储库安装   安装yum-config-manager所需依赖包
1 2 3  $~:sudo yum install -y yum-utils \  device-mapper-persistent-data \  lvm2     通过yum-config-manager添加存储库
1 2 3  $~:sudo yum-config-manager \  --add-repo \  https://download.docker.com/linux/centos/docker-ce.repo     列出存储库中排序后可用的全部版本
1  yum list docker-ce --showduplicates | sort -r     进行安装
1 2 3 4  # 指定版本号安装 sudo yum install docker-ce-&amp;lt;VERSION_STRING&amp;gt; docker-ce-cli-&amp;lt;VERSION_STRING&amp;gt; containerd.io # 安装最新版本（不指定版本号默认为最新） sudo yum install docker-ce docker-ce-cli containerd.</description>
      
    </item>
    
    <item>
      <title>3.Docker之jdk1.8最简镜像构建</title>
      <link>https://touch-star.com/post/cloudnative/docker/docker/3-jdk_mirror/</link>
      <pubDate>Sat, 22 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://touch-star.com/post/cloudnative/docker/docker/3-jdk_mirror/</guid>
      
        <description>Docker之jdk1.8最简镜像构建 1.准备JRE 在Java下载网站下载JRE。 Tips:此JRE为Oracle作品，而非Openjdk
2.精简JRE中无关文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # 进入已经下载jre压缩包的路径,执行解压 tar xzvf ~/Downloads/jre-8u241-linux-x64.tar.gz&amp;amp;&amp;amp;cd jre1.8.0_241 # 删除说明、其他文档 rm -rf COPYRIGHT LICENSE README \ THIRDPARTYLICENSEREADME-JAVAFX.txt \ THIRDPARTYLICENSEREADME.txt \ Welcome.html # 删除非必要依赖文件 rm -rf lib/plugin.jar \  lib/ext/jfxrt.jar \  bin/javaws \  lib/javaws.jar \  lib/desktop \  plugin \  lib/deploy* \  lib/*javafx* \  lib/*jfx* \  lib/amd64/libdecora_sse.</description>
      
    </item>
    
    <item>
      <title>4.SpringBoot项目Docker化</title>
      <link>https://touch-star.com/post/cloudnative/docker/docker/4-docker4springboot/</link>
      <pubDate>Sat, 22 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://touch-star.com/post/cloudnative/docker/docker/4-docker4springboot/</guid>
      
        <description>SpringBoot项目Docker化 一 </description>
      
    </item>
    
    <item>
      <title>5.Docker-本地构建none包处理</title>
      <link>https://touch-star.com/post/cloudnative/docker/docker/5-docker4rmnone/</link>
      <pubDate>Sat, 22 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://touch-star.com/post/cloudnative/docker/docker/5-docker4rmnone/</guid>
      
        <description>Docker-本地构建none包处理 踩坑①.打包构建Dockerfile镜像 每次本地打包构建Dockerfile镜像，如果更新镜像版本号会出现none的镜像在仓库中
1 2 3 4 5 6 7 8 9 10 11 12 13  # 停掉none相关的镜像进程占用 docker rm $(docker ps -a | grep &amp;#34;Exited&amp;#34; | awk &amp;#39;{print $1 }&amp;#39;) # 递归依次从仓库移除这些镜像 docker rmi $(docker images | grep &amp;#34;^&amp;lt;none&amp;gt;&amp;#34; | awk &amp;#34;{print $3}&amp;#34;) # 或者，使用一下命令进行移除 docker image prune # (此命令用于删除未使用的映像) # docker image prune [options] # -- options可选值： # -a 显示所有映像(默认隐藏中间映像) # -f 不提示确认，强制直接执行删除   </description>
      
    </item>
    
    <item>
      <title>6.关于命名空间(Linux Namespace)</title>
      <link>https://touch-star.com/post/cloudnative/docker/docker/6-namespace-linux4docker/</link>
      <pubDate>Sat, 22 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://touch-star.com/post/cloudnative/docker/docker/6-namespace-linux4docker/</guid>
      
        <description>关于命名空间(Linux Namespace) 概念  1.Linux Namespace 是Kernel的一个功能，可以针对一系列的系统资源进行隔离。例如：PID(process id)、UID(User id)、Network so on.
  2.就像chroot允许把当前目录变成根目录一样进行隔离。
  3.Namespace进行隔离用户，当前用户将在特定的Namespace中具有root权限。但在真是物理机层面，此用户仍然是以UID运行的那个用户。
 Linux包含的Namespace类型    Type Params Kernel Effect Information     Mount CLONE_NEWNS 2.4.19 隔离Namespace下的文件系统   UTS CLONE_NEWUTS 2.6.19 用作隔离nodename和domainname   IPC CLONE_NEWIPC 2.6.19 隔离System V IPC 和POSIX Message queues   PID CLONE_NEWPID 2.6.24 针对进程ID进行隔离   Network CLONE_NEWNET 2.6.29 用于隔离网络设备、IP地址端口等网络栈   User CLONE_NEWUSER 3.8 用于隔离用户及用户组    #Demo Coding </description>
      
    </item>
    
  </channel>
</rss>
