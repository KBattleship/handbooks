<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s on Quinn</title>
    <link>https://example.com/tags/k8s/</link>
    <description>Recent content in k8s on Quinn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 18 Sep 2020 15:36:27 +0800</lastBuildDate><atom:link href="https://example.com/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[ Kubernetes ] 3-1.深入Service</title>
      <link>https://example.com/p/kubernetes-3-1.%E6%B7%B1%E5%85%A5service/</link>
      <pubDate>Fri, 18 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-3-1.%E6%B7%B1%E5%85%A5service/</guid>
      <description>深入Service  Service是Kubernetes最为核心的概念。Service可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发至后端的各个容器应用上。
 1.Service参数定义 # 必填，版本号apiVersion:v1# 必填kind:Service# 必填，元数据metadata:# 必填，Service名称(符合RFC 1035规范)name:string# 必填，命名空间(默认default)namespace:string# 自定义标签属性列表labels:- name:string# 自定义注解属性列表annotations:- name:string# 必填，配置内容详细描述 spec:# 必填，LabelSelector配置，将选择具有特定Label标签的Pod对象作为管理对象selector:[]# 必填，可选值[ClusterIP | NodePort | LoadBalancer]## ClusterIP:虚拟服务IP地址，该地址用于Kubernetes集群内部Pod对象访问，#在Node节点上Kube-proxy通过设置的Iptables规则进行转发## NodePort:使用宿主机端口，使能够访问各Node的外部客户端通过Node的#IP地址和端口号即可访问到应用。## LoadBalancer:使用外接负载均衡器完成到服务的负载分发，需要#spec.status.loadBalaner字段指定外部负载均衡器的IP地址，并同时定义#nodePort和clusterIP，用于公有云环境。type:string# 虚拟服务IP地址：当type为ClusterIP时，如果不指定将自动进行分配；也可手动指定。#当type为LoadBalancer时，必须指定clusterIP:string# 是否支持Session，可选值为ClientIP，默认值为空。#ClientIP表示将同一个客户端(有客户端IP地址决定)的访问请求都转发到同一个后端#Pod对象sessionAffinity:string# Service需要暴露的端口列表ports:- name:string# 端口协议：TCP/UDP(默认TCP)protocol:string# 服务监听端口号port:int# 需要转发到后端Pod对象的端口号targetPort:int# type为NodePort时，指定映射到物理机的端口号nodePort:int# type为LoadBalancer时，设置外部负载均衡器的地址，用于公有云环境。status:# 外部负载均衡器loadBalancer:ingress:ip:stringhostname:string</description>
    </item>
    
    <item>
      <title>[ Kubernetes ] 2-1.了解Pod对象</title>
      <link>https://example.com/p/kubernetes-2-1.%E4%BA%86%E8%A7%A3pod%E5%AF%B9%E8%B1%A1/</link>
      <pubDate>Thu, 17 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-2-1.%E4%BA%86%E8%A7%A3pod%E5%AF%B9%E8%B1%A1/</guid>
      <description>了解Pod对象 1.Pod参数定义 # 必填,版本号apiVersion:stringkind:Pod# 必填,元数据metadata:# 必填,Pod对象的名称(命名规范需要符合RFC 1035规范)name:string# 必填,Pod对象所属的命名空间,默认值为defaultnamespace:string# 自定义标签列表(取值类型:List)labels:- name:string# 自定义标签注解(取值类型:List)annotations:- name:string# 必填,Pod对象中容器的详细定义 spec:# 必填,Pod对象容器列表(取值类型:List)containers:# 必填,容器的名称(需要符合RFC 1035规范)- name:string# 必填,容器镜像名称image:string# 获取镜像的策略，默认值为:Always# Always: 每次都尝试重新下载镜像# Never: 仅使用本地镜像# IfNotPresent: 如果本地不存在，就下载镜像imagePullPolicy:[Always | Never | IfNotPresent]# 容器启动命令列表，若不指定则使用镜像打包时使用的启动命令command:[string]# 容器的启动命令参数列表args:[string]# 容器的工作目录workingDir:string# 挂载到容器内部的存储卷配置(取值类型:List)volumeMounts:# 引用Pod定义的共享存储卷的名称，需使用镜像volumes[]部分定义的共享卷名称- name:string# 存储卷在容器内Mount的绝对路径(应少于512个字符)mountPath:string# 是否只读模式,默认false(读写模式)readOnly:boolean# 容器需要暴露的端口号(取值类型:List) ports:# 端口的名称- name:string# 容器需要监听的端口号containerPort:int# 容器所在主机需要监听的端口号，默认与containerPort相同# (设置hostPort时，同一台宿主机将无法启动该容器的第二副本，由于端口占用问题)hostPort:int# 端口协议[TCP/UDP],默认为TCPprotocol:string# 容器运行前需要设置的环境变量列表env:# 环境变量的名称- name:string# 环境变量的值value:string# 资源限制和资源请求的设置resource:# 资源限制设置limits:# CPU限制(单位为：core)将用于docker run --cpu-shares参数cpu:string# 内存限制(单位为：MiB/GiB)将用于docker run --memory参数memory:string# 资源限制设置(请求)requests:# CPU请求(单位为：core)将用于容器启动的初始化可用数量cpu:string# 内存请求(单位为：MiB/GiB)将用于容器启动的初始化可用数量memory:string# 对Pod对象内各个容器进行安全检查的设置，当探测无响应几次后，将自动重启该容器# 包含[exec | httpGet | TcpSocket]三种方式，任选其一即可livenessProbe:exec:# 需要执行的脚本command:[string]httpGet:# 请求路径path:string# 请求端口port:numberhost:stringscheme:stringhttpHeader:- name:stringvalue:stringtcpSocket:port:number# 完成容器启动后首次进行探测的时间(单位为：s)initialDelaySeconds:0# 对容器健康检查探测等待超时时间(单位为：s)，默认值为1timeoutSeconds:0# 对容器健康检查的探测时间周期(单位为：s)，默认值为10periodSeconds:0successThreshold:0failureThreshold:0securityContext:privileged:boolean## Pod对象的重启策略，可选值[Always | Never | OnFailure]## Always: Pod对象一旦终止，则不关心容器是如何停止的，kubelet都将重器容器## Never: Pod对象终止后，kubelet将退出码返回给Master，不再重启该容器## OnFailure: 只有当Pod对象以非零退出码终止时，kubelet才会重启该容器# (容器正常结束的退出码为零)#restartPolicy:[Always | Never | OnFailure]# 表示将Pod对象调度到包含这些label的Node上(以key:value形式指定)nodeSelector:object# Pull镜像时使用的secret名称(以name:secretValue形式指定)imagePullSecrets:- name:string# 是否使用主机模式(默认值为:false)## 如果设置为true，表示容器使用宿主机网络，不再使用Docker网桥# 该Pod对象将无法在同一台宿主机上启动第二个副本hostNetwork:boolean# 在该Pod对象上定义的共享储存卷列表volumes:# 共享储存卷名称，一个Pod对象中每个储存卷定义一个名称(命名应按照RFC 1035规范)- name:string# Pod对象同生命周期的一个临时目录，值为{}空对象emptyDir:{}# 挂载Pod对象所在宿主机的目录hostPath:# 将用于容器中mount的目录path:string# 挂载集群中预定义的secret对象到容器内部secret:secretName:stringitems:- key:stringpath:string# 挂载集群预定义的configMap对象到容器内部configMap:name:stringitems:- key:stringpath:string2.</description>
    </item>
    
    <item>
      <title>[ Kubernetes ] 2-2.SpringBoot--Pods项目初体验</title>
      <link>https://example.com/p/kubernetes-2-2.springboot-pods%E9%A1%B9%E7%9B%AE%E5%88%9D%E4%BD%93%E9%AA%8C/</link>
      <pubDate>Thu, 17 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-2-2.springboot-pods%E9%A1%B9%E7%9B%AE%E5%88%9D%E4%BD%93%E9%AA%8C/</guid>
      <description>SpringBoot&amp;ndash;Pods项目初体验  在SpringBoot项目中通过fabric8打包插件构建docker镜像
 通过Kubernetes的接口请求Pod对象，相信信息如下：
{ &amp;#34;kind&amp;#34;: &amp;#34;Pod&amp;#34;, &amp;#34;apiVersion&amp;#34;: &amp;#34;v1&amp;#34;, &amp;#34;metadata&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;kubernetes-hello-world-779c4c748b-2rv27&amp;#34;, &amp;#34;generateName&amp;#34;: &amp;#34;kubernetes-hello-world-779c4c748b-&amp;#34;, &amp;#34;namespace&amp;#34;: &amp;#34;default&amp;#34;, &amp;#34;selfLink&amp;#34;: &amp;#34;/api/v1/namespaces/default/pods/kubernetes-hello-world-779c4c748b-2rv27&amp;#34;, &amp;#34;uid&amp;#34;: &amp;#34;dea46f9f-cd4b-11e9-b38e-025000000001&amp;#34;, &amp;#34;resourceVersion&amp;#34;: &amp;#34;34209&amp;#34;, &amp;#34;creationTimestamp&amp;#34;: &amp;#34;2019-09-02T06:35:35Z&amp;#34;, &amp;#34;labels&amp;#34;: { &amp;#34;app&amp;#34;: &amp;#34;kubernetes-hello-world&amp;#34;, &amp;#34;group&amp;#34;: &amp;#34;org.springframework.cloud&amp;#34;, &amp;#34;pod-template-hash&amp;#34;: &amp;#34;779c4c748b&amp;#34;, &amp;#34;provider&amp;#34;: &amp;#34;fabric8&amp;#34;, &amp;#34;version&amp;#34;: &amp;#34;1.1.0.M2&amp;#34; }, &amp;#34;annotations&amp;#34;: { &amp;#34;fabric8.io/docs-url&amp;#34;: &amp;#34;scp://static.springframework.org/var/www/domains/springframework.org/static/htdocs/spring-cloud/docs/kubernetes-hello-world/1.1.0.M2/spring-cloud-kubernetes/spring-cloud-kubernetes-examples/kubernetes-hello-world&amp;#34;, &amp;#34;fabric8.io/iconUrl&amp;#34;: &amp;#34;img/icons/spring-boot.svg&amp;#34;, &amp;#34;fabric8.io/metrics-path&amp;#34;: &amp;#34;dashboard/file/kubernetes-pods.json/?var-project=kubernetes-hello-world\u0026var-version=1.1.0.M2&amp;#34;, &amp;#34;fabric8.io/scm-con-url&amp;#34;: &amp;#34;scm:git:git://github.com/spring-cloud-incubator/spring-cloud-kubernetes.git/spring-cloud-kubernetes-examples/kubernetes-hello-world&amp;#34;, &amp;#34;fabric8.io/scm-devcon-url&amp;#34;: &amp;#34;scm:git:ssh://git@github.com/spring-cloud-incubator/spring-cloud-kubernetes.git/spring-cloud-kubernetes-examples/kubernetes-hello-world&amp;#34;, &amp;#34;fabric8.io/scm-tag&amp;#34;: &amp;#34;HEAD&amp;#34;, &amp;#34;fabric8.io/scm-url&amp;#34;: &amp;#34;https://github.com/spring-cloud-incubator/spring-cloud-kubernetes/spring-cloud-kubernetes-examples/kubernetes-hello-world&amp;#34; }, &amp;#34;ownerReferences&amp;#34;: [ { &amp;#34;apiVersion&amp;#34;: &amp;#34;apps/v1&amp;#34;, &amp;#34;kind&amp;#34;: &amp;#34;ReplicaSet&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;kubernetes-hello-world-779c4c748b&amp;#34;, &amp;#34;uid&amp;#34;: &amp;#34;dea3c6a5-cd4b-11e9-b38e-025000000001&amp;#34;, &amp;#34;controller&amp;#34;: true, &amp;#34;blockOwnerDeletion&amp;#34;: true } ] }, &amp;#34;spec&amp;#34;: { &amp;#34;volumes&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;default-token-b4crd&amp;#34;, &amp;#34;secret&amp;#34;: { &amp;#34;secretName&amp;#34;: &amp;#34;default-token-b4crd&amp;#34;, &amp;#34;defaultMode&amp;#34;: 420 } } ], &amp;#34;containers&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;spring-boot&amp;#34;, &amp;#34;image&amp;#34;: &amp;#34;cloud/kubernetes-hello-world:1.</description>
    </item>
    
    <item>
      <title>[ Kubernetes ] 1-4.kubeadm命令使用</title>
      <link>https://example.com/p/kubernetes-1-4.kubeadm%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Mon, 14 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-1-4.kubeadm%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/</guid>
      <description>安装初始化K8s集群 1.安装K8s 1.1CentOS安装   预先准备工作
# 修改设置主机名称 hostnamectl set-hostname master # 绑定主机各节点hosts 192.168.0.1 master 192.168.0.2 node1 192.168.0.3 node2 # 验证每节点的Mac地址与UUID是否唯一 # mac地址注意查看网卡 cat /sys/class/net/eth1/address cat /sys/class/dmi/id/product_uuid # 关闭缓存交换swap swapoff -a # 临时关闭 sed -i.bak &amp;#39;/swap/s/^/#/&amp;#39; /etc/fstab #永久关闭   安装Kubernetes
# 设置K8s安装源，由于防火墙问题使用阿里云源 cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo   [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 更新源缓存 yum clean all yum -y makecache
# 查看k8s版本 yum list kubelet --showduplicates | sort -r # 默认安装最新版本 yum install -y kubelet kubeadm kubectl # 选择指定版本进行安装 yum install -y kubelet-&amp;lt;version&amp;gt; kubeadm-&amp;lt;version&amp;gt; kubectl-&amp;lt;version&amp;gt; ```  1.</description>
    </item>
    
    <item>
      <title>[ Kubernetes ] 1-2.Kubectl命令行工具</title>
      <link>https://example.com/p/kubernetes-1-2.kubectl%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Sat, 12 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-1-2.kubectl%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</guid>
      <description>Kubectl命令行工具 1.kubectl用法  $~: kubectl [command] [TYPE] [NAME] [flags]
   [command] 子命令。用于操作Kubernetes集群资源对象。
可取值：[create | delete | describe | get | apply]
  [TYPE] 资源对象的类型。区分大小写
备注：可以通过单数形式、复数形式、简写形式表示。
# 例：不同写法的Type,但是效果一致 kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1   [NAME] 资源对象名称。区分大小写 备注： 如果不指定名称，将返回属于TYPE的所有对象列表。
# 例：返回所有对象列表 kubectl get pods   [flags] kubectl子命令的可选参数
  2.kubectl操作实例   创建资源对象
# 由配置文件(*.yaml)创建一次性对象 # 创建一个对象 kubectl create -f service.</description>
    </item>
    
    <item>
      <title>[ Kubernetes ] 1-3.kubeadm命令使用</title>
      <link>https://example.com/p/kubernetes-1-3.kubeadm%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Sat, 12 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-1-3.kubeadm%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/</guid>
      <description>kubeadm命令使用 一、kubeadm概述 $~:kubeadm --help # kubeadm [command] |———— alpha [command] |———— completion |———— config |———— images |———— list 列出所有依赖镜像 |———— pull |———— help 查看命令详细描述 |———— init 初始化Kubernetes集群Master |———— join 在Kubernetes集群中增加Node |———— reset 重置Kubernetes集群 |———— token |———— upgrade |———— version </description>
    </item>
    
    <item>
      <title>[ Kubernetes ] 1-5.K8s之Helm包管理工具</title>
      <link>https://example.com/p/kubernetes-1-5.k8s%E4%B9%8Bhelm%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Sat, 12 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-1-5.k8s%E4%B9%8Bhelm%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/</guid>
      <description>K8s之Helm包管理工具  Helm 是 Deis 开发的一个用于 Kubernetes 应用的包管理工具，主要用来管理 Charts。有点类似于 Ubuntu 中的 APT 或 CentOS 中的 YUM。
 一、安装 Helm Release Link
1.OS-CentOS 当前使用版本为Helm v3.1.2 linux
# 下载二进制可执行文件压缩包 wget -O /data/helm.tar.gz https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz # 解压 tar -xzvf /data/helm.tar.gz # 移动helm二进制文件，方便全局访问 mv linux-amd64/helm /usr/local/bin/helm 2.OS-MacOS ①.自动安装 PS：操作系统已安装brew工具
brew install helm ②.手动安装 当前使用版本为Helm v3.1.2 darwin
# 下载二进制可执行文件压缩包 wget -O ~/helm.tar.gz https://get.helm.sh/helm-v3.1.2-darwin-amd64.tar.gz # 解压缩 tar -xzvf ~/helm.tar.gz # 移动helm二进制文件，方便全局访问 mv darwin-amd64/helm /usr/local/bin/helm 二、入门 1.调整helm源 # 查看源 helm repo list # 设置国内镜像源(选用阿里云源) helm repo add stable https://kubernetes.</description>
    </item>
    
    <item>
      <title>[ Kubernetes ] 0.Kubernetes</title>
      <link>https://example.com/p/kubernetes-0.kubernetes/</link>
      <pubDate>Fri, 11 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-0.kubernetes/</guid>
      <description>Kubernetes Kubernetes(K8s)，译文成为【舵手】。从官网的Logo可以看出是轮船上的舵。结合container(集装箱，容器)的概念，Kubernetes看起来则是管理这些容器的。是一个自动化的容器编排平台，负责应用的部署、应用的弹性以及应用的管理，前提则是这些应用都是基于容器的。
核心功能   服务的发现与负载均衡
附属组件KubeDNS为系统内置了服务发现功能，可以将每一个Service增加DNS名称，使得集群内节点直接通过此名称访问到；同时Service通过iptables、ipvs支持了负载均衡。
  自动装箱
构建于容器之上，基于资源依赖及其他约束在不影响其可用性的情况下自动完成容器的部署工作。
  自我修复
容器故障后自动重启、节点故障后自动重新进行容器调度、节点健康状态检查异常后会关闭容器进行重新创建。
  水平扩展
通过命令、UI手动水平扩展、基于CPU等资源负载率进行自动水平扩展。
  自动发布与回滚
使用灰度方式更新应用或其配置，过程中的应用健康状态将得到监控，以保证不在同一时刻kill掉所有实例；同时，过程中健康状态出现异常情况，将会立即自动执行回滚。
  存储编排
Pod对象自动挂载不同类型的存储系统。
  批量处理
支持批处理作业、CI持续集成。
  秘钥与配置管理
K8s的configMap将配置与Docker镜像解耦，更新配置时，无需重新构建Docker镜像。同时，敏感数据将通过Secret对象进行解耦，以保障一定程度上的最大安全。
  </description>
    </item>
    
    <item>
      <title>[ Kubernetes ] 1-1.初识K8s</title>
      <link>https://example.com/p/kubernetes-1-1.%E5%88%9D%E8%AF%86k8s/</link>
      <pubDate>Fri, 11 Sep 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/kubernetes-1-1.%E5%88%9D%E8%AF%86k8s/</guid>
      <description>初识K8s 术语及原理   Master(主节点:control plane) 集群中的神经中枢网关。负责整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控、纠错等管理功能。
  ApiServer
集群的网关。
 负责输出RESTful风格K8s接口，则是通往集群所有REST操作命令的入口，并负责接收、校验、相应所有的REST请求，最终结果状态存储在etcd中。
   Controller Manager
负责生命周期功能及API业务逻辑。
 **a.生命周期功能：**Namespace创建和生命周期、Event垃圾回收、Pod对象终止相关的垃圾回收、级联垃圾回收、Node的垃圾回收 **b.API业务逻辑：**由ReplicaSet执行的Pod对象扩展
   Scheduler
在API Server确认Pod对象之后，由调度器(Scheduler)根据集群中各节点的可用资源状态、目标运行容器的资源需求做出调度策略。
  Etcd
基于Raft协议开发的分布式键值存储，用于服务发现、共享配置、保证一致性(数据库的主从节点选择，分布式锁等)。
 a.etcd是独立的组件，并不属于K8s集群。 b.生产环境etcd应该按照集群方式部署运行，以提升高可用。
     Node(从节点:worker node)
工作节点。负责接收来自Master节点的工作指令并根据指令相应的创建或者销毁Pod对象，以及调整网络规则以合理的路由转发流量。
  Pod
Kubernetes并不会直接运行容器，而是使用一个抽象的资源对象封装一个或者多个容器，此对象就是Pod对象。 是K8s最小的调度单元。 **一个Pod对象可以拥有多个Container容器应用。**通常情况下，这些在同一个Pod对象中的Container容器是高耦合。因为其共用同一个Pod对象下的网络名称空间、存储资源、UTS命名空间(同一个主机名称)、PID命名空间(不同应用程序可以看到其他应用程序的PID)
  Pod Controller(Pod 控制器)
虽说Pod对象是最小的调度单元，但实际应用中，并不会直接部署、管理Pod对象，而是借助Pod Controller对其进行管理。
  Replication Controller(复制控制器)
K8s的核心概念，用于管理Pod的声明周期。在主节点中，控制管理器进程同RC的定义完成Pod的创建、监控、启停等操作。
  Replica Set
保证在某个时间点儿上，一定数量的Pod对象在运行。是Replication Controller的升级版本。
 主要区别在于Selector选择器 Replica Set:支持集合级别的选择器。 Replication Controller:支持在等号描述的选择器。</description>
    </item>
    
    <item>
      <title>[ Etcd ] 1.基础入门(1)</title>
      <link>https://example.com/p/etcd-1.%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A81/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/p/etcd-1.%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A81/</guid>
      <description>ETCD  其中ETCD是一个用于存储关键数据的键值存储，ZK是一个用于管理配置等信息的中心化服务 ETCD包括 Raft 协议、存储两大模块. etcd 的使用其实非常简单，它对外提供了 gRPC 接口，我们可以通过 Protobuf 和 gRPC 直接对 etcd 中存储的数据进行管理，也可以使用官方提供的 etcdctl 操作存储的数据。
 raft协议  每一个 Raft 集群中都包含多个服务器，在任意时刻，每一台服务器只可能处于 Leader、Follower 以及 Candidate 三种状态；在处于正常的状态时，集群中只会存在一个 Leader，其余的服务器都是 Follower。
 节点选举  使用 Raft 协议的 etcd 集群在启动节点时，会遵循 Raft 协议的规则，所有节点一开始都被初始化为 Follower 状态，新加入的节点会在 NewNode 中做一些配置的初始化，包括用于接收各种信息的 Channel
 竞选流程 如果集群中的某一个 Follower 节点长时间内没有收到来自 Leader 的心跳请求，当前节点就会通过 MsgHup 消息进入预选举或者选举的流程。 如果收到 MsgHup 消息的节点不是 Leader 状态，就会根据当前集群的配置选择进入 PreElection 或者 Election 阶段，PreElection 阶段并不会真正增加当前节点的 Term，它的主要作用是得到当前集群能否成功选举出一个 Leader 的答案，如果当前集群中只有两个节点而且没有预选举阶段，那么这两个节点的 Term 会无休止的增加，预选举阶段就是为了解决这一问题而出现的。 当前节点会立刻调用 becomeCandidate 将当前节点的 Raft 状态变成候选人；在这之后，它会将票投给自己，如果当前集群只有一个节点，该节点就会直接成为集群中的 Leader 节点。</description>
    </item>
    
    <item>
      <title>[ Etcd ] 2.基础入门(2)</title>
      <link>https://example.com/p/etcd-2.%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A82/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/p/etcd-2.%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A82/</guid>
      <description>Etcd源码阅读与分析①-raft demo Etcd 与 Zookeeper 对比  一致性协议:配置共享&amp;amp;服务发现组件的核心基础。  Zookeeper采用ZAB协议(一种类Paxos协议)实现一致性 Etcd采用Raft协议，相比Paxos协议更容易理解，工程化。   API接口: 包含有两个版本V2、V3  V2: 提供HTTP+Json方式调用 V3: 提供grpc方式调用   性能  官方测试数据显示：10000+/s写入(优于Zookeeper性能)   安全  Etcd支持TSL(权限控制优于Zookeeper)    Etcd是一个基于Raft协议的简单内存KV项目
源码分析 本文档将以etcd作者在项目中所提供的demo程序进行源码试读。demo名称为raftexample。 路径在
1.项目结构 (base) {11:45}~/etcd:master ✗ ➭ tree -d -L 1 . . ├── Documentation # 项目文档 ├── auth # 认证授权 ├── client # 客户端相关(v2) ├── clientv3 # 客户端相关(v3) ├── contrib # (待验证) ├── default.etcd # 已编译完成的etcd ├── embed # 封装的etcd函数 ├── etcdctl # etcd操作命令，命令行客户端 ├── etcdmain # main函数入口这里 ├── etcdserver # 服务端相关 ├── functional # 目测验证功能测试套件 ├── hack # 开发者相关 ├── integration # (待验证) ├── lease # 实现etcd租约 ├── logos # 日志相关 ├── mvcc # MVCC存储相关 ├── pkg # 通用依赖库 ├── proxy # 代理相关Http、Https、Socks ├── raft # raft一致性协议实现 ├── scripts # 各类脚本 ├── security # 安全相关 ├── tests # (待验证) ├── tools # 工具 ├── vendor # go vendor依赖环境 ├── version # 版本信息 └── wal # Write-Ahead-Log实现 </description>
    </item>
    
    <item>
      <title>[ Docker ] 8.文件系统之UFS</title>
      <link>https://example.com/p/docker-8.%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B9%8Bufs/</link>
      <pubDate>Sun, 23 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/docker-8.%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B9%8Bufs/</guid>
      <description>文件系统之UFS UFS 联合文件系统[Union File System]，把其他文件系统联合到一个联合挂载点的文件系统服务(适用于Linux、FreeBSD、NetBSD OS)。
 原理： 使用branch把不同文件系统的文件、目录「透明的」进行覆盖，形成一个单一一直的文件系统。branch具有要么read-only,要么read-write的特点。
  思想： 写时复制(copy-on-write),如果一个资源重复，但并未被修改，将不被立即创建出新的资源，直接为新旧实例提供共享。创建新资源将发生在第一次被修改写入时。该资源共享方式，可以明显降低未修改资源复制时的消耗，但同样的，也会在写入修改是增加部分开销。
  AFUS(Advanced Multi-Layered Unification FileSystem)  </description>
    </item>
    
    <item>
      <title>[ Docker ] 7.关于Linux的Cgroups</title>
      <link>https://example.com/p/docker-7.%E5%85%B3%E4%BA%8Elinux%E7%9A%84cgroups/</link>
      <pubDate>Sat, 22 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/docker-7.%E5%85%B3%E4%BA%8Elinux%E7%9A%84cgroups/</guid>
      <description>关于Linux的Cgroups 概念  Linux Cgroups(Control Groups)在Linux Namespace为进程隔离出一定空间的基础上为此进行的资源限制、控制以及统计的能力。资源包含有：CPU、内存、存储、网络等。通过Cgroups可以限制某个进程的资源占用、并且可以实时监控进程以及统计信息。
 cgroups各模块   cgroup: 针对进程进行分组的一种策略机制。
 每一个cgroup中包含有一组进程。并且可以使用subsystem模块进行参数控制作用于此cgroup上的进程。
   subsystem: 此模块对资源进行控制。
 Ubuntu OS可以通过apt install cgroup-bin安装命令行工具，使用lssubsys查看Kernel所支持的subsystem list。
  blkio: 设置对块设备输入输出进行控制 cpu: 设置cgroup中进程的CPU调度策略 cpuacct: 统计cgroup中进程CPU占用情况 cpuset: 在多核机器上设置cgroup中进程可以使用的CPU和内存(内存仅适用于NUMA架构) devices: 控制cgroup中进程对设备的访问 freezer: 用于挂起(suspend)和恢复(resume)cgroup中的进程 memory: 限制cgroup中进程的内存占用 net_cls: 将cgroup中进程的网络包进行分类，以至于通过分类区区分不同cgroup中进程的网络包，并进行监控、限流等。 net_prio: 设置cgroup中进程产生的网络流量的优先级 ns: 使cgroup中进程在新的Namespace中fork出新进程(NEWNS)，同时创建出新的cgroup，并且此cgroup包含有新Namespace中的进程。    hierarchy: cgroup进程的继承关系。
 例如： 系统通过cgroup1针对一组定时任务进程进行CPU使用限制，同时其中一个进程还需要限制磁盘IO，这时候将可以通过cgroup2继承cgroup1限制CPU的同时增加磁盘IO限制。即cgroup2同时具有CPU、IO限制，并且不影响cgroup1组中其他进程的IO限制。
   cgroup各模块间关系  系统创建hierarchy后，系统下所有进程都将被加入cgroup中，cgroup为根节点，被hierarchy创建的cgroup将被作为此cgroup根节点下的子节点; subsystem与hierarchy是1:1关系(即，一个subsystem只能作用于一个hierarchy之上); hierarchy与subsystem是1:n关系(即，一个hierarchy可以作用于多个subsystem之上); 一个进程可以分布在不同的cgroup中，但需满足cgroup分布在不同的hierarchy中; 一个进程fork出一个子进程的同时，子进程与父进程在同一cgroup中，但可以根据需求调整到其他cgroup中。  </description>
    </item>
    
    <item>
      <title>[ Docker ] 5.Docker-本地构建none包处理</title>
      <link>https://example.com/p/docker-5.docker-%E6%9C%AC%E5%9C%B0%E6%9E%84%E5%BB%BAnone%E5%8C%85%E5%A4%84%E7%90%86/</link>
      <pubDate>Fri, 21 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/docker-5.docker-%E6%9C%AC%E5%9C%B0%E6%9E%84%E5%BB%BAnone%E5%8C%85%E5%A4%84%E7%90%86/</guid>
      <description>Docker-本地构建none包处理 踩坑①.打包构建Dockerfile镜像 每次本地打包构建Dockerfile镜像，如果更新镜像版本号会出现none的镜像在仓库中
# 停掉none相关的镜像进程占用 docker rm $(docker ps -a | grep &amp;#34;Exited&amp;#34; | awk &amp;#39;{print $1 }&amp;#39;) # 递归依次从仓库移除这些镜像 docker rmi $(docker images | grep &amp;#34;^&amp;lt;none&amp;gt;&amp;#34; | awk &amp;#34;{print $3}&amp;#34;) # 或者，使用一下命令进行移除 docker image prune # (此命令用于删除未使用的映像) # docker image prune [options] # -- options可选值： # -a 显示所有映像(默认隐藏中间映像) # -f 不提示确认，强制直接执行删除 </description>
    </item>
    
    <item>
      <title>[ Docker ] 6.关于命名空间(Linux Namespace)</title>
      <link>https://example.com/p/docker-6.%E5%85%B3%E4%BA%8E%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4linux-namespace/</link>
      <pubDate>Fri, 21 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/docker-6.%E5%85%B3%E4%BA%8E%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4linux-namespace/</guid>
      <description>关于命名空间(Linux Namespace) 概念  1.Linux Namespace 是Kernel的一个功能，可以针对一系列的系统资源进行隔离。例如：PID(process id)、UID(User id)、Network so on.
  2.就像chroot允许把当前目录变成根目录一样进行隔离。
  3.Namespace进行隔离用户，当前用户将在特定的Namespace中具有root权限。但在真是物理机层面，此用户仍然是以UID运行的那个用户。
 Linux包含的Namespace类型    Type Params Kernel Effect Information     Mount CLONE_NEWNS 2.4.19 隔离Namespace下的文件系统   UTS CLONE_NEWUTS 2.6.19 用作隔离nodename和domainname   IPC CLONE_NEWIPC 2.6.19 隔离System V IPC 和POSIX Message queues   PID CLONE_NEWPID 2.6.24 针对进程ID进行隔离   Network CLONE_NEWNET 2.6.29 用于隔离网络设备、IP地址端口等网络栈   User CLONE_NEWUSER 3.8 用于隔离用户及用户组    #Demo Coding </description>
    </item>
    
    <item>
      <title>[ Docker ] 4.SpringBoot项目Docker化</title>
      <link>https://example.com/p/docker-4.springboot%E9%A1%B9%E7%9B%AEdocker%E5%8C%96/</link>
      <pubDate>Thu, 20 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/docker-4.springboot%E9%A1%B9%E7%9B%AEdocker%E5%8C%96/</guid>
      <description>SpringBoot项目Docker化 一 </description>
    </item>
    
    <item>
      <title>[ Docker ] 2.Docker安装</title>
      <link>https://example.com/p/docker-2.docker%E5%AE%89%E8%A3%85/</link>
      <pubDate>Tue, 18 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/docker-2.docker%E5%AE%89%E8%A3%85/</guid>
      <description>Docker安装  存储库安装   安装yum-config-manager所需依赖包
$~:sudo yum install -y yum-utils \  device-mapper-persistent-data \  lvm2   通过yum-config-manager添加存储库
$~:sudo yum-config-manager \  --add-repo \  https://download.docker.com/linux/centos/docker-ce.repo   列出存储库中排序后可用的全部版本
yum list docker-ce --showduplicates | sort -r   进行安装
# 指定版本号安装 sudo yum install docker-ce-&amp;lt;VERSION_STRING&amp;gt; docker-ce-cli-&amp;lt;VERSION_STRING&amp;gt; containerd.io # 安装最新版本（不指定版本号默认为最新） sudo yum install docker-ce docker-ce-cli containerd.io   安装docker-compose
# 下载二进制可执行文件，并保存在指定路径 sudo curl -L &amp;#34;https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)&amp;#34; -o /usr/local/bin/docker-compose # 修改文件权限 sudo chmod +x /usr/local/bin/docker-compose # 创建软链到全局可执行路径 sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose    软件包安装  </description>
    </item>
    
    <item>
      <title>[ Docker ] 3.Docker之jdk1.8最简镜像构建</title>
      <link>https://example.com/p/docker-3.docker%E4%B9%8Bjdk1.8%E6%9C%80%E7%AE%80%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA/</link>
      <pubDate>Tue, 18 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/docker-3.docker%E4%B9%8Bjdk1.8%E6%9C%80%E7%AE%80%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA/</guid>
      <description>Docker之jdk1.8最简镜像构建 1.准备JRE 在Java下载网站下载JRE。 Tips:此JRE为Oracle作品，而非Openjdk
2.精简JRE中无关文件 # 进入已经下载jre压缩包的路径,执行解压 tar xzvf ~/Downloads/jre-8u241-linux-x64.tar.gz&amp;amp;&amp;amp;cd jre1.8.0_241 # 删除说明、其他文档 rm -rf COPYRIGHT LICENSE README \ THIRDPARTYLICENSEREADME-JAVAFX.txt \ THIRDPARTYLICENSEREADME.txt \ Welcome.html # 删除非必要依赖文件 rm -rf lib/plugin.jar \  lib/ext/jfxrt.jar \  bin/javaws \  lib/javaws.jar \  lib/desktop \  plugin \  lib/deploy* \  lib/*javafx* \  lib/*jfx* \  lib/amd64/libdecora_sse.so \  lib/amd64/libprism_*.so \  lib/amd64/libfxplugins.so \  lib/amd64/libglass.so \  lib/amd64/libgstreamer-lite.so \  lib/amd64/libjavafx*.</description>
    </item>
    
    <item>
      <title>[ Docker ] 1.Docker命令</title>
      <link>https://example.com/p/docker-1.docker%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Sun, 16 Aug 2020 15:36:27 +0800</pubDate>
      
      <guid>https://example.com/p/docker-1.docker%E5%91%BD%E4%BB%A4/</guid>
      <description>Docker命令  docker [option] command
   option
 &amp;ndash;config string: 客户端配置文件的位置 &amp;ndash;context string[-c]: 用于连接到守护程序的上下文的名称 &amp;ndash;debug[-D]: 调试模式 &amp;ndash;host list[-H]: 要连接的守护程序套接字 &amp;ndash;log-level string[-l]: 日志等级[ debug | info | warn | error | fatal ]默认为info &amp;ndash;tls: 使用加密模式 &amp;ndash;tlscacert string: 签名证书文件路径 &amp;ndash;tlscert string: 密钥文件路径 &amp;ndash;tlskey string: key文件路径 &amp;ndash;tlsverify: 使用加密并验证远程连接 &amp;ndash;version[-v]: 版本信息    Management Commands(管理命令)
 builder: 管理构建 config: 管理Docker配置 container: 管理容器 context: 管理镜像构建上下文 image: 管理镜像 network: 管理网络 node: 管理Swarm节点 plugin: 管理插件 secret: 管理Docker secrets service: 管理服务 stack: 管理Docker stacks swarm: 管理Swarm集群 system: 查看系统信息 trust: 管理对Docker映像的信任 volume: 管理卷    Commands(命令)</description>
    </item>
    
  </channel>
</rss>
